

<!DOCTYPE html>


<html lang="en" data-content_root="" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Program Listing for File function.h &#8212; PyTorch main documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/theme.css" />
    <link rel="stylesheet" type="text/css" href="../_static/collapsible-lists/css/tree_view.css" />
    <link rel="stylesheet" type="text/css" href="../_static/cpp_theme.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/sphinx_highlight.js"></script>
    <script src="../_static/collapsible-lists/js/CollapsibleLists.compressed.js"></script>
    <script src="../_static/collapsible-lists/js/apply-collapsible-lists.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'api/program_listing_file_torch_csrc_autograd_function.h';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="File functional.h" href="file_torch_csrc_api_include_torch_nn_functional.h.html" />
    <link rel="prev" title="File function.h" href="file_torch_csrc_autograd_function.h.html" />

  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
<script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/2.3.1/list.min.js"></script>
<script>
  if (window.location.hostname === 'docs.pytorch.org' || window.location.hostname === 'docs-preview.pytorch.org') {
    const script = document.createElement('script');
    script.src = 'https://cmp.osano.com/16A0DbT9yDNIaQkvZ/31b1b91a-e0b6-47ea-bde2-7f2bd13dbe5c/osano.js?variant=one';
    document.head.appendChild(script);
  }
</script>
<script>
  // Cookie banner for non-LF projects
  document.addEventListener('DOMContentLoaded', function () {
    // Hide cookie banner on local environments and LF owned docs
    if (window.location.hostname === 'localhost' ||
      window.location.hostname === '0.0.0.0' ||
      window.location.hostname === '127.0.0.1' ||
      window.location.hostname === 'docs.pytorch.org' ||
      window.location.hostname === 'docs-preview.pytorch.org' ||
      window.location.hostname.startsWith('192.168.')) {
      const banner = document.querySelector('.cookie-banner-wrapper');
      if (banner) {
        banner.style.display = 'none';
      }
    }
  });
</script>
<!-- Conditional CSS for header and footer height adjustment -->

<link rel="stylesheet" type="text/css" href="../_static/css/theme.css" crossorigin="anonymous">
<script type="text/javascript" src="../_static/js/theme.js"></script>
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Montserrat:wght@300;400&display=swap" rel="stylesheet">
<meta property="og:image" content="../_static/img/pytorch_seo.png" />
<link rel="stylesheet" href="../_static/webfonts/all.min.css" crossorigin="anonymous">
<meta http-equiv="Content-Security-Policy"
  content="default-src * 'unsafe-inline' 'unsafe-eval' data: blob:; style-src * 'unsafe-inline'; script-src * 'unsafe-inline' 'unsafe-eval' blob:;">
<meta name="pytorch_project" content="">
<!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-T8XT4PS" height="0" width="0"
    style="display:none;visibility:hidden"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->
<!-- Google Tag Manager -->
<script>(function (w, d, s, l, i) {
    w[l] = w[l] || []; w[l].push({
      'gtm.start':
        new Date().getTime(), event: 'gtm.js'
    }); var f = d.getElementsByTagName(s)[0],
      j = d.createElement(s), dl = l != 'dataLayer' ? '&l=' + l : ''; j.async = true; j.src =
        'https://www.googletagmanager.com/gtm.js?id=' + i + dl; f.parentNode.insertBefore(j, f);
    j.onload = function () {
      window.dispatchEvent(new Event('gtm_loaded'));
      console.log('GTM loaded successfully');
    };
  })(window, document, 'script', 'dataLayer', 'GTM-T8XT4PS');
</script>
<!-- End Google Tag Manager -->
<!-- Facebook Pixel Code -->
<script>
  !function (f, b, e, v, n, t, s) {
    if (f.fbq) return; n = f.fbq = function () {
      n.callMethod ?
        n.callMethod.apply(n, arguments) : n.queue.push(arguments)
    };
    if (!f._fbq) f._fbq = n; n.push = n; n.loaded = !0; n.version = '2.0';
    n.queue = []; t = b.createElement(e); t.async = !0;
    t.src = v; s = b.getElementsByTagName(e)[0];
    s.parentNode.insertBefore(t, s)
  }(window, document, 'script',
    'https://connect.facebook.net/en_US/fbevents.js');
  fbq('init', '243028289693773');
  fbq('track', 'PageView');
</script>
<script>
  document.documentElement.setAttribute('data-version', 'main');
</script>
<noscript>
  <img height="1" width="1" src="https://www.facebook.com/tr?id=243028289693773&ev=PageView&noscript=1" />
</noscript>
<script>
  function gtag() {
    window.dataLayer.push(arguments);
  }
</script>
<!-- End Facebook Pixel Code -->
<!-- Script to Fix scrolling -->
<script>
  document.addEventListener('DOMContentLoaded', function () {
    // Fix anchor scrolling
    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
      anchor.addEventListener('click', function (e) {
        e.preventDefault();
        const targetId = this.getAttribute('href').substring(1);
        const targetElement = document.getElementById(targetId);

        if (targetElement) {
          const headerHeight =
            (document.querySelector('.header-holder') ? document.querySelector('.header-holder').offsetHeight : 0) +
            (document.querySelector('.bd-header') ? document.querySelector('.bd-header').offsetHeight : 0) + 20;

          const targetPosition = targetElement.getBoundingClientRect().top + window.pageYOffset - headerHeight;
          window.scrollTo({
            top: targetPosition,
            behavior: 'smooth'
          });

          // Update URL hash without scrolling
          history.pushState(null, null, '#' + targetId);
        }
      });
    });
  });
</script>

<script async src="https://cse.google.com/cse.js?cx=e65585f8c3ea1440e"></script>

<!--
   Search engines should not index the main version of documentation.
   Stable documentation are built without release == 'main'.
   -->
<meta name="robots" content="noindex">


  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>

  </head>

<body data-feedback-url="https://github.com/pytorch/pytorch" class="pytorch-body">
  
    <div class="container-fluid header-holder tutorials-header" id="header-holder">
   <div class="header-container-wrapper">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>

          <li class="main-menu-item">
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                <span>Learn</span>
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/get-started/locally">
                  <span class=dropdown-title>Get Started</span>
                </a>
                <a class="nav-dropdown-item" href="https://docs.pytorch.org/tutorials">
                  <span class="dropdown-title">Tutorials</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/basics/intro.html">
                  <span class="dropdown-title">Learn the Basics</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/recipes/recipes_index.html">
                  <span class="dropdown-title">PyTorch Recipes</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/introyt.html">
                  <span class="dropdown-title">Intro to PyTorch - YouTube Series</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/webinars/">
                  <span class="dropdown-title">Webinars</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
              <span>Community</span>
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://landscape.pytorch.org/" target="_blank">
                  <span class="dropdown-title">Landscape</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/join-ecosystem">
                  <span class="dropdown-title">Join the Ecosystem</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/community-hub/">
                  <span class="dropdown-title">Community Hub</span>
                </a>
                <a class="nav-dropdown-item" href="https://discuss.pytorch.org/">
                  <span class="dropdown-title">Forums</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/resources">
                  <span class=dropdown-title>Developer Resources</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/contributor-awards/">
                  <span class="dropdown-title">Contributor Awards</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/community-events/">
                  <span class="dropdown-title">Community Events</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/programs/ambassadors/">
                  <span class="dropdown-title">PyTorch Ambassadors</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
              <span>Projects</span>
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/projects/pytorch/">
                  <span class="dropdown-title">PyTorch</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/projects/vllm/">
                  <span class="dropdown-title">vLLM</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/projects/deepspeed/">
                  <span class="dropdown-title">DeepSpeed</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/projects/host-your-project/">
                  <span class="dropdown-title">Host Your Project</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
              <span> Docs</span>
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://docs.pytorch.org/docs/stable/index.html">
                  <span class="dropdown-title">PyTorch</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/domains">
                  <span class="dropdown-title">Domains</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
              <span>Blogs & News</span>
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/blog/">
                  <span class="dropdown-title">Blog</span>
                </a>
                 <a class="nav-dropdown-item" href="https://pytorch.org/announcements">
                  <span class="dropdown-title">Announcements</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/case-studies/">
                  <span class="dropdown-title">Case Studies</span>
                <a class="nav-dropdown-item" href="https://pytorch.org/events">
                  <span class="dropdown-title">Events</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/newsletter">
                  <span class="dropdown-title">Newsletter</span>
                </a>
            </div>
          </li>

          <li class="main-menu-item">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
              <span>About</span>
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/foundation">
                  <span class="dropdown-title">PyTorch Foundation</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/members">
                  <span class="dropdown-title">Members</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/governing-board">
                  <span class="dropdown-title">Governing Board</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tac">
                  <span class="dropdown-title">Technical Advisory Council</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/credits">
                  <span class="dropdown-title">Cloud Credit Program</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/staff">
                  <span class="dropdown-title">Staff</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/contact">
                  <span class="dropdown-title">Contact</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <div class="no-dropdown main-menu-button">
              <a href="https://pytorch.org/join" data-cta="join">
                JOIN
              </a>
            </div>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu">
        <i class="fa-solid fa-ellipsis"></i>
      </a>
    </div>
  </div>
 </div>

 <!-- Begin Mobile Menu -->

<div class="mobile-main-menu">
  <div class="container-fluid">
    <div class="header-container-wrapper">
      <div class="mobile-main-menu-header-container">
        <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu">
        </a>
      </div>
    </div>
  </div>

  <div class="mobile-main-menu-links-container">
    <div class="main-menu">
      <ul>
         <li class="resources-mobile-menu-title">
           <a>Learn</a>
         </li>
         <ul class="resources-mobile-menu-items">
           <li>
             <a href="https://pytorch.org/get-started/locally">Get Started</a>
           </li>
           <li>
             <a href="https://docs.pytorch.org/tutorials">Tutorials</a>
           </li>
           <li>
             <a href="https://pytorch.org/tutorials/beginner/basics/intro.html">Learn the Basics</a>
           </li>
           <li>
             <a href="https://pytorch.org/tutorials/recipes/recipes_index.html">PyTorch Recipes</a>
           </li>
           <li>
             <a href="https://pytorch.org/tutorials/beginner/introyt.html">Introduction to PyTorch - YouTube Series</a>
           </li>
           <li>
            <a href="https://pytorch.org/webinars/">Webinars</a>
          </li>
        </ul>
         <li class="resources-mobile-menu-title">
           <a>Community</a>
         </li>
         <ul class="resources-mobile-menu-items">
          <li>
            <a href="https://landscape.pytorch.org/">Landscape</a>
          </li>
          <li>
             <a href="https://pytorch.org/join-ecosystem">Join the Ecosystem</a>
           </li>
           <li>
             <a href="https://pytorch.org/community-hub/">Community Hub</a>
           </li>
           <li>
             <a href="https://discuss.pytorch.org/">Forums</a>
           </li>
           <li>
             <a href="https://pytorch.org/resources">Developer Resources</a>
           </li>
           <li>
             <a href="https://pytorch.org/contributor-awards/">Contributor Awards</a>
           </li>
           <li>
            <a href="https://pytorch.org/community-events/">Community Events</a>
          </li>
          <li>
            <a href="https://pytorch.org/programs/ambassadors/">PyTorch Ambassadors</a>
          </li>
       </ul>

         <li class="resources-mobile-menu-title">
           <a>Projects</a>
         </li>

         <ul class="resources-mobile-menu-items">
           <li>
             <a href="https://pytorch.org/projects/pytorch/">PyTorch</a>
           </li>

           <li>
             <a href="https://pytorch.org/projects/vllm/">vLLM</a>
           </li>
           <li>
            <a href="https://pytorch.org/projects/deepspeed/">DeepSpeed</a>
          </li>
          <li>
             <a href="https://pytorch.org/projects/host-your-project/">Host Your Project</a>
           </li>
         </ul>

         <li class="resources-mobile-menu-title">
           <a>Docs</a>
         </li>

         <ul class="resources-mobile-menu-items">
          <li>
            <a href="https://docs.pytorch.org/docs/stable/index.html">PyTorch</a>
          </li>

          <li>
            <a href="https://pytorch.org/domains">Domains</a>
          </li>
        </ul>

        <li class="resources-mobile-menu-title">
          <a>Blog & News</a>
        </li>

         <ul class="resources-mobile-menu-items">
          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>
          <li>
            <a href="https://pytorch.org/announcements">Announcements</a>
          </li>

          <li>
            <a href="https://pytorch.org/case-studies/">Case Studies</a>
          </li>
          <li>
            <a href="https://pytorch.org/events">Events</a>
          </li>
          <li>
             <a href="https://pytorch.org/newsletter">Newsletter</a>
           </li>
        </ul>

        <li class="resources-mobile-menu-title">
          <a>About</a>
        </li>

        <ul class="resources-mobile-menu-items">
          <li>
            <a href="https://pytorch.org/foundation">PyTorch Foundation</a>
          </li>
          <li>
            <a href="https://pytorch.org/members">Members</a>
          </li>
          <li>
            <a href="https://pytorch.org/governing-board">Governing Board</a>
          </li>
          <li>
            <a href="https://pytorch.org/tac">Technical Advisory Council</a>
         </li>
         <li>
             <a href="https://pytorch.org/credits">Cloud Credit Program</a>
          </li>
          <li>
             <a href="https://pytorch.org/staff">Staff</a>
          </li>
          <li>
             <a href="https://pytorch.org/contact">Contact</a>
          </li>
        </ul>
      </ul>
    </div>
  </div>
</div>

<!-- End Mobile Menu -->
  
  
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
<div class="bd-header__inner bd-page-width">
  <button class="pst-navbar-icon sidebar-toggle primary-toggle" aria-label="Site navigation">
    <span class="fa-solid fa-bars"></span>
  </button>
  
  
  <div class=" navbar-header-items__start">
    
      <div class="navbar-item">
  <a href="../index.html" class="version">main</a>
</div>
    
  </div>
  
  <div class=" navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../installing.html">
    Installing C++ Distributions of PyTorch
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../frontend.html">
    The C++ Frontend
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="library_root.html">
    Library API
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../notes/faq.html">
    FAQ
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../notes/inference_mode.html">
    Inference Mode
  </a>
</li>

            <li class="nav-item dropdown">
                <button class="btn dropdown-toggle nav-item" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-controls="pst-nav-more-links">
                    More
                </button>
                <ul id="pst-nav-more-links" class="dropdown-menu">
                    
<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="../notes/maybe_owned.html">
    MaybeOwned<Tensor>
  </a>
</li>


<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="../notes/tensor_basics.html">
    Tensor Basics
  </a>
</li>


<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="../notes/tensor_creation.html">
    Tensor Creation API
  </a>
</li>


<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="../notes/tensor_cuda_stream.html">
    Tensor CUDA Stream API
  </a>
</li>


<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="../notes/tensor_indexing.html">
    Tensor Indexing API
  </a>
</li>


<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="../notes/versioning.html">
    Library Versioning
  </a>
</li>

                </ul>
            </li>
            
  </ul>
</nav></div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
      
        <div class="navbar-item">
  
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
</div>
      
        <div class="navbar-item">

<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
      
        <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://x.com/PyTorch" title="X" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-x-twitter fa-lg" aria-hidden="true"></i>
            <span class="sr-only">X</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/pytorch/pytorch" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://discuss.pytorch.org/" title="PyTorch Forum" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-discourse fa-lg" aria-hidden="true"></i>
            <span class="sr-only">PyTorch Forum</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://pypi.org/project/torch/" title="PyPi" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-python fa-lg" aria-hidden="true"></i>
            <span class="sr-only">PyPi</span></a>
        </li>
</ul></div>
      
    </div>
    
  </div>
  
  

  
    <button class="pst-navbar-icon sidebar-toggle secondary-toggle" aria-label="On this page">
      <span class="fa-solid fa-outdent"></span>
    </button>
  
</div>

    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
        
          
          
            <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../installing.html">
    Installing C++ Distributions of PyTorch
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../frontend.html">
    The C++ Frontend
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="library_root.html">
    Library API
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../notes/faq.html">
    FAQ
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../notes/inference_mode.html">
    Inference Mode
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../notes/maybe_owned.html">
    MaybeOwned<Tensor>
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../notes/tensor_basics.html">
    Tensor Basics
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../notes/tensor_creation.html">
    Tensor Creation API
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../notes/tensor_cuda_stream.html">
    Tensor CUDA Stream API
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../notes/tensor_indexing.html">
    Tensor Indexing API
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../notes/versioning.html">
    Library Versioning
  </a>
</li>

  </ul>
</nav></div>
          
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item">
  
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
</div>
        
          <div class="navbar-item">

<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
        
          <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://x.com/PyTorch" title="X" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-x-twitter fa-lg" aria-hidden="true"></i>
            <span class="sr-only">X</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/pytorch/pytorch" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://discuss.pytorch.org/" title="PyTorch Forum" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-discourse fa-lg" aria-hidden="true"></i>
            <span class="sr-only">PyTorch Forum</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://pypi.org/project/torch/" title="PyPi" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-python fa-lg" aria-hidden="true"></i>
            <span class="sr-only">PyPi</span></a>
        </li>
</ul></div>
        
      </div>
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
<nav class="bd-docs-nav bd-links"
     aria-label="Section Navigation">
  <p class="bd-links__title" role="heading" aria-level="1">Section Navigation</p>
  <div class="bd-toc-item navbar-nav"><ul class="current nav bd-sidenav">
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Program Listing for File function.h</a></li>
</ul>
</div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">



<nav aria-label="Breadcrumb" class="d-print-none">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    
    <li class="breadcrumb-item"><a href="library_root.html" class="nav-link">Library API</a></li>
    
    
    <li class="breadcrumb-item"><a href="file_torch_csrc_autograd_function.h.html" class="nav-link">File function.h</a></li>
    
    <li class="breadcrumb-item active" aria-current="page">Program...</li>
  </ul>
</nav>
</div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">
<div class="rating">
    Rate this Page
    <div class="stars">
        
        <span class="star" data-behavior="tutorial-rating" data-count="1" data-value="1">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="2" data-value="2">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="3" data-value="3">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="4" data-value="4">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="5" data-value="5">★</span>
        
    </div>
</div>
</div>
      
    </div>
  
</div>
</div>
              
              
  
<div id="searchbox"></div>
  <article class="bd-article" id="pytorch-article">
    <!-- Hidden breadcrumb schema for SEO only -->
    <div style="display:none;" itemscope itemtype="https://schema.org/BreadcrumbList">
      
      <div itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
        <link itemprop="item" href="library_root.html">
        <meta itemprop="name" content="Library API">
        <meta itemprop="position" content="1">
      </div>
      
      <div itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
        <link itemprop="item" href="file_torch_csrc_autograd_function.h.html">
        <meta itemprop="name" content="File function.h">
        <meta itemprop="position" content="2">
      </div>
      
      <div itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
        <meta itemprop="name" content="Program Listing for File function.h">
        <meta itemprop="position" content="3">
      </div>
    </div>

    

    
              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <div class="section" id="program-listing-for-file-function-h">
<span id="program-listing-file-torch-csrc-autograd-function-h"></span><h1>Program Listing for File function.h<a class="headerlink" href="#program-listing-for-file-function-h" title="Permalink to this heading">#</a></h1>
<p>↰ <a class="reference internal" href="file_torch_csrc_autograd_function.h.html#file-torch-csrc-autograd-function-h"><span class="std std-ref">Return to documentation for file</span></a> (<code class="docutils literal notranslate"><span class="pre">torch/csrc/autograd/function.h</span></code>)</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="cp">#pragma once</span>

<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;torch/csrc/autograd/anomaly_mode.h&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;torch/csrc/autograd/edge.h&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;torch/csrc/autograd/grad_mode.h&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;torch/csrc/autograd/graph_task.h&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;torch/csrc/autograd/input_metadata.h&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;torch/csrc/autograd/saved_variable.h&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;torch/csrc/autograd/variable.h&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;torch/csrc/utils/python_stub.h&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;torch/csrc/utils/variadic.h&gt;</span>

<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;ATen/SequenceNumber.h&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;ATen/core/Tensor.h&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;ATen/record_function.h&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;c10/util/Exception.h&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;c10/util/irange.h&gt;</span>

<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;algorithm&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;cstdint&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;initializer_list&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;memory&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;string&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;utility&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;vector&gt;</span>

<span class="k">namespace</span><span class="w"> </span><span class="nn">torch</span><span class="o">::</span><span class="nn">autograd</span><span class="w"> </span><span class="p">{</span>

<span class="k">struct</span><span class="w"> </span><span class="nc">Edge</span><span class="p">;</span>
<span class="k">struct</span><span class="w"> </span><span class="nc">FunctionPostHook</span><span class="p">;</span>
<span class="k">struct</span><span class="w"> </span><span class="nc">FunctionPreHook</span><span class="p">;</span>

<span class="k">using</span><span class="w"> </span><span class="n">tensor_list</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="p">;</span>
<span class="k">using</span><span class="w"> </span><span class="n">variable_list</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">Variable</span><span class="o">&gt;</span><span class="p">;</span>
<span class="k">using</span><span class="w"> </span><span class="n">edge_list</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">Edge</span><span class="o">&gt;</span><span class="p">;</span>
<span class="k">using</span><span class="w"> </span><span class="n">saved_variable_list</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">SavedVariable</span><span class="o">&gt;</span><span class="p">;</span>
<span class="k">using</span><span class="w"> </span><span class="n">ivalue_list</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">c10</span><span class="o">::</span><span class="n">IValue</span><span class="o">&gt;</span><span class="p">;</span>
<span class="k">using</span><span class="w"> </span><span class="n">functional_apply_t</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">function</span><span class="o">&lt;</span>
<span class="w">    </span><span class="n">variable_list</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">variable_list</span><span class="o">&amp;</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">c10</span><span class="o">::</span><span class="n">IValue</span><span class="o">&gt;&amp;</span><span class="p">)</span><span class="o">&gt;</span><span class="p">;</span>
<span class="k">using</span><span class="w"> </span><span class="n">IndexRange</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">pair</span><span class="o">&lt;</span><span class="kt">size_t</span><span class="p">,</span><span class="w"> </span><span class="kt">size_t</span><span class="o">&gt;</span><span class="p">;</span>
<span class="k">using</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">dynamo</span><span class="o">::</span><span class="n">autograd</span><span class="o">::</span><span class="n">CompiledNodeArgs</span><span class="p">;</span>
<span class="k">using</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">dynamo</span><span class="o">::</span><span class="n">autograd</span><span class="o">::</span><span class="n">PackedArgs</span><span class="p">;</span>
<span class="k">using</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">dynamo</span><span class="o">::</span><span class="n">autograd</span><span class="o">::</span><span class="n">SwapSavedVariables</span><span class="p">;</span>

<span class="c1">// Custom deleter to prevent stack overflows.</span>
<span class="n">TORCH_API</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="n">deleteNode</span><span class="p">(</span><span class="n">Node</span><span class="o">*</span><span class="w"> </span><span class="n">function</span><span class="p">);</span>

<span class="c1">// Guard that sets and restores the evaluating node</span>
<span class="k">class</span><span class="w"> </span><span class="nc">NodeGuard</span><span class="w"> </span><span class="p">{</span>
<span class="w"> </span><span class="k">public</span><span class="o">:</span>
<span class="w">  </span><span class="k">explicit</span><span class="w"> </span><span class="n">NodeGuard</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">Node</span><span class="o">&gt;</span><span class="w"> </span><span class="n">node</span><span class="p">);</span>
<span class="w">  </span><span class="o">~</span><span class="n">NodeGuard</span><span class="p">();</span>

<span class="w"> </span><span class="k">private</span><span class="o">:</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">Node</span><span class="o">&gt;</span><span class="w"> </span><span class="n">last_evaluating_node_</span><span class="p">;</span>
<span class="p">};</span>

<span class="c1">// Return the Node currently being evaluated (if any)</span>
<span class="c1">// This is only set during the backward pass while a Node is being</span>
<span class="c1">// executed.</span>
<span class="n">TORCH_API</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">Node</span><span class="o">&gt;</span><span class="w"> </span><span class="n">get_current_node</span><span class="p">();</span>

<span class="c1">//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
<span class="c1">//                               Node</span>
<span class="c1">//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
<span class="c1">// A `Node` is an abstract class that represents an operation taking zero</span>
<span class="c1">// or more input `Variable`s and producing zero or more output `Variable`s. All</span>
<span class="c1">// functions in PyTorch&#39;s autograd machinery derive from this class and</span>
<span class="c1">// override its `apply` method. Instances of such subclasses will then be</span>
<span class="c1">// invocable via the call operator.</span>
<span class="c1">//</span>
<span class="c1">//                    Nodes in the Autograd Graph</span>
<span class="c1">//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
<span class="c1">// When viewing the autograd system as a graph, `Node`s are the vertices or</span>
<span class="c1">// nodes, connected to each other via (directed) `Edge`s, which themselves are</span>
<span class="c1">// represented via (`Node`, input_nr) pairs. `Variable`s are the outputs to</span>
<span class="c1">// and inputs of `Node`s, and travel between these edges during execution</span>
<span class="c1">// of the graph. When two or more `Edge`s (from different sources) point at the</span>
<span class="c1">// same input to a `Node`, the values produced along all of these edges are</span>
<span class="c1">// implicitly summed prior to being forwarded to the target `Node`.</span>
<span class="c1">//</span>
<span class="c1">//                              Hierarchy</span>
<span class="c1">//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
<span class="c1">// Subclasses usually represent differentiable functions as well as their</span>
<span class="c1">// gradient operators. Note, however, that due to the very general definition</span>
<span class="c1">// of a `Node` taking *zero* or more inputs and producing *zero* or more</span>
<span class="c1">// outputs, uses of `Node`s are flexible and extend beyond purely</span>
<span class="c1">// mathematical operations. For example, the `AccumulateGrad` function is a</span>
<span class="c1">// *sink*: it takes one input, but produces no outputs, instead accumulating</span>
<span class="c1">// the input as a side effect. At the other extreme, the `GraphRoot` function</span>
<span class="c1">// receives no inputs from other functions, but produces multiple outputs.</span>
<span class="c1">//</span>
<span class="c1">//                              Interface</span>
<span class="c1">//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
<span class="c1">// The most important method on `Node` is the call operator, which takes in</span>
<span class="c1">// a list of variables and produces a list of variables. The precise size of</span>
<span class="c1">// these lists can be determined with `num_inputs()` and `num_outputs()`.</span>
<span class="c1">// `Node`s are stitched together via their `next_edge` interface, which let</span>
<span class="c1">// you manipulate the set of outgoing edges of a `Node`. You can add an</span>
<span class="c1">// edge with `add_next_edge()`, retrieve an edge with `next_edge(index)` and</span>
<span class="c1">// iterate over them via the `next_edges()` method. Other methods exist for</span>
<span class="c1">// integration with the JIT and other parts of PyTorch. Every `Node` has a</span>
<span class="c1">// *sequence number* that increases monotonically in the order of `Node`</span>
<span class="c1">// construction. It can be retrieved via the `sequence_nr()` method. Note that</span>
<span class="c1">// this sequence number is *thread local*. This means that when `Node`s</span>
<span class="c1">// `A`, `B` and `C` are created consecutively in the same thread, their</span>
<span class="c1">// sequence numbers will be ordered `A` &lt; `B` &lt; `C`. If, however, `A` and `B`</span>
<span class="c1">// are created in one thread and `C` is created in a new thread, there are *no</span>
<span class="c1">// guarantees* w.r.t. the ordering of `C` relative to `A` or `B`.</span>
<span class="c1">// See NOTE [ Sequence Number] for more details on the usages of sequence</span>
<span class="c1">// number.</span>
<span class="c1">//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
<span class="k">struct</span><span class="w"> </span><span class="nc">TORCH_API</span><span class="w"> </span><span class="n">Node</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">enable_shared_from_this</span><span class="o">&lt;</span><span class="n">Node</span><span class="o">&gt;</span><span class="w"> </span><span class="p">{</span>
<span class="w"> </span><span class="k">public</span><span class="o">:</span>
<span class="w">  </span><span class="k">explicit</span><span class="w"> </span><span class="n">Node</span><span class="p">(</span><span class="kt">uint64_t</span><span class="w"> </span><span class="n">sequence_nr</span><span class="p">,</span><span class="w"> </span><span class="n">edge_list</span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="n">next_edges</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">edge_list</span><span class="p">())</span>
<span class="w">      </span><span class="o">:</span><span class="w"> </span><span class="n">sequence_nr_</span><span class="p">(</span><span class="n">sequence_nr</span><span class="p">),</span><span class="w"> </span><span class="n">next_edges_</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">move</span><span class="p">(</span><span class="n">next_edges</span><span class="p">))</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">Edge</span><span class="o">&amp;</span><span class="w"> </span><span class="n">edge</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="n">next_edges_</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="n">update_topological_nr</span><span class="p">(</span><span class="n">edge</span><span class="p">);</span>
<span class="w">    </span><span class="p">}</span>

<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">AnomalyMode</span><span class="o">::</span><span class="n">is_enabled</span><span class="p">())</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="n">metadata</span><span class="p">()</span><span class="o">-&gt;</span><span class="n">store_stack</span><span class="p">();</span>

<span class="w">      </span><span class="c1">// If anomaly mode is enabled and graph is constructed, then assign the</span>
<span class="w">      </span><span class="c1">// currently evaluating node as the parent of this node.</span>
<span class="w">      </span><span class="c1">// A parent is a Node where this Node is created.</span>
<span class="w">      </span><span class="c1">// We are tracking the parents to track multiple backward operations.</span>
<span class="w">      </span><span class="n">assign_parent</span><span class="p">();</span>
<span class="w">    </span><span class="p">}</span>

<span class="w">    </span><span class="c1">// Store the thread_id of the forward operator.</span>
<span class="w">    </span><span class="c1">// See NOTE [ Sequence Numbers ]</span>
<span class="w">    </span><span class="n">thread_id_</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">RecordFunction</span><span class="o">::</span><span class="n">currentThreadId</span><span class="p">();</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="k">explicit</span><span class="w"> </span><span class="n">Node</span><span class="p">(</span><span class="n">edge_list</span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="n">next_edges</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">edge_list</span><span class="p">())</span>
<span class="w">      </span><span class="o">:</span><span class="w"> </span><span class="n">Node</span><span class="p">(</span>
<span class="w">            </span><span class="cm">/*sequence_nr=*/</span><span class="n">at</span><span class="o">::</span><span class="n">sequence_number</span><span class="o">::</span><span class="n">get_and_increment</span><span class="p">(),</span>
<span class="w">            </span><span class="n">std</span><span class="o">::</span><span class="n">move</span><span class="p">(</span><span class="n">next_edges</span><span class="p">))</span><span class="w"> </span><span class="p">{}</span>

<span class="w">  </span><span class="n">Node</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">Node</span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">delete</span><span class="p">;</span>
<span class="w">  </span><span class="n">Node</span><span class="p">(</span><span class="n">Node</span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">delete</span><span class="p">;</span>
<span class="w">  </span><span class="n">Node</span><span class="o">&amp;</span><span class="w"> </span><span class="k">operator</span><span class="o">=</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">Node</span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">delete</span><span class="p">;</span>
<span class="w">  </span><span class="n">Node</span><span class="o">&amp;</span><span class="w"> </span><span class="k">operator</span><span class="o">=</span><span class="p">(</span><span class="n">Node</span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">delete</span><span class="p">;</span>
<span class="w">  </span><span class="k">virtual</span><span class="w"> </span><span class="o">~</span><span class="n">Node</span><span class="p">()</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">default</span><span class="p">;</span>

<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">Node</span><span class="o">&gt;</span><span class="w"> </span><span class="n">getptr</span><span class="p">()</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">shared_from_this</span><span class="p">();</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="n">variable_list</span><span class="w"> </span><span class="k">operator</span><span class="p">()(</span><span class="n">variable_list</span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="n">inputs</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="c1">// In the first iteration of named tensors, autograd ignores names and</span>
<span class="w">    </span><span class="c1">// operates on unnamed tensors. In the long term, autograd should</span>
<span class="w">    </span><span class="c1">// probably operate with names.</span>
<span class="w">    </span><span class="n">at</span><span class="o">::</span><span class="n">NoNamesGuard</span><span class="w"> </span><span class="n">no_names_guard</span><span class="p">;</span>

<span class="cp">#ifdef USE_ROCM</span>
<span class="w">    </span><span class="c1">// Keep track of backward pass for rocblas.</span>
<span class="w">    </span><span class="n">at</span><span class="o">::</span><span class="n">ROCmBackwardPassGuard</span><span class="w"> </span><span class="n">in_backward</span><span class="p">;</span>
<span class="cp">#endif</span>

<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">step_callbacks</span><span class="w"> </span><span class="o">=</span>
<span class="w">        </span><span class="n">at</span><span class="o">::</span><span class="n">getStepCallbacksUnlessEmpty</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">RecordScope</span><span class="o">::</span><span class="n">BACKWARD_FUNCTION</span><span class="p">);</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">C10_UNLIKELY</span><span class="p">(</span><span class="n">step_callbacks</span><span class="p">.</span><span class="n">has_value</span><span class="p">()))</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="n">at</span><span class="o">::</span><span class="n">RecordFunction</span><span class="w"> </span><span class="nf">guard</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">move</span><span class="p">(</span><span class="o">*</span><span class="n">step_callbacks</span><span class="p">));</span>
<span class="w">      </span><span class="c1">// Using sequence number and thread id to correlate with</span>
<span class="w">      </span><span class="c1">// the forward pass function</span>
<span class="w">      </span><span class="n">guard</span><span class="p">.</span><span class="n">setForwardThreadId</span><span class="p">(</span><span class="n">thread_id_</span><span class="p">);</span>
<span class="w">      </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">guard</span><span class="p">.</span><span class="n">needsInputs</span><span class="p">())</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">c10</span><span class="o">::</span><span class="n">IValue</span><span class="o">&gt;</span><span class="w"> </span><span class="n">inputs_vec</span><span class="p">(</span><span class="n">inputs</span><span class="p">.</span><span class="n">begin</span><span class="p">(),</span><span class="w"> </span><span class="n">inputs</span><span class="p">.</span><span class="n">end</span><span class="p">());</span>
<span class="w">        </span><span class="n">guard</span><span class="p">.</span><span class="n">before</span><span class="p">(</span>
<span class="w">            </span><span class="n">name</span><span class="p">(),</span>
<span class="w">            </span><span class="n">c10</span><span class="o">::</span><span class="n">ArrayRef</span><span class="o">&lt;</span><span class="k">const</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">IValue</span><span class="o">&gt;</span><span class="p">(</span>
<span class="w">                </span><span class="n">inputs_vec</span><span class="p">.</span><span class="n">data</span><span class="p">(),</span><span class="w"> </span><span class="n">inputs_vec</span><span class="p">.</span><span class="n">size</span><span class="p">()),</span>
<span class="w">            </span><span class="k">static_cast</span><span class="o">&lt;</span><span class="kt">int64_t</span><span class="o">&gt;</span><span class="p">(</span><span class="n">sequence_nr</span><span class="p">()));</span>
<span class="w">      </span><span class="p">}</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="n">guard</span><span class="p">.</span><span class="n">before</span><span class="p">(</span><span class="n">name</span><span class="p">(),</span><span class="w"> </span><span class="k">static_cast</span><span class="o">&lt;</span><span class="kt">int64_t</span><span class="o">&gt;</span><span class="p">(</span><span class="n">sequence_nr</span><span class="p">()));</span>
<span class="w">      </span><span class="p">}</span>
<span class="w">      </span><span class="k">return</span><span class="w"> </span><span class="n">apply</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">move</span><span class="p">(</span><span class="n">inputs</span><span class="p">));</span>
<span class="w">    </span><span class="p">}</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="k">return</span><span class="w"> </span><span class="n">apply</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">move</span><span class="p">(</span><span class="n">inputs</span><span class="p">));</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="c1">// Graph Connectivity API</span>
<span class="w">  </span><span class="c1">//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>

<span class="w">  </span><span class="c1">// Inputs. NOTE: inputs of the grad_fn correspond to Tensor outputs of the</span>
<span class="w">  </span><span class="c1">// forward function.</span>

<span class="w">  </span><span class="c1">// Marker for expected undefined input</span>
<span class="w">  </span><span class="k">struct</span><span class="w"> </span><span class="nc">undefined_input</span><span class="w"> </span><span class="p">{};</span>

<span class="w">  </span><span class="kt">uint32_t</span><span class="w"> </span><span class="nf">add_input_metadata</span><span class="p">(</span>
<span class="w">      </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">TensorOptions</span><span class="o">&amp;</span><span class="w"> </span><span class="n">options</span><span class="p">,</span>
<span class="w">      </span><span class="n">c10</span><span class="o">::</span><span class="n">SymIntArrayRef</span><span class="w"> </span><span class="n">shape</span><span class="p">,</span>
<span class="w">      </span><span class="kt">bool</span><span class="w"> </span><span class="n">is_tensor_subclass</span><span class="p">,</span>
<span class="w">      </span><span class="kt">bool</span><span class="w"> </span><span class="n">is_nested</span><span class="p">,</span>
<span class="w">      </span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">ScalarType</span><span class="o">&gt;</span><span class="w"> </span><span class="n">grad_dtype</span><span class="p">)</span><span class="w"> </span><span class="k">noexcept</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="kt">uint32_t</span><span class="w"> </span><span class="n">input_nr</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">input_metadata_</span><span class="p">.</span><span class="n">size</span><span class="p">();</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">meta_shape</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">MetadataShape</span><span class="p">{</span><span class="n">std</span><span class="o">::</span><span class="n">in_place_type</span><span class="o">&lt;</span><span class="n">SymIntSmallVec</span><span class="o">&gt;</span><span class="p">,</span><span class="w"> </span><span class="n">shape</span><span class="p">};</span>
<span class="w">    </span><span class="n">input_metadata_</span><span class="p">.</span><span class="n">emplace_back</span><span class="p">(</span>
<span class="w">        </span><span class="n">options</span><span class="p">,</span><span class="w"> </span><span class="n">meta_shape</span><span class="p">,</span><span class="w"> </span><span class="n">is_tensor_subclass</span><span class="p">,</span><span class="w"> </span><span class="n">is_nested</span><span class="p">,</span><span class="w"> </span><span class="n">grad_dtype</span><span class="p">);</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">input_nr</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="kt">uint32_t</span><span class="w"> </span><span class="nf">add_input_metadata</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&amp;</span><span class="w"> </span><span class="n">t</span><span class="p">)</span><span class="w"> </span><span class="k">noexcept</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="kt">uint32_t</span><span class="w"> </span><span class="n">input_nr</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">input_metadata_</span><span class="p">.</span><span class="n">size</span><span class="p">();</span>
<span class="w">    </span><span class="n">input_metadata_</span><span class="p">.</span><span class="n">emplace_back</span><span class="p">(</span><span class="n">t</span><span class="p">);</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">input_nr</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="kt">uint32_t</span><span class="w"> </span><span class="nf">add_input_metadata</span><span class="p">(</span><span class="n">undefined_input</span><span class="w"> </span><span class="n">u</span><span class="p">)</span><span class="w"> </span><span class="k">noexcept</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="kt">uint32_t</span><span class="w"> </span><span class="n">input_nr</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">input_metadata_</span><span class="p">.</span><span class="n">size</span><span class="p">();</span>
<span class="w">    </span><span class="n">input_metadata_</span><span class="p">.</span><span class="n">emplace_back</span><span class="p">();</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">input_nr</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="kt">uint32_t</span><span class="w"> </span><span class="nf">num_inputs</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="k">noexcept</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">input_metadata_</span><span class="p">.</span><span class="n">size</span><span class="p">();</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="k">const</span><span class="w"> </span><span class="n">InputMetadata</span><span class="o">&amp;</span><span class="w"> </span><span class="nf">input_metadata</span><span class="p">(</span><span class="kt">size_t</span><span class="w"> </span><span class="n">index</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">input_metadata_</span><span class="p">[</span><span class="n">index</span><span class="p">];</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="c1">// Danger: not thread safe, caller must protect with lock</span>
<span class="w">  </span><span class="n">InputMetadata</span><span class="o">&amp;</span><span class="w"> </span><span class="nf">mutable_input_metadata</span><span class="p">(</span><span class="kt">size_t</span><span class="w"> </span><span class="n">index</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">input_metadata_</span><span class="p">[</span><span class="n">index</span><span class="p">];</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">c10</span><span class="o">::</span><span class="n">Stream</span><span class="o">&gt;</span><span class="w"> </span><span class="n">stream</span><span class="p">()</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">opt_device_type</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">getAccelerator</span><span class="p">();</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="o">!</span><span class="n">opt_device_type</span><span class="p">.</span><span class="n">has_value</span><span class="p">())</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="k">return</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">;</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="k">auto</span><span class="o">&amp;</span><span class="w"> </span><span class="n">metadata</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="n">input_metadata_</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">metadata</span><span class="p">.</span><span class="n">device</span><span class="p">().</span><span class="n">type</span><span class="p">()</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="n">opt_device_type</span><span class="p">.</span><span class="n">value</span><span class="p">())</span>
<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="n">metadata</span><span class="p">.</span><span class="n">stream</span><span class="p">();</span>
<span class="w">    </span><span class="p">}</span>

<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="c1">// Used by the engine to determine what device thread to run on</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Device</span><span class="w"> </span><span class="n">device</span><span class="p">()</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="c1">// Since we pick the first non-CPU tensor, this won&#39;t work with</span>
<span class="w">    </span><span class="c1">// mixed device-type operations (e.g., an op that is both CUDA</span>
<span class="w">    </span><span class="c1">// and XLA).  This is *incredibly* unlikely, so we don&#39;t worry</span>
<span class="w">    </span><span class="c1">// about it.</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="k">auto</span><span class="o">&amp;</span><span class="w"> </span><span class="n">metadata</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="n">input_metadata_</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="k">auto</span><span class="w"> </span><span class="n">device</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">metadata</span><span class="p">.</span><span class="n">device</span><span class="p">();</span>
<span class="w">      </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">device</span><span class="p">.</span><span class="n">type</span><span class="p">()</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">kCPU</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="n">device</span><span class="p">;</span>
<span class="w">      </span><span class="p">}</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span><span class="c1">// Only report to the CPU thread if there really were no tensors</span>
<span class="w">    </span><span class="c1">// from other devices.</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">kCPU</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="kt">void</span><span class="w"> </span><span class="n">clear_input_metadata</span><span class="p">()</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">input_metadata_</span><span class="p">.</span><span class="n">clear</span><span class="p">();</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="c1">// Outputs (&quot;Next Edges&quot;)</span>

<span class="w">  </span><span class="kt">void</span><span class="w"> </span><span class="n">update_topological_nr</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">Edge</span><span class="o">&amp;</span><span class="w"> </span><span class="n">edge</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">TORCH_INTERNAL_ASSERT</span><span class="p">(</span>
<span class="w">        </span><span class="o">!</span><span class="n">has_parent_</span><span class="p">,</span>
<span class="w">        </span><span class="s">&quot;Cannot update a node&#39;s topological_nr after it already has a parent.&quot;</span>
<span class="w">        </span><span class="s">&quot; If we allow this, we can no longer guarantee that a parent&#39;s&quot;</span>
<span class="w">        </span><span class="s">&quot; topo_nr is always greater than those of all its children&quot;</span><span class="p">)</span>
<span class="w">    </span><span class="n">Node</span><span class="o">*</span><span class="w"> </span><span class="n">node</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">edge</span><span class="p">.</span><span class="n">function</span><span class="p">.</span><span class="n">get</span><span class="p">();</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">node</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="k">auto</span><span class="w"> </span><span class="n">topo_nr</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">node</span><span class="o">-&gt;</span><span class="n">topological_nr</span><span class="p">();</span>
<span class="w">      </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">topological_nr_</span><span class="w"> </span><span class="o">&lt;=</span><span class="w"> </span><span class="n">topo_nr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="n">topological_nr_</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">topo_nr</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="p">;</span>
<span class="w">      </span><span class="p">}</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="kt">void</span><span class="w"> </span><span class="n">set_next_edge</span><span class="p">(</span><span class="kt">size_t</span><span class="w"> </span><span class="n">index</span><span class="p">,</span><span class="w"> </span><span class="n">Edge</span><span class="w"> </span><span class="n">edge</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">update_topological_nr</span><span class="p">(</span><span class="n">edge</span><span class="p">);</span>
<span class="w">    </span><span class="n">next_edges_</span><span class="p">[</span><span class="n">index</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">move</span><span class="p">(</span><span class="n">edge</span><span class="p">);</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="kt">void</span><span class="w"> </span><span class="n">add_next_edge</span><span class="p">(</span><span class="n">Edge</span><span class="w"> </span><span class="n">edge</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">update_topological_nr</span><span class="p">(</span><span class="n">edge</span><span class="p">);</span>
<span class="w">    </span><span class="n">next_edges_</span><span class="p">.</span><span class="n">emplace_back</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">move</span><span class="p">(</span><span class="n">edge</span><span class="p">));</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="kt">void</span><span class="w"> </span><span class="n">set_next_edges</span><span class="p">(</span><span class="n">edge_list</span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="n">next_edges</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">next_edges_</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">move</span><span class="p">(</span><span class="n">next_edges</span><span class="p">);</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="k">auto</span><span class="o">&amp;</span><span class="w"> </span><span class="n">next_edge</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="n">next_edges_</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="n">update_topological_nr</span><span class="p">(</span><span class="n">next_edge</span><span class="p">);</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="k">const</span><span class="w"> </span><span class="n">Edge</span><span class="o">&amp;</span><span class="w"> </span><span class="n">next_edge</span><span class="p">(</span><span class="kt">size_t</span><span class="w"> </span><span class="n">index</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="k">noexcept</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">next_edges_</span><span class="p">[</span><span class="n">index</span><span class="p">];</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="k">const</span><span class="w"> </span><span class="n">edge_list</span><span class="o">&amp;</span><span class="w"> </span><span class="n">next_edges</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="k">noexcept</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">next_edges_</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="n">edge_list</span><span class="o">&amp;</span><span class="w"> </span><span class="n">next_edges</span><span class="p">()</span><span class="w"> </span><span class="k">noexcept</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">next_edges_</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="kt">uint32_t</span><span class="w"> </span><span class="n">num_outputs</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="k">noexcept</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">next_edges_</span><span class="p">.</span><span class="n">size</span><span class="p">();</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="c1">// Miscellaneous Methods</span>
<span class="w">  </span><span class="c1">//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>

<span class="w">  </span><span class="kt">uint64_t</span><span class="w"> </span><span class="n">sequence_nr</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="k">noexcept</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">sequence_nr_</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="kt">void</span><span class="w"> </span><span class="n">set_sequence_nr</span><span class="p">(</span><span class="kt">uint64_t</span><span class="w"> </span><span class="n">sequence_nr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">sequence_nr_</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">sequence_nr</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="c1">// NOTE [ Topological Number ]</span>
<span class="w">  </span><span class="c1">//</span>
<span class="w">  </span><span class="c1">// topological_nr is used to prune branches in the DAG during autograd</span>
<span class="w">  </span><span class="c1">// discovery as maintaining topological_nr helps us check in O(1) if there</span>
<span class="w">  </span><span class="c1">// does NOT exist a directed path between two nodes.</span>
<span class="w">  </span><span class="c1">//</span>
<span class="w">  </span><span class="c1">// The topological order number of this `Node` representing the length of the</span>
<span class="w">  </span><span class="c1">// longest possible path from this Node to any leaf node. If you are leaf</span>
<span class="w">  </span><span class="c1">// node, aka AccumulateGrad, this will be zero. This value has the property</span>
<span class="w">  </span><span class="c1">// that For every pair of nodes X, Y in G, existence of a directed path from X</span>
<span class="w">  </span><span class="c1">// to Y implies topo_nr(X) &gt; topo_nr(Y). The converse is not true, however, so</span>
<span class="w">  </span><span class="c1">// we cannot prove existence of a path from X to Y, only non-existence.</span>
<span class="w">  </span><span class="c1">//</span>
<span class="w">  </span><span class="c1">// One assumption we make when using topo_nr is that once a node</span>
<span class="w">  </span><span class="c1">// has been used, i.e., has a parent node, its own topo_nr does not change</span>
<span class="w">  </span><span class="c1">// we have added some checks with the `has_parent_` field to enforce this.</span>
<span class="w">  </span><span class="c1">//</span>
<span class="w">  </span><span class="c1">// What NOT to do:</span>
<span class="w">  </span><span class="c1">//</span>
<span class="w">  </span><span class="c1">//   1) 2 -&gt; 1 -&gt; 0               In this diagram we label nodes with their</span>
<span class="w">  </span><span class="c1">//   topo_nr.</span>
<span class="w">  </span><span class="c1">//      2 -&gt; 1 -&gt; 0               We have two simple graphs that can each</span>
<span class="w">  </span><span class="c1">//      arise from</span>
<span class="w">  </span><span class="c1">//                                `t.exp().exp()`, for example.</span>
<span class="w">  </span><span class="c1">//   2)        2 -&gt; 1 -&gt; 0</span>
<span class="w">  </span><span class="c1">//            /</span>
<span class="w">  </span><span class="c1">//      2 -&gt; 1 -&gt; 0               We add 2 as a next edge to 1 even though 1</span>
<span class="w">  </span><span class="c1">//      already</span>
<span class="w">  </span><span class="c1">//                                has a parent.</span>
<span class="w">  </span><span class="c1">//   3)        2 -&gt; 1 -&gt; 0</span>
<span class="w">  </span><span class="c1">//            /</span>
<span class="w">  </span><span class="c1">//      2 -&gt; 3 -&gt; 0               2 &lt; 3, yet there exists a path from 2 to 3!</span>
<span class="w">  </span><span class="c1">//</span>
<span class="w">  </span><span class="kt">uint64_t</span><span class="w"> </span><span class="n">topological_nr</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="k">noexcept</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">has_parent_</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nb">true</span><span class="p">;</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">topological_nr_</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="c1">// assigning a node as a parent to this node</span>
<span class="w">  </span><span class="kt">void</span><span class="w"> </span><span class="n">assign_parent</span><span class="p">();</span>

<span class="w">  </span><span class="kt">uint64_t</span><span class="w"> </span><span class="nf">thread_id</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="k">noexcept</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">thread_id_</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="k">virtual</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="w"> </span><span class="nf">name</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>

<span class="w">  </span><span class="kt">bool</span><span class="w"> </span><span class="nf">should_compute_output</span><span class="p">(</span><span class="kt">size_t</span><span class="w"> </span><span class="n">output_edge_index</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">TORCH_CHECK</span><span class="p">(</span><span class="n">output_edge_index</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">num_outputs</span><span class="p">(),</span><span class="w"> </span><span class="s">&quot;Index out of range&quot;</span><span class="p">);</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">next_edges_</span><span class="p">[</span><span class="n">output_edge_index</span><span class="p">].</span><span class="n">is_valid</span><span class="p">();</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="kt">bool</span><span class="w"> </span><span class="nf">should_compute_output</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">initializer_list</span><span class="o">&lt;</span><span class="n">IndexRange</span><span class="o">&gt;</span><span class="w"> </span><span class="n">idxs</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">any_of</span><span class="p">(</span><span class="n">idxs</span><span class="p">.</span><span class="n">begin</span><span class="p">(),</span><span class="w"> </span><span class="n">idxs</span><span class="p">.</span><span class="n">end</span><span class="p">(),</span><span class="w"> </span><span class="p">[</span><span class="k">this</span><span class="p">](</span><span class="n">IndexRange</span><span class="w"> </span><span class="n">range</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="k">auto</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">irange</span><span class="p">(</span><span class="n">range</span><span class="p">.</span><span class="n">first</span><span class="p">,</span><span class="w"> </span><span class="n">range</span><span class="p">.</span><span class="n">second</span><span class="p">))</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">should_compute_output</span><span class="p">(</span><span class="n">i</span><span class="p">))</span>
<span class="w">          </span><span class="k">return</span><span class="w"> </span><span class="nb">true</span><span class="p">;</span>
<span class="w">      </span><span class="p">}</span>
<span class="w">      </span><span class="k">return</span><span class="w"> </span><span class="nb">false</span><span class="p">;</span>
<span class="w">    </span><span class="p">});</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="kt">bool</span><span class="w"> </span><span class="nf">task_should_compute_output</span><span class="p">(</span><span class="kt">size_t</span><span class="w"> </span><span class="n">output_edge_index</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">TORCH_CHECK</span><span class="p">(</span><span class="n">output_edge_index</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">num_outputs</span><span class="p">(),</span><span class="w"> </span><span class="s">&quot;Index out of range&quot;</span><span class="p">);</span>
<span class="w">    </span><span class="k">const</span><span class="w"> </span><span class="k">auto</span><span class="o">&amp;</span><span class="w"> </span><span class="n">next</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">next_edges_</span><span class="p">[</span><span class="n">output_edge_index</span><span class="p">];</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">next</span><span class="p">.</span><span class="n">is_valid</span><span class="p">())</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="k">const</span><span class="w"> </span><span class="k">auto</span><span class="w"> </span><span class="n">exec_info</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">get_current_graph_task_exec_info</span><span class="p">();</span>
<span class="w">      </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">exec_info</span><span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="o">!</span><span class="n">exec_info</span><span class="o">-&gt;</span><span class="n">empty</span><span class="p">())</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="k">auto</span><span class="w"> </span><span class="n">it</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">exec_info</span><span class="o">-&gt;</span><span class="n">find</span><span class="p">(</span><span class="n">next</span><span class="p">.</span><span class="n">function</span><span class="p">.</span><span class="n">get</span><span class="p">());</span>
<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">it</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="n">exec_info</span><span class="o">-&gt;</span><span class="n">end</span><span class="p">()</span><span class="w"> </span><span class="o">||</span><span class="w"> </span><span class="o">!</span><span class="n">it</span><span class="o">-&gt;</span><span class="n">second</span><span class="p">.</span><span class="n">should_execute</span><span class="p">())</span><span class="w"> </span><span class="p">{</span>
<span class="w">          </span><span class="k">return</span><span class="w"> </span><span class="nb">false</span><span class="p">;</span><span class="w"> </span><span class="c1">// this edge is not needed for the current graph_task</span>
<span class="w">        </span><span class="p">}</span>
<span class="w">      </span><span class="p">}</span>
<span class="w">      </span><span class="k">return</span><span class="w"> </span><span class="nb">true</span><span class="p">;</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="nb">false</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="kt">bool</span><span class="w"> </span><span class="nf">task_should_compute_output</span><span class="p">(</span>
<span class="w">      </span><span class="n">std</span><span class="o">::</span><span class="n">initializer_list</span><span class="o">&lt;</span><span class="n">IndexRange</span><span class="o">&gt;</span><span class="w"> </span><span class="n">idxs</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">any_of</span><span class="p">(</span><span class="n">idxs</span><span class="p">.</span><span class="n">begin</span><span class="p">(),</span><span class="w"> </span><span class="n">idxs</span><span class="p">.</span><span class="n">end</span><span class="p">(),</span><span class="w"> </span><span class="p">[</span><span class="k">this</span><span class="p">](</span><span class="n">IndexRange</span><span class="w"> </span><span class="n">range</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="k">auto</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">irange</span><span class="p">(</span><span class="n">range</span><span class="p">.</span><span class="n">first</span><span class="p">,</span><span class="w"> </span><span class="n">range</span><span class="p">.</span><span class="n">second</span><span class="p">))</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">task_should_compute_output</span><span class="p">(</span><span class="n">i</span><span class="p">))</span>
<span class="w">          </span><span class="k">return</span><span class="w"> </span><span class="nb">true</span><span class="p">;</span>
<span class="w">      </span><span class="p">}</span>
<span class="w">      </span><span class="k">return</span><span class="w"> </span><span class="nb">false</span><span class="p">;</span>
<span class="w">    </span><span class="p">});</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="n">PyObject</span><span class="o">*</span><span class="w"> </span><span class="nf">pyobj</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="k">noexcept</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">pyobj_</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="kt">void</span><span class="w"> </span><span class="nf">set_pyobj</span><span class="p">(</span><span class="n">PyObject</span><span class="o">*</span><span class="w"> </span><span class="n">pyobj</span><span class="p">)</span><span class="w"> </span><span class="k">noexcept</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">pyobj_</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">pyobj</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="n">AnomalyMetadata</span><span class="o">*</span><span class="w"> </span><span class="nf">metadata</span><span class="p">()</span><span class="w"> </span><span class="k">noexcept</span><span class="p">;</span>

<span class="w">  </span><span class="c1">// Hook API</span>
<span class="w">  </span><span class="c1">//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>

<span class="w">  </span><span class="kt">uintptr_t</span><span class="w"> </span><span class="nf">add_post_hook</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">unique_ptr</span><span class="o">&lt;</span><span class="n">FunctionPostHook</span><span class="o">&gt;&amp;&amp;</span><span class="w"> </span><span class="n">post_hook</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">post_hooks_</span><span class="p">.</span><span class="n">emplace_back</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">move</span><span class="p">(</span><span class="n">post_hook</span><span class="p">));</span>
<span class="w">    </span><span class="c1">// Use the raw pointer as the unique key to identify this hook. This key</span>
<span class="w">    </span><span class="c1">// can then be used in del_post_hook(key) to remove this hook.</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="k">reinterpret_cast</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="kt">uintptr_t</span><span class="o">&gt;</span><span class="p">(</span><span class="n">post_hooks_</span><span class="p">.</span><span class="n">back</span><span class="p">().</span><span class="n">get</span><span class="p">());</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">unique_ptr</span><span class="o">&lt;</span><span class="n">FunctionPostHook</span><span class="o">&gt;&gt;&amp;</span><span class="w"> </span><span class="n">post_hooks</span><span class="p">()</span>
<span class="w">      </span><span class="k">const</span><span class="w"> </span><span class="k">noexcept</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">post_hooks_</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="c1">// delete a post hook matching the key</span>
<span class="w">  </span><span class="kt">bool</span><span class="w"> </span><span class="n">del_post_hook</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="kt">uintptr_t</span><span class="o">&amp;</span><span class="w"> </span><span class="n">key</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="k">auto</span><span class="w"> </span><span class="n">it</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">post_hooks_</span><span class="p">.</span><span class="n">begin</span><span class="p">();</span><span class="w"> </span><span class="n">it</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">post_hooks_</span><span class="p">.</span><span class="n">end</span><span class="p">();</span><span class="w"> </span><span class="o">++</span><span class="n">it</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">key</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">reinterpret_cast</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="kt">uintptr_t</span><span class="o">&gt;</span><span class="p">(</span><span class="n">it</span><span class="o">-&gt;</span><span class="n">get</span><span class="p">()))</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="n">post_hooks_</span><span class="p">.</span><span class="n">erase</span><span class="p">(</span><span class="n">it</span><span class="p">);</span>
<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="nb">true</span><span class="p">;</span>
<span class="w">      </span><span class="p">}</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="nb">false</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">unique_ptr</span><span class="o">&lt;</span><span class="n">FunctionPostHook</span><span class="o">&gt;&gt;&amp;</span><span class="w"> </span><span class="n">post_hooks</span><span class="p">()</span><span class="w"> </span><span class="k">noexcept</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">post_hooks_</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="kt">void</span><span class="w"> </span><span class="n">add_pre_hook</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">unique_ptr</span><span class="o">&lt;</span><span class="n">FunctionPreHook</span><span class="o">&gt;&amp;&amp;</span><span class="w"> </span><span class="n">pre_hook</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">pre_hooks_</span><span class="p">.</span><span class="n">emplace_back</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">move</span><span class="p">(</span><span class="n">pre_hook</span><span class="p">));</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="kt">void</span><span class="w"> </span><span class="n">add_tensor_pre_hook</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">unique_ptr</span><span class="o">&lt;</span><span class="n">FunctionPreHook</span><span class="o">&gt;&amp;&amp;</span><span class="w"> </span><span class="n">pre_hook</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">tensor_pre_hooks_</span><span class="p">.</span><span class="n">emplace_back</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">move</span><span class="p">(</span><span class="n">pre_hook</span><span class="p">));</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="kt">void</span><span class="w"> </span><span class="n">add_retains_grad_hook</span><span class="p">(</span>
<span class="w">      </span><span class="n">std</span><span class="o">::</span><span class="n">unique_ptr</span><span class="o">&lt;</span><span class="n">FunctionPreHook</span><span class="o">&gt;&amp;&amp;</span><span class="w"> </span><span class="n">pre_hook</span><span class="p">,</span>
<span class="w">      </span><span class="kt">size_t</span><span class="w"> </span><span class="n">output_idx</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">retains_grad_hooks_</span><span class="p">[</span><span class="n">output_idx</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">move</span><span class="p">(</span><span class="n">pre_hook</span><span class="p">);</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">unique_ptr</span><span class="o">&lt;</span><span class="n">FunctionPreHook</span><span class="o">&gt;</span><span class="w"> </span><span class="n">pop_retains_grad_hook</span><span class="p">(</span><span class="kt">size_t</span><span class="w"> </span><span class="n">output_idx</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">ret</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">move</span><span class="p">(</span><span class="n">retains_grad_hooks_</span><span class="p">[</span><span class="n">output_idx</span><span class="p">]);</span>
<span class="w">    </span><span class="n">retains_grad_hooks_</span><span class="p">.</span><span class="n">erase</span><span class="p">(</span><span class="n">output_idx</span><span class="p">);</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">ret</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">unique_ptr</span><span class="o">&lt;</span><span class="n">FunctionPreHook</span><span class="o">&gt;&gt;&amp;</span><span class="w"> </span><span class="n">pre_hooks</span><span class="p">()</span>
<span class="w">      </span><span class="k">const</span><span class="w"> </span><span class="k">noexcept</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">pre_hooks_</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">unique_ptr</span><span class="o">&lt;</span><span class="n">FunctionPreHook</span><span class="o">&gt;&gt;&amp;</span><span class="w"> </span><span class="n">pre_hooks</span><span class="p">()</span><span class="w"> </span><span class="k">noexcept</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">pre_hooks_</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="k">virtual</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">unique_ptr</span><span class="o">&lt;</span><span class="n">FunctionPreHook</span><span class="o">&gt;&gt;&amp;</span>
<span class="w">  </span><span class="n">tensor_pre_hooks</span><span class="p">()</span><span class="w"> </span><span class="k">noexcept</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">tensor_pre_hooks_</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="k">virtual</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">unique_ptr</span><span class="o">&lt;</span><span class="n">PostAccumulateGradHook</span><span class="o">&gt;&amp;</span><span class="w"> </span><span class="n">tensor_post_acc_grad_hooks</span><span class="p">()</span>
<span class="w">      </span><span class="k">const</span><span class="w"> </span><span class="k">noexcept</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">static</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">unique_ptr</span><span class="o">&lt;</span><span class="n">PostAccumulateGradHook</span><span class="o">&gt;</span><span class="w"> </span><span class="n">empty</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">nullptr</span><span class="p">;</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">empty</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">unordered_map</span><span class="o">&lt;</span><span class="kt">size_t</span><span class="p">,</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">unique_ptr</span><span class="o">&lt;</span><span class="n">FunctionPreHook</span><span class="o">&gt;&gt;&amp;</span>
<span class="w">  </span><span class="n">retains_grad_hooks</span><span class="p">()</span><span class="w"> </span><span class="k">noexcept</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">retains_grad_hooks_</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="c1">// Customization Points for Subclasses</span>
<span class="w">  </span><span class="c1">//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>

<span class="w">  </span><span class="k">virtual</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="n">release_variables</span><span class="p">()</span><span class="w"> </span><span class="p">{}</span>

<span class="w">  </span><span class="k">virtual</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="n">will_release_variables</span><span class="p">()</span><span class="w"> </span><span class="p">{}</span>

<span class="w">  </span><span class="k">virtual</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">is_traceable</span><span class="p">()</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="nb">false</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="k">virtual</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">passes_state_transparently</span><span class="p">()</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="nb">false</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="c1">// see [Note: Compiled Autograd]</span>
<span class="w">  </span><span class="c1">// Used by compiled autograd to</span>
<span class="w">  </span><span class="c1">//   1) Extract tensors/symint args</span>
<span class="w">  </span><span class="c1">//   2) Collect node information for specialization and caching</span>
<span class="w">  </span><span class="c1">// Implementations in subclasses should call args.collect() with all node</span>
<span class="w">  </span><span class="c1">// attrs. These functions are only called during backward.</span>
<span class="w">  </span><span class="k">virtual</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="n">compiled_args</span><span class="p">(</span><span class="n">CompiledNodeArgs</span><span class="o">&amp;</span><span class="w"> </span><span class="n">args</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">TORCH_CHECK_NOT_IMPLEMENTED</span><span class="p">(</span>
<span class="w">        </span><span class="nb">false</span><span class="p">,</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="p">(</span><span class="s">&quot;compiled_args not implemented: &quot;</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">name</span><span class="p">());</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="c1">// Used by compiled autograd to call apply() with different saved tensors</span>
<span class="w">  </span><span class="c1">// Implementations should call saved.before() on all attrs, then apply(), then</span>
<span class="w">  </span><span class="c1">// saved.after() on all attrs in the same order.</span>
<span class="w">  </span><span class="k">virtual</span><span class="w"> </span><span class="n">variable_list</span><span class="w"> </span><span class="n">apply_with_saved</span><span class="p">(</span>
<span class="w">      </span><span class="k">const</span><span class="w"> </span><span class="n">variable_list</span><span class="o">&amp;</span><span class="w"> </span><span class="n">inputs</span><span class="p">,</span>
<span class="w">      </span><span class="n">SwapSavedVariables</span><span class="o">&amp;</span><span class="w"> </span><span class="n">saved</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">TORCH_CHECK_NOT_IMPLEMENTED</span><span class="p">(</span>
<span class="w">        </span><span class="nb">false</span><span class="p">,</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="p">(</span><span class="s">&quot;apply_with_saved not implemented: &quot;</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">name</span><span class="p">());</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="c1">// If this node is the AOTBackward node produced by torch.compile.</span>
<span class="w">  </span><span class="c1">// Compiled Autograd special-cases on this information.</span>
<span class="w">  </span><span class="k">virtual</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">is_aot_backward</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="nb">false</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>

<span class="w"> </span><span class="k">protected</span><span class="o">:</span>
<span class="w">  </span><span class="k">virtual</span><span class="w"> </span><span class="n">variable_list</span><span class="w"> </span><span class="n">apply</span><span class="p">(</span><span class="n">variable_list</span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="n">inputs</span><span class="p">)</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>

<span class="w">  </span><span class="n">variable_list</span><span class="w"> </span><span class="nf">traced_apply</span><span class="p">(</span><span class="n">variable_list</span><span class="w"> </span><span class="n">inputs</span><span class="p">);</span>

<span class="w">  </span><span class="c1">// Sequence number used to correlate backward nodes with forward ops in the</span>
<span class="w">  </span><span class="c1">// profiler and provide determinism in the engine.</span>
<span class="w">  </span><span class="c1">// NOLINTNEXTLINE(cppcoreguidelines-avoid-const-or-ref-data-members)</span>
<span class="w">  </span><span class="kt">uint64_t</span><span class="w"> </span><span class="n">sequence_nr_</span><span class="p">;</span>

<span class="w">  </span><span class="c1">// See NOTE [ Topological Number ]</span>
<span class="w">  </span><span class="kt">uint64_t</span><span class="w"> </span><span class="n">topological_nr_</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>

<span class="w">  </span><span class="c1">// Tracks whether this node has been added as the next_edge of another node</span>
<span class="w">  </span><span class="c1">// via set_next_edge(s), which always calls topological_nr() of all its</span>
<span class="w">  </span><span class="c1">// children See NOTE [ Topological Number ] for why we need this.</span>
<span class="w">  </span><span class="k">mutable</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">has_parent_</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nb">false</span><span class="p">;</span>

<span class="w">  </span><span class="c1">// Id of the thread that created the instance</span>
<span class="w">  </span><span class="kt">uint64_t</span><span class="w"> </span><span class="n">thread_id_</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>

<span class="w">  </span><span class="c1">// Note [Thread Safety on Autograd Node]</span>
<span class="w">  </span><span class="c1">// ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
<span class="w">  </span><span class="c1">// Autograd Engine let the owning thread which calls Engine::execute to drive</span>
<span class="w">  </span><span class="c1">// the GraphTask execution, there might be cases that part of the GraphTask is</span>
<span class="w">  </span><span class="c1">// shared across different `backward()` or `grad()` calls, i.e. fork new</span>
<span class="w">  </span><span class="c1">// threads in the middle of the forward and call `backward()` separately from</span>
<span class="w">  </span><span class="c1">// different threads. We need to protect the thread safety on NodeTask to</span>
<span class="w">  </span><span class="c1">// prevent data racing on shared variables read/write.</span>
<span class="w">  </span><span class="c1">//</span>
<span class="w">  </span><span class="c1">// NB: This is only needed for Autograd Nodes that runs on CPU, technically</span>
<span class="w">  </span><span class="c1">// &quot;CUDA&quot;, &quot;XLA&quot; nodes don&#39;t need locking because device threads are always</span>
<span class="w">  </span><span class="c1">// single threaded.</span>
<span class="w">  </span><span class="c1">//</span>
<span class="w">  </span><span class="c1">// Here we add a thread mutex to help protect the Node&#39;s thread safety, so</span>
<span class="w">  </span><span class="c1">// that different threads cannot race the shared data when executing the same</span>
<span class="w">  </span><span class="c1">// NodeTask from multiple CPU threads. It IS the user/developer responsibility</span>
<span class="w">  </span><span class="c1">// to take advantage of this mutex to protect the thread safety of their</span>
<span class="w">  </span><span class="c1">// autograd Node. The general strategy of thread safety on autograd Node:</span>
<span class="w">  </span><span class="c1">//</span>
<span class="w">  </span><span class="c1">// 1. User should lock the mutex during Node::release_variables() if the Node</span>
<span class="w">  </span><span class="c1">// needs</span>
<span class="w">  </span><span class="c1">//    to release the variables on the fly, this serve the purpose that when we</span>
<span class="w">  </span><span class="c1">//    release saved_variables from one thread, no other threads can release</span>
<span class="w">  </span><span class="c1">//    the saved variables concurrently. call the Node::apply(),</span>
<span class="w">  </span><span class="c1">// 2. User should lock the mutex during Node::apply(), this is to ensure Node</span>
<span class="w">  </span><span class="c1">// that</span>
<span class="w">  </span><span class="c1">//    writing to the shared variable are not racing across threads (i.e.</span>
<span class="w">  </span><span class="c1">//    AccumulateGrad and custom C++ Autograd Node if writing to shared</span>
<span class="w">  </span><span class="c1">//    variables )</span>
<span class="w">  </span><span class="c1">// 3. item 2 and item 3 should work together so that when we release saved</span>
<span class="w">  </span><span class="c1">// variables</span>
<span class="w">  </span><span class="c1">//    from one thread, no other threads can call Node::apply(), this ensures</span>
<span class="w">  </span><span class="c1">//    the variable references from other threads aren&#39;t dangling.</span>
<span class="w">  </span><span class="c1">// 4. if the Node don&#39;t release any variables and no shared data read/write in</span>
<span class="w">  </span><span class="c1">// the Node</span>
<span class="w">  </span><span class="c1">//    i.e. purely functional, user don&#39;t need to lock the mutex</span>
<span class="w">  </span><span class="c1">//</span>
<span class="w">  </span><span class="c1">// This way we could protect the thread safety on Autograd Node, but we could</span>
<span class="w">  </span><span class="c1">// still not protect the thread safety on Node pre/post C++ hooks (python</span>
<span class="w">  </span><span class="c1">// hooks are automatically thread safe), we rely on the user to write thread</span>
<span class="w">  </span><span class="c1">// safe C++ hooks if they want the hook to be correctly applied in</span>
<span class="w">  </span><span class="c1">// multithreading environment.</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">mutex</span><span class="w"> </span><span class="n">mutex_</span><span class="p">;</span>

<span class="w">  </span><span class="n">edge_list</span><span class="w"> </span><span class="n">next_edges_</span><span class="p">;</span>
<span class="w">  </span><span class="n">PyObject</span><span class="o">*</span><span class="w"> </span><span class="n">pyobj_</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">nullptr</span><span class="p">;</span><span class="w"> </span><span class="c1">// weak reference</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">unique_ptr</span><span class="o">&lt;</span><span class="n">AnomalyMetadata</span><span class="o">&gt;</span><span class="w"> </span><span class="n">anomaly_metadata_</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">nullptr</span><span class="p">;</span>

<span class="w">  </span><span class="c1">// NOTE [Hooks ordering]</span>
<span class="w">  </span><span class="c1">// We have 3 separate fields for pre hooks registered to the autograd nodes</span>
<span class="w">  </span><span class="c1">// because the conditions under which they execute are different, and we</span>
<span class="w">  </span><span class="c1">// want more fine-grained control over the order in which different types</span>
<span class="w">  </span><span class="c1">// of hooks are executed.</span>
<span class="w">  </span><span class="c1">// - pre_hooks  are only executed when the node itself is executed</span>
<span class="w">  </span><span class="c1">// - tensor_pre_hook is executed as long as the engine traverses over it</span>
<span class="w">  </span><span class="c1">//   even if that node won&#39;t be executed.</span>
<span class="w">  </span><span class="c1">// - retains_grad_hook are like tensor_pre_hooks except they are always</span>
<span class="w">  </span><span class="c1">//   ordered after all other tensor pre hooks</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">unique_ptr</span><span class="o">&lt;</span><span class="n">FunctionPreHook</span><span class="o">&gt;&gt;</span><span class="w"> </span><span class="n">pre_hooks_</span><span class="p">;</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">unique_ptr</span><span class="o">&lt;</span><span class="n">FunctionPreHook</span><span class="o">&gt;&gt;</span><span class="w"> </span><span class="n">tensor_pre_hooks_</span><span class="p">;</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">unordered_map</span><span class="o">&lt;</span><span class="kt">size_t</span><span class="p">,</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">unique_ptr</span><span class="o">&lt;</span><span class="n">FunctionPreHook</span><span class="o">&gt;&gt;</span>
<span class="w">      </span><span class="n">retains_grad_hooks_</span><span class="p">;</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">unique_ptr</span><span class="o">&lt;</span><span class="n">FunctionPostHook</span><span class="o">&gt;&gt;</span><span class="w"> </span><span class="n">post_hooks_</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">SmallVector</span><span class="o">&lt;</span><span class="n">InputMetadata</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="o">&gt;</span><span class="w"> </span><span class="n">input_metadata_</span><span class="p">;</span>
<span class="p">};</span>

<span class="k">struct</span><span class="w"> </span><span class="nc">TraceableFunction</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="k">public</span><span class="w"> </span><span class="n">Node</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="k">using</span><span class="w"> </span><span class="n">Node</span><span class="o">::</span><span class="n">Node</span><span class="p">;</span>
<span class="w">  </span><span class="kt">bool</span><span class="w"> </span><span class="nf">is_traceable</span><span class="p">()</span><span class="w"> </span><span class="k">final</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="nb">true</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>
<span class="p">};</span>

<span class="c1">//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
<span class="c1">//                       Associated Free Nodes</span>
<span class="c1">//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>

<span class="k">namespace</span><span class="w"> </span><span class="nn">detail</span><span class="w"> </span><span class="p">{</span>
<span class="c1">// Implementation of `collect_next_edges` (see below).</span>
<span class="k">struct</span><span class="w"> </span><span class="nc">MakeNextFunctionList</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="n">IterArgs</span><span class="o">&lt;</span><span class="n">MakeNextFunctionList</span><span class="o">&gt;</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">edge_list</span><span class="w"> </span><span class="n">next_edges</span><span class="p">;</span>
<span class="w">  </span><span class="k">using</span><span class="w"> </span><span class="n">IterArgs</span><span class="o">&lt;</span><span class="n">MakeNextFunctionList</span><span class="o">&gt;::</span><span class="k">operator</span><span class="p">();</span>
<span class="w">  </span><span class="kt">void</span><span class="w"> </span><span class="nf">operator</span><span class="p">()(</span><span class="k">const</span><span class="w"> </span><span class="n">Variable</span><span class="o">&amp;</span><span class="w"> </span><span class="n">variable</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">variable</span><span class="p">.</span><span class="n">defined</span><span class="p">())</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="n">next_edges</span><span class="p">.</span><span class="n">emplace_back</span><span class="p">(</span><span class="n">impl</span><span class="o">::</span><span class="n">gradient_edge</span><span class="p">(</span><span class="n">variable</span><span class="p">));</span>
<span class="w">    </span><span class="p">}</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="n">next_edges</span><span class="p">.</span><span class="n">emplace_back</span><span class="p">();</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="kt">void</span><span class="w"> </span><span class="nf">operator</span><span class="p">()(</span><span class="k">const</span><span class="w"> </span><span class="n">Variable</span><span class="o">*</span><span class="w"> </span><span class="n">variable</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">operator</span><span class="p">()(</span><span class="o">*</span><span class="n">variable</span><span class="p">);</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="kt">void</span><span class="w"> </span><span class="nf">operator</span><span class="p">()(</span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">Variable</span><span class="o">&gt;&amp;</span><span class="w"> </span><span class="n">variable</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">variable</span><span class="p">.</span><span class="n">has_value</span><span class="p">())</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="k">operator</span><span class="p">()(</span><span class="o">*</span><span class="n">variable</span><span class="p">);</span>
<span class="w">    </span><span class="p">}</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="n">next_edges</span><span class="p">.</span><span class="n">emplace_back</span><span class="p">();</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">  </span><span class="p">}</span>
<span class="p">};</span>
<span class="p">}</span><span class="w"> </span><span class="c1">// namespace detail</span>

<span class="kr">inline</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="n">create_gradient_edge</span><span class="p">(</span>
<span class="w">    </span><span class="n">Variable</span><span class="o">&amp;</span><span class="w"> </span><span class="n">variable</span><span class="p">,</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">Node</span><span class="o">&gt;</span><span class="w"> </span><span class="n">function</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="c1">// Copy before move.</span>
<span class="w">  </span><span class="k">const</span><span class="w"> </span><span class="k">auto</span><span class="w"> </span><span class="n">input_nr</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">function</span><span class="o">-&gt;</span><span class="n">add_input_metadata</span><span class="p">(</span><span class="n">variable</span><span class="p">);</span>
<span class="w">  </span><span class="n">impl</span><span class="o">::</span><span class="n">set_gradient_edge</span><span class="p">(</span><span class="n">variable</span><span class="p">,</span><span class="w"> </span><span class="p">{</span><span class="n">std</span><span class="o">::</span><span class="n">move</span><span class="p">(</span><span class="n">function</span><span class="p">),</span><span class="w"> </span><span class="n">input_nr</span><span class="p">});</span>
<span class="p">}</span>

<span class="kr">inline</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">any_variable_requires_grad</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">variable_list</span><span class="o">&amp;</span><span class="w"> </span><span class="n">variables</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">any_of</span><span class="p">(</span>
<span class="w">      </span><span class="n">variables</span><span class="p">.</span><span class="n">begin</span><span class="p">(),</span><span class="w"> </span><span class="n">variables</span><span class="p">.</span><span class="n">end</span><span class="p">(),</span><span class="w"> </span><span class="p">[](</span><span class="k">const</span><span class="w"> </span><span class="n">Variable</span><span class="o">&amp;</span><span class="w"> </span><span class="n">variable</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="n">variable</span><span class="p">.</span><span class="n">defined</span><span class="p">()</span><span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="n">variable</span><span class="p">.</span><span class="n">requires_grad</span><span class="p">();</span>
<span class="w">      </span><span class="p">});</span>
<span class="p">}</span>

<span class="k">template</span><span class="w"> </span><span class="o">&lt;</span><span class="k">typename</span><span class="p">...</span><span class="w"> </span><span class="n">Variables</span><span class="o">&gt;</span>
<span class="n">edge_list</span><span class="w"> </span><span class="n">collect_next_edges</span><span class="p">(</span><span class="n">Variables</span><span class="o">&amp;&amp;</span><span class="p">...</span><span class="w"> </span><span class="n">variables</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">detail</span><span class="o">::</span><span class="n">MakeNextFunctionList</span><span class="w"> </span><span class="n">make</span><span class="p">;</span>
<span class="w">  </span><span class="n">make</span><span class="p">.</span><span class="n">apply</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">forward</span><span class="o">&lt;</span><span class="n">Variables</span><span class="o">&gt;</span><span class="p">(</span><span class="n">variables</span><span class="p">)...);</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">move</span><span class="p">(</span><span class="n">make</span><span class="p">.</span><span class="n">next_edges</span><span class="p">);</span>
<span class="p">}</span>

<span class="k">struct</span><span class="w"> </span><span class="nc">TypeAndSize</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">TypeAndSize</span><span class="p">()</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">default</span><span class="p">;</span>
<span class="w">  </span><span class="cm">/* implicit */</span>
<span class="w">  </span><span class="n">TypeAndSize</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&amp;</span><span class="w"> </span><span class="n">t</span><span class="p">)</span>
<span class="w">      </span><span class="o">:</span><span class="w"> </span><span class="n">sym_sizes</span><span class="p">(</span><span class="n">t</span><span class="p">.</span><span class="n">sym_sizes</span><span class="p">().</span><span class="n">vec</span><span class="p">()),</span><span class="w"> </span><span class="n">options</span><span class="p">(</span><span class="n">t</span><span class="p">.</span><span class="n">options</span><span class="p">())</span><span class="w"> </span><span class="p">{}</span>

<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">zeros</span><span class="p">();</span>

<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">c10</span><span class="o">::</span><span class="n">SymInt</span><span class="o">&gt;</span><span class="w"> </span><span class="n">sym_sizes</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">TensorOptions</span><span class="w"> </span><span class="n">options</span><span class="p">;</span>
<span class="p">};</span>

<span class="p">}</span><span class="w"> </span><span class="c1">// namespace torch::autograd</span>
</pre></div>
</div>
</div>


                </article>
              
  </article>
  
              
              
                <footer class="bd-footer-article">
                  <div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item">
<div class="feedback">
  
<div class="rating">
    Rate this Page
    <div class="stars">
        
        <span class="star" data-behavior="tutorial-rating" data-count="1" data-value="1">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="2" data-value="2">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="3" data-value="3">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="4" data-value="4">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="5" data-value="5">★</span>
        
    </div>
</div>

  <div class="feedback-send">
    <button class="feedback-btn"
            onclick="openGitHubIssue()"
            data-bs-title="Create a GitHub Issue"
            data-bs-placement="bottom"
            data-bs-toggle="tooltip"
            data-gtm="feedback-btn-click">Send Feedback
    </button>
  </div>
</div>

<div class="prev-next-area">
    <a class="left-prev"
       href="file_torch_csrc_autograd_function.h.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">File function.h</p>
      </div>
    </a>
    <a class="right-next"
       href="file_torch_csrc_api_include_torch_nn_functional.h.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">File functional.h</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>

<div class="footer-info">
  <p class="copyright">
    
      
        © Copyright PyTorch Contributors.
      
      <br/>
    
  </p>

  <p class="theme-version">
    Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.15.4.
  </p>
</div>
</div>
  
</div>
                </footer>
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="file_torch_csrc_autograd_function.h.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">File function.h</p>
      </div>
    </a>
    <a class="right-next"
       href="file_torch_csrc_api_include_torch_nn_functional.h.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">File functional.h</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc">
<div class="sidebar-secondary-items sidebar-secondary__inner">
    
       <div class="sidebar-secondary-item">
    <div class="tocsection sourcelink">
      <a href="../_sources/api/program_listing_file_torch_csrc_autograd_function.h.rst.txt">
        <i class="fa-solid fa-file-lines"></i> Show Source
      </a>
    </div>
</div>
    




<div class="sidebar-secondary-item">
  <div class="sidebar-heading">PyTorch Libraries</div>
  <ul style="list-style-type: none; padding: 0; padding-bottom: 80px;">
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/ao" style="color: var(--pst-color-text-muted)">torchao</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/torchrec" style="color: var(--pst-color-text-muted)">torchrec</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/torchft" style="color: var(--pst-color-text-muted)">torchft</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/torchcodec" style="color: var(--pst-color-text-muted)">TorchCodec</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/vision" style="color: var(--pst-color-text-muted)">torchvision</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/executorch" style="color: var(--pst-color-text-muted)">ExecuTorch</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/xla" style="color: var(--pst-color-text-muted)">PyTorch on XLA Devices</a></li>
  
  </ul>
</div>

</div>
</div>
              
            
          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
  <div class="container">
    <div class="row">
      <div class="col-md-4">
        <h2>Docs</h2>
        <p>Access comprehensive developer documentation for PyTorch</p>
        <a class="with-right-arrow" href="https://docs.pytorch.org/docs/stable/index.html">View Docs</a>
      </div>

      <div class="col-md-4">
        <h2>Tutorials</h2>
        <p>Get in-depth tutorials for beginners and advanced developers</p>
        <a class="with-right-arrow" href="https://docs.pytorch.org/tutorials">View Tutorials</a>
      </div>

      <div class="col-md-4">
        <h2>Resources</h2>
        <p>Find development resources and get your questions answered</p>
        <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
      </div>
    </div>
  </div>
</div>

<footer class="site-footer">
  <div class="container footer-container">

    
    <div class="newsletter" id="newsletter">

      <p class="newsletter__title is-style-max-width-800"><strong>Stay in touch</strong> for updates, event info, and
        the latest news</p>


      <script charset="utf-8" type="text/javascript" src="//js.hsforms.net/forms/embed/v2.js"></script>
      <script>
        hbspt.forms.create({
          region: "na1",
          portalId: "8112310",
          formId: "2fb2231c-000b-4ec5-88a0-1ab242549c9e"
        });
      </script>


      <p class="newsletter__privacy">By submitting this form, I consent to receive marketing emails from the LF and its
        projects regarding their events, training, research, developments, and related announcements. I understand that
        I can unsubscribe at any time using the links in the footers of the emails I receive. <a
          href="https://www.linuxfoundation.org/privacy/">Privacy Policy</a>.</p>

    </div>
    

    <div class="lf-grid">
      <ul class="social-links">
        <li><a href="https://www.facebook.com/pytorch" target="_blank" title="PyTorch on Facebook">
            <svg xmlns="http://www.w3.org/2000/svg" viewbox="-0.51 -0.26 26.45 26.45" aria-label="Facebook">
              <path fill="currentColor"
                d="M25.497 13.075c0-2.45-.698-4.848-2.011-6.911a12.765 12.765 0 0 0-5.398-4.73A12.671 12.671 0 0 0 11.008.38a12.705 12.705 0 0 0-6.529 2.95A12.827 12.827 0 0 0 .563 9.358a12.896 12.896 0 0 0-.07 7.201 12.831 12.831 0 0 0 3.801 6.103 12.709 12.709 0 0 0 6.471 3.078v-8.957H7.53v-3.708h3.235v-2.824c0-3.213 1.903-4.988 4.813-4.988.956.014 1.909.097 2.852.25V8.67h-1.607a1.83 1.83 0 0 0-1.518.497 1.854 1.854 0 0 0-.561 1.505v2.404h3.535l-.563 3.708h-2.97v8.957a12.725 12.725 0 0 0 7.697-4.337 12.87 12.87 0 0 0 3.054-8.328z" />
            </svg>
          </a></li>
        <li><a href="https://twitter.com/pytorch" target="_blank" title="PyTorch on X">
            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 300 300" aria-label="X">
              <path fill="currentColor"
                d="M178.57 127.15 290.27 0h-26.46l-97.03 110.38L89.34 0H0l117.13 166.93L0 300.25h26.46l102.4-116.59 81.8 116.59h89.34M36.01 19.54H76.66l187.13 262.13h-40.66" />
            </svg>
          </a></li>
        <li><a href="https://www.youtube.com/pytorch" target="_blank" title="PyTorch on YouTube">
            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0.21 0.27 34.45 25.07" aria-label="YouTube">
              <path fill="currentColor"
                d="M33.729 6.084s-.327-2.33-1.317-3.356a4.691 4.691 0 0 0-3.32-1.432c-4.634-.34-11.589-.34-11.589-.34h-.014s-6.954 0-11.59.342a4.692 4.692 0 0 0-3.32 1.432c-.993 1.025-1.315 3.354-1.315 3.354a52.189 52.189 0 0 0-.331 5.473v2.566c.014 1.829.125 3.656.331 5.472 0 0 .322 2.33 1.316 3.36 1.26 1.345 2.916 1.3 3.653 1.445 2.65.26 11.263.34 11.263.34s6.96-.01 11.597-.353a4.691 4.691 0 0 0 3.32-1.432c.993-1.026 1.316-3.356 1.316-3.356.206-1.817.316-3.644.33-5.473v-2.57a52.26 52.26 0 0 0-.33-5.472zM14.076 17.232V7.729l8.951 4.768-8.95 4.735z" />
            </svg>
          </a></li>
        <li><a href="https://www.linkedin.com/company/pytorch" target="_blank" title="PyTorch on LinkedIn">
            <svg xmlns="http://www.w3.org/2000/svg" viewbox="-10.23 -10.23 531.96 531.96" aria-label="LinkedIn">
              <rect width="512" height="512" rx="0" fill="currentColor" />
              <circle fill="#000" cx="142" cy="138" r="37" />
              <path stroke="#000" stroke-width="66" d="M244 194v198M142 194v198" />
              <path fill="#000"
                d="M276 282c0-20 13-40 36-40 24 0 33 18 33 45v105h66V279c0-61-32-89-76-89-34 0-51 19-59 32" />
            </svg>
          </a></li>
        <li><a href="https://pytorch.slack.com" target="_blank" title="PyTorch Slack">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0.16 -0.03 21.19 21.19" aria-label="Slack">
              <path fill="currentColor"
                d="M4.896 13.27a2.147 2.147 0 0 1-2.141 2.142A2.147 2.147 0 0 1 .613 13.27c0-1.178.963-2.141 2.142-2.141h2.141v2.141zm1.08 0c0-1.178.962-2.141 2.141-2.141s2.142.963 2.142 2.141v5.363a2.147 2.147 0 0 1-2.142 2.141 2.147 2.147 0 0 1-2.141-2.142V13.27zm2.141-8.6a2.147 2.147 0 0 1-2.141-2.14c0-1.18.962-2.142 2.141-2.142s2.142.963 2.142 2.141v2.142H8.117zm0 1.08c1.179 0 2.141.962 2.141 2.141a2.147 2.147 0 0 1-2.141 2.142H2.755A2.147 2.147 0 0 1 .613 7.89c0-1.179.963-2.141 2.142-2.141h5.362zm8.599 2.141c0-1.179.963-2.141 2.141-2.141 1.179 0 2.143.962 2.143 2.14a2.147 2.147 0 0 1-2.142 2.142h-2.141V7.89zm-1.08 0a2.147 2.147 0 0 1-2.141 2.142 2.147 2.147 0 0 1-2.141-2.142V2.53c0-1.178.962-2.141 2.141-2.141s2.142.963 2.142 2.141v5.362zm-2.141 8.6c1.179 0 2.142.962 2.142 2.14a2.147 2.147 0 0 1-2.142 2.142 2.147 2.147 0 0 1-2.141-2.141V16.49h2.141zm0-1.08a2.147 2.147 0 0 1-2.141-2.141c0-1.179.962-2.142 2.141-2.142h5.362c1.179 0 2.142.963 2.142 2.142a2.147 2.147 0 0 1-2.142 2.142h-5.362z">
              </path>
            </svg>
          </a></li>
        <li><a href="https://pytorch.org/wechat" title="PyTorch on WeChat">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0.14 -0.17 38.02 33.02" aria-label="WeChat">
              <path fill="currentColor"
                d="M26.289 10.976a12.972 12.972 0 0 0-8.742 3.53 10.386 10.386 0 0 0-3.224 8.795c-1.326-.164-2.535-.345-3.75-.448a2.332 2.332 0 0 0-1.273.216c-1.18.666-2.311 1.418-3.652 2.255.246-1.112.405-2.087.687-3.024a1.15 1.15 0 0 0-.523-1.52C1.737 17.902.02 13.601 1.307 9.165c1.189-4.1 4.11-6.587 8.077-7.884A13.54 13.54 0 0 1 24.18 5.617a10.135 10.135 0 0 1 2.109 5.359zM10.668 9.594a1.564 1.564 0 0 0-2.095-1.472 1.52 1.52 0 0 0-.895 1.964 1.502 1.502 0 0 0 1.391.966 1.545 1.545 0 0 0 1.598-1.46v.002zm8.15-1.566a1.567 1.567 0 0 0-1.528 1.543 1.528 1.528 0 0 0 1.571 1.492 1.52 1.52 0 0 0 1.375-2.117 1.518 1.518 0 0 0-1.415-.919l-.003.001z">
              </path>
              <path fill="currentColor"
                d="M33.914 32.137c-1.075-.478-2.062-1.196-3.11-1.306-1.049-.11-2.145.494-3.24.605a10.821 10.821 0 0 1-8.781-2.864c-4.682-4.33-4.013-10.97 1.403-14.518 4.811-3.154 11.874-2.102 15.268 2.273a8.671 8.671 0 0 1-1.002 12.095c-1.046.929-1.422 1.693-.751 2.917.102.257.174.525.213.798zM21.68 20.292a1.264 1.264 0 1 0 .01-2.528 1.264 1.264 0 0 0-.01 2.528zm7.887-2.526a1.266 1.266 0 0 0-1.256 1.21 1.247 1.247 0 1 0 1.256-1.21z">
              </path>
            </svg>
          </a></li>
      </ul>
    </div>

    <div class="privacy-policy">
      <div class="copyright">
        
        <p>
          &copy; PyTorch. Copyright © The Linux Foundation®. All rights reserved. The Linux Foundation has registered
          trademarks and uses trademarks. For more information, including terms of use, privacy policy, and trademark
          usage, please see our <a href="https://www.linuxfoundation.org/legal/policies">Policies</a> page. <a
            href="https://www.linuxfoundation.org/trademark-usage">Trademark Usage</a>. <a
            href="http://www.linuxfoundation.org/privacy">Privacy Policy</a>.
        </p>
        
      </div>
    </div>


  </div>
</footer>

<div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../_static/img/pytorch-x.svg">
  </div>
</div>
  
  <footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
  
    <div class="footer-items__start">
      
        <div class="footer-item">

  <p class="copyright">
    
      © Copyright PyTorch Contributors.
      <br/>
    
  </p>
</div>
      
        <div class="footer-item">

  <p class="sphinx-version">
    Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 5.3.0.
    <br/>
  </p>
</div>
      
    </div>
  
  
  
    <div class="footer-items__end">
      
        <div class="footer-item">
<p class="theme-version">
  Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.15.4.
</p></div>
      
    </div>
  
</div>

  </footer>
  <script type="application/ld+json">
    {
       "@context": "https://schema.org",
       "@type": "Article",
       "name": "Program Listing for File function.h",
       "headline": "Program Listing for File function.h",
       "description": "PyTorch Documentation. Explore PyTorch, an open-source machine learning library that accelerates the path from research prototyping to production deployment. Discover tutorials, API references, and guides to help you build and deploy deep learning models efficiently.",
       "url": "/api/program_listing_file_torch_csrc_autograd_function.h.html",
       "articleBody": "Program Listing for File function.h# \u21b0 Return to documentation for file (torch/csrc/autograd/function.h) #pragma once #include \u003ctorch/csrc/autograd/anomaly_mode.h\u003e #include \u003ctorch/csrc/autograd/edge.h\u003e #include \u003ctorch/csrc/autograd/grad_mode.h\u003e #include \u003ctorch/csrc/autograd/graph_task.h\u003e #include \u003ctorch/csrc/autograd/input_metadata.h\u003e #include \u003ctorch/csrc/autograd/saved_variable.h\u003e #include \u003ctorch/csrc/autograd/variable.h\u003e #include \u003ctorch/csrc/utils/python_stub.h\u003e #include \u003ctorch/csrc/utils/variadic.h\u003e #include \u003cATen/SequenceNumber.h\u003e #include \u003cATen/core/Tensor.h\u003e #include \u003cATen/record_function.h\u003e #include \u003cc10/util/Exception.h\u003e #include \u003cc10/util/irange.h\u003e #include \u003calgorithm\u003e #include \u003ccstdint\u003e #include \u003cinitializer_list\u003e #include \u003cmemory\u003e #include \u003cstring\u003e #include \u003cutility\u003e #include \u003cvector\u003e namespace torch::autograd { struct Edge; struct FunctionPostHook; struct FunctionPreHook; using tensor_list = std::vector\u003cat::Tensor\u003e; using variable_list = std::vector\u003cVariable\u003e; using edge_list = std::vector\u003cEdge\u003e; using saved_variable_list = std::vector\u003cSavedVariable\u003e; using ivalue_list = std::vector\u003cc10::IValue\u003e; using functional_apply_t = std::function\u003c variable_list(const variable_list\u0026, const std::vector\u003cc10::IValue\u003e\u0026)\u003e; using IndexRange = std::pair\u003csize_t, size_t\u003e; using torch::dynamo::autograd::CompiledNodeArgs; using torch::dynamo::autograd::PackedArgs; using torch::dynamo::autograd::SwapSavedVariables; // Custom deleter to prevent stack overflows. TORCH_API void deleteNode(Node* function); // Guard that sets and restores the evaluating node class NodeGuard { public: explicit NodeGuard(std::shared_ptr\u003cNode\u003e node); ~NodeGuard(); private: std::shared_ptr\u003cNode\u003e last_evaluating_node_; }; // Return the Node currently being evaluated (if any) // This is only set during the backward pass while a Node is being // executed. TORCH_API std::shared_ptr\u003cNode\u003e get_current_node(); //~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ // Node //~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ // A `Node` is an abstract class that represents an operation taking zero // or more input `Variable`s and producing zero or more output `Variable`s. All // functions in PyTorch\u0027s autograd machinery derive from this class and // override its `apply` method. Instances of such subclasses will then be // invocable via the call operator. // // Nodes in the Autograd Graph //~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ // When viewing the autograd system as a graph, `Node`s are the vertices or // nodes, connected to each other via (directed) `Edge`s, which themselves are // represented via (`Node`, input_nr) pairs. `Variable`s are the outputs to // and inputs of `Node`s, and travel between these edges during execution // of the graph. When two or more `Edge`s (from different sources) point at the // same input to a `Node`, the values produced along all of these edges are // implicitly summed prior to being forwarded to the target `Node`. // // Hierarchy //~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ // Subclasses usually represent differentiable functions as well as their // gradient operators. Note, however, that due to the very general definition // of a `Node` taking *zero* or more inputs and producing *zero* or more // outputs, uses of `Node`s are flexible and extend beyond purely // mathematical operations. For example, the `AccumulateGrad` function is a // *sink*: it takes one input, but produces no outputs, instead accumulating // the input as a side effect. At the other extreme, the `GraphRoot` function // receives no inputs from other functions, but produces multiple outputs. // // Interface //~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ // The most important method on `Node` is the call operator, which takes in // a list of variables and produces a list of variables. The precise size of // these lists can be determined with `num_inputs()` and `num_outputs()`. // `Node`s are stitched together via their `next_edge` interface, which let // you manipulate the set of outgoing edges of a `Node`. You can add an // edge with `add_next_edge()`, retrieve an edge with `next_edge(index)` and // iterate over them via the `next_edges()` method. Other methods exist for // integration with the JIT and other parts of PyTorch. Every `Node` has a // *sequence number* that increases monotonically in the order of `Node` // construction. It can be retrieved via the `sequence_nr()` method. Note that // this sequence number is *thread local*. This means that when `Node`s // `A`, `B` and `C` are created consecutively in the same thread, their // sequence numbers will be ordered `A` \u003c `B` \u003c `C`. If, however, `A` and `B` // are created in one thread and `C` is created in a new thread, there are *no // guarantees* w.r.t. the ordering of `C` relative to `A` or `B`. // See NOTE [ Sequence Number] for more details on the usages of sequence // number. //~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ struct TORCH_API Node : std::enable_shared_from_this\u003cNode\u003e { public: explicit Node(uint64_t sequence_nr, edge_list\u0026\u0026 next_edges = edge_list()) : sequence_nr_(sequence_nr), next_edges_(std::move(next_edges)) { for (const Edge\u0026 edge : next_edges_) { update_topological_nr(edge); } if (AnomalyMode::is_enabled()) { metadata()-\u003estore_stack(); // If anomaly mode is enabled and graph is constructed, then assign the // currently evaluating node as the parent of this node. // A parent is a Node where this Node is created. // We are tracking the parents to track multiple backward operations. assign_parent(); } // Store the thread_id of the forward operator. // See NOTE [ Sequence Numbers ] thread_id_ = at::RecordFunction::currentThreadId(); } explicit Node(edge_list\u0026\u0026 next_edges = edge_list()) : Node( /*sequence_nr=*/at::sequence_number::get_and_increment(), std::move(next_edges)) {} Node(const Node\u0026 other) = delete; Node(Node\u0026\u0026 other) = delete; Node\u0026 operator=(const Node\u0026 other) = delete; Node\u0026 operator=(Node\u0026\u0026 other) = delete; virtual ~Node() = default; std::shared_ptr\u003cNode\u003e getptr() { return shared_from_this(); } variable_list operator()(variable_list\u0026\u0026 inputs) { // In the first iteration of named tensors, autograd ignores names and // operates on unnamed tensors. In the long term, autograd should // probably operate with names. at::NoNamesGuard no_names_guard; #ifdef USE_ROCM // Keep track of backward pass for rocblas. at::ROCmBackwardPassGuard in_backward; #endif auto step_callbacks = at::getStepCallbacksUnlessEmpty(at::RecordScope::BACKWARD_FUNCTION); if (C10_UNLIKELY(step_callbacks.has_value())) { at::RecordFunction guard(std::move(*step_callbacks)); // Using sequence number and thread id to correlate with // the forward pass function guard.setForwardThreadId(thread_id_); if (guard.needsInputs()) { std::vector\u003cc10::IValue\u003e inputs_vec(inputs.begin(), inputs.end()); guard.before( name(), c10::ArrayRef\u003cconst c10::IValue\u003e( inputs_vec.data(), inputs_vec.size()), static_cast\u003cint64_t\u003e(sequence_nr())); } else { guard.before(name(), static_cast\u003cint64_t\u003e(sequence_nr())); } return apply(std::move(inputs)); } else { return apply(std::move(inputs)); } } // Graph Connectivity API //~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ // Inputs. NOTE: inputs of the grad_fn correspond to Tensor outputs of the // forward function. // Marker for expected undefined input struct undefined_input {}; uint32_t add_input_metadata( const at::TensorOptions\u0026 options, c10::SymIntArrayRef shape, bool is_tensor_subclass, bool is_nested, std::optional\u003cat::ScalarType\u003e grad_dtype) noexcept { uint32_t input_nr = input_metadata_.size(); auto meta_shape = MetadataShape{std::in_place_type\u003cSymIntSmallVec\u003e, shape}; input_metadata_.emplace_back( options, meta_shape, is_tensor_subclass, is_nested, grad_dtype); return input_nr; } uint32_t add_input_metadata(const at::Tensor\u0026 t) noexcept { uint32_t input_nr = input_metadata_.size(); input_metadata_.emplace_back(t); return input_nr; } uint32_t add_input_metadata(undefined_input u) noexcept { uint32_t input_nr = input_metadata_.size(); input_metadata_.emplace_back(); return input_nr; } uint32_t num_inputs() const noexcept { return input_metadata_.size(); } const InputMetadata\u0026 input_metadata(size_t index) const { return input_metadata_[index]; } // Danger: not thread safe, caller must protect with lock InputMetadata\u0026 mutable_input_metadata(size_t index) { return input_metadata_[index]; } std::optional\u003cc10::Stream\u003e stream() { auto opt_device_type = at::getAccelerator(); if (!opt_device_type.has_value()) { return std::nullopt; } for (const auto\u0026 metadata : input_metadata_) { if (metadata.device().type() == opt_device_type.value()) return metadata.stream(); } return std::nullopt; } // Used by the engine to determine what device thread to run on at::Device device() { // Since we pick the first non-CPU tensor, this won\u0027t work with // mixed device-type operations (e.g., an op that is both CUDA // and XLA). This is *incredibly* unlikely, so we don\u0027t worry // about it. for (const auto\u0026 metadata : input_metadata_) { auto device = metadata.device(); if (device.type() != at::kCPU) { return device; } } // Only report to the CPU thread if there really were no tensors // from other devices. return at::kCPU; } void clear_input_metadata() { input_metadata_.clear(); } // Outputs (\"Next Edges\") void update_topological_nr(const Edge\u0026 edge) { TORCH_INTERNAL_ASSERT( !has_parent_, \"Cannot update a node\u0027s topological_nr after it already has a parent.\" \" If we allow this, we can no longer guarantee that a parent\u0027s\" \" topo_nr is always greater than those of all its children\") Node* node = edge.function.get(); if (node) { auto topo_nr = node-\u003etopological_nr(); if (topological_nr_ \u003c= topo_nr) { topological_nr_ = topo_nr + 1; } } } void set_next_edge(size_t index, Edge edge) { update_topological_nr(edge); next_edges_[index] = std::move(edge); } void add_next_edge(Edge edge) { update_topological_nr(edge); next_edges_.emplace_back(std::move(edge)); } void set_next_edges(edge_list\u0026\u0026 next_edges) { next_edges_ = std::move(next_edges); for (const auto\u0026 next_edge : next_edges_) { update_topological_nr(next_edge); } } const Edge\u0026 next_edge(size_t index) const noexcept { return next_edges_[index]; } const edge_list\u0026 next_edges() const noexcept { return next_edges_; } edge_list\u0026 next_edges() noexcept { return next_edges_; } uint32_t num_outputs() const noexcept { return next_edges_.size(); } // Miscellaneous Methods //~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ uint64_t sequence_nr() const noexcept { return sequence_nr_; } void set_sequence_nr(uint64_t sequence_nr) { sequence_nr_ = sequence_nr; } // NOTE [ Topological Number ] // // topological_nr is used to prune branches in the DAG during autograd // discovery as maintaining topological_nr helps us check in O(1) if there // does NOT exist a directed path between two nodes. // // The topological order number of this `Node` representing the length of the // longest possible path from this Node to any leaf node. If you are leaf // node, aka AccumulateGrad, this will be zero. This value has the property // that For every pair of nodes X, Y in G, existence of a directed path from X // to Y implies topo_nr(X) \u003e topo_nr(Y). The converse is not true, however, so // we cannot prove existence of a path from X to Y, only non-existence. // // One assumption we make when using topo_nr is that once a node // has been used, i.e., has a parent node, its own topo_nr does not change // we have added some checks with the `has_parent_` field to enforce this. // // What NOT to do: // // 1) 2 -\u003e 1 -\u003e 0 In this diagram we label nodes with their // topo_nr. // 2 -\u003e 1 -\u003e 0 We have two simple graphs that can each // arise from // `t.exp().exp()`, for example. // 2) 2 -\u003e 1 -\u003e 0 // / // 2 -\u003e 1 -\u003e 0 We add 2 as a next edge to 1 even though 1 // already // has a parent. // 3) 2 -\u003e 1 -\u003e 0 // / // 2 -\u003e 3 -\u003e 0 2 \u003c 3, yet there exists a path from 2 to 3! // uint64_t topological_nr() const noexcept { has_parent_ = true; return topological_nr_; } // assigning a node as a parent to this node void assign_parent(); uint64_t thread_id() const noexcept { return thread_id_; } virtual std::string name() const; bool should_compute_output(size_t output_edge_index) const { TORCH_CHECK(output_edge_index \u003c num_outputs(), \"Index out of range\"); return next_edges_[output_edge_index].is_valid(); } bool should_compute_output(std::initializer_list\u003cIndexRange\u003e idxs) const { return std::any_of(idxs.begin(), idxs.end(), [this](IndexRange range) { for (const auto i : c10::irange(range.first, range.second)) { if (should_compute_output(i)) return true; } return false; }); } bool task_should_compute_output(size_t output_edge_index) const { TORCH_CHECK(output_edge_index \u003c num_outputs(), \"Index out of range\"); const auto\u0026 next = next_edges_[output_edge_index]; if (next.is_valid()) { const auto exec_info = get_current_graph_task_exec_info(); if (exec_info \u0026\u0026 !exec_info-\u003eempty()) { auto it = exec_info-\u003efind(next.function.get()); if (it == exec_info-\u003eend() || !it-\u003esecond.should_execute()) { return false; // this edge is not needed for the current graph_task } } return true; } return false; } bool task_should_compute_output( std::initializer_list\u003cIndexRange\u003e idxs) const { return std::any_of(idxs.begin(), idxs.end(), [this](IndexRange range) { for (const auto i : c10::irange(range.first, range.second)) { if (task_should_compute_output(i)) return true; } return false; }); } PyObject* pyobj() const noexcept { return pyobj_; } void set_pyobj(PyObject* pyobj) noexcept { pyobj_ = pyobj; } AnomalyMetadata* metadata() noexcept; // Hook API //~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ uintptr_t add_post_hook(std::unique_ptr\u003cFunctionPostHook\u003e\u0026\u0026 post_hook) { post_hooks_.emplace_back(std::move(post_hook)); // Use the raw pointer as the unique key to identify this hook. This key // can then be used in del_post_hook(key) to remove this hook. return reinterpret_cast\u003cstd::uintptr_t\u003e(post_hooks_.back().get()); } const std::vector\u003cstd::unique_ptr\u003cFunctionPostHook\u003e\u003e\u0026 post_hooks() const noexcept { return post_hooks_; } // delete a post hook matching the key bool del_post_hook(const uintptr_t\u0026 key) { for (auto it = post_hooks_.begin(); it != post_hooks_.end(); ++it) { if (key == reinterpret_cast\u003cstd::uintptr_t\u003e(it-\u003eget())) { post_hooks_.erase(it); return true; } } return false; } std::vector\u003cstd::unique_ptr\u003cFunctionPostHook\u003e\u003e\u0026 post_hooks() noexcept { return post_hooks_; } void add_pre_hook(std::unique_ptr\u003cFunctionPreHook\u003e\u0026\u0026 pre_hook) { pre_hooks_.emplace_back(std::move(pre_hook)); } void add_tensor_pre_hook(std::unique_ptr\u003cFunctionPreHook\u003e\u0026\u0026 pre_hook) { tensor_pre_hooks_.emplace_back(std::move(pre_hook)); } void add_retains_grad_hook( std::unique_ptr\u003cFunctionPreHook\u003e\u0026\u0026 pre_hook, size_t output_idx) { retains_grad_hooks_[output_idx] = std::move(pre_hook); } std::unique_ptr\u003cFunctionPreHook\u003e pop_retains_grad_hook(size_t output_idx) { auto ret = std::move(retains_grad_hooks_[output_idx]); retains_grad_hooks_.erase(output_idx); return ret; } const std::vector\u003cstd::unique_ptr\u003cFunctionPreHook\u003e\u003e\u0026 pre_hooks() const noexcept { return pre_hooks_; } std::vector\u003cstd::unique_ptr\u003cFunctionPreHook\u003e\u003e\u0026 pre_hooks() noexcept { return pre_hooks_; } virtual std::vector\u003cstd::unique_ptr\u003cFunctionPreHook\u003e\u003e\u0026 tensor_pre_hooks() noexcept { return tensor_pre_hooks_; } virtual std::unique_ptr\u003cPostAccumulateGradHook\u003e\u0026 tensor_post_acc_grad_hooks() const noexcept { static std::unique_ptr\u003cPostAccumulateGradHook\u003e empty = nullptr; return empty; } std::unordered_map\u003csize_t, std::unique_ptr\u003cFunctionPreHook\u003e\u003e\u0026 retains_grad_hooks() noexcept { return retains_grad_hooks_; } // Customization Points for Subclasses //~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ virtual void release_variables() {} virtual void will_release_variables() {} virtual bool is_traceable() { return false; } virtual bool passes_state_transparently() { return false; } // see [Note: Compiled Autograd] // Used by compiled autograd to // 1) Extract tensors/symint args // 2) Collect node information for specialization and caching // Implementations in subclasses should call args.collect() with all node // attrs. These functions are only called during backward. virtual void compiled_args(CompiledNodeArgs\u0026 args) const { TORCH_CHECK_NOT_IMPLEMENTED( false, std::string(\"compiled_args not implemented: \") + name()); } // Used by compiled autograd to call apply() with different saved tensors // Implementations should call saved.before() on all attrs, then apply(), then // saved.after() on all attrs in the same order. virtual variable_list apply_with_saved( const variable_list\u0026 inputs, SwapSavedVariables\u0026 saved) { TORCH_CHECK_NOT_IMPLEMENTED( false, std::string(\"apply_with_saved not implemented: \") + name()); } // If this node is the AOTBackward node produced by torch.compile. // Compiled Autograd special-cases on this information. virtual bool is_aot_backward() const { return false; } protected: virtual variable_list apply(variable_list\u0026\u0026 inputs) = 0; variable_list traced_apply(variable_list inputs); // Sequence number used to correlate backward nodes with forward ops in the // profiler and provide determinism in the engine. // NOLINTNEXTLINE(cppcoreguidelines-avoid-const-or-ref-data-members) uint64_t sequence_nr_; // See NOTE [ Topological Number ] uint64_t topological_nr_ = 0; // Tracks whether this node has been added as the next_edge of another node // via set_next_edge(s), which always calls topological_nr() of all its // children See NOTE [ Topological Number ] for why we need this. mutable bool has_parent_ = false; // Id of the thread that created the instance uint64_t thread_id_ = 0; // Note [Thread Safety on Autograd Node] // ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ // Autograd Engine let the owning thread which calls Engine::execute to drive // the GraphTask execution, there might be cases that part of the GraphTask is // shared across different `backward()` or `grad()` calls, i.e. fork new // threads in the middle of the forward and call `backward()` separately from // different threads. We need to protect the thread safety on NodeTask to // prevent data racing on shared variables read/write. // // NB: This is only needed for Autograd Nodes that runs on CPU, technically // \"CUDA\", \"XLA\" nodes don\u0027t need locking because device threads are always // single threaded. // // Here we add a thread mutex to help protect the Node\u0027s thread safety, so // that different threads cannot race the shared data when executing the same // NodeTask from multiple CPU threads. It IS the user/developer responsibility // to take advantage of this mutex to protect the thread safety of their // autograd Node. The general strategy of thread safety on autograd Node: // // 1. User should lock the mutex during Node::release_variables() if the Node // needs // to release the variables on the fly, this serve the purpose that when we // release saved_variables from one thread, no other threads can release // the saved variables concurrently. call the Node::apply(), // 2. User should lock the mutex during Node::apply(), this is to ensure Node // that // writing to the shared variable are not racing across threads (i.e. // AccumulateGrad and custom C++ Autograd Node if writing to shared // variables ) // 3. item 2 and item 3 should work together so that when we release saved // variables // from one thread, no other threads can call Node::apply(), this ensures // the variable references from other threads aren\u0027t dangling. // 4. if the Node don\u0027t release any variables and no shared data read/write in // the Node // i.e. purely functional, user don\u0027t need to lock the mutex // // This way we could protect the thread safety on Autograd Node, but we could // still not protect the thread safety on Node pre/post C++ hooks (python // hooks are automatically thread safe), we rely on the user to write thread // safe C++ hooks if they want the hook to be correctly applied in // multithreading environment. std::mutex mutex_; edge_list next_edges_; PyObject* pyobj_ = nullptr; // weak reference std::unique_ptr\u003cAnomalyMetadata\u003e anomaly_metadata_ = nullptr; // NOTE [Hooks ordering] // We have 3 separate fields for pre hooks registered to the autograd nodes // because the conditions under which they execute are different, and we // want more fine-grained control over the order in which different types // of hooks are executed. // - pre_hooks are only executed when the node itself is executed // - tensor_pre_hook is executed as long as the engine traverses over it // even if that node won\u0027t be executed. // - retains_grad_hook are like tensor_pre_hooks except they are always // ordered after all other tensor pre hooks std::vector\u003cstd::unique_ptr\u003cFunctionPreHook\u003e\u003e pre_hooks_; std::vector\u003cstd::unique_ptr\u003cFunctionPreHook\u003e\u003e tensor_pre_hooks_; std::unordered_map\u003csize_t, std::unique_ptr\u003cFunctionPreHook\u003e\u003e retains_grad_hooks_; std::vector\u003cstd::unique_ptr\u003cFunctionPostHook\u003e\u003e post_hooks_; at::SmallVector\u003cInputMetadata, 2\u003e input_metadata_; }; struct TraceableFunction : public Node { using Node::Node; bool is_traceable() final { return true; } }; //~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ // Associated Free Nodes //~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ namespace detail { // Implementation of `collect_next_edges` (see below). struct MakeNextFunctionList : IterArgs\u003cMakeNextFunctionList\u003e { edge_list next_edges; using IterArgs\u003cMakeNextFunctionList\u003e::operator(); void operator()(const Variable\u0026 variable) { if (variable.defined()) { next_edges.emplace_back(impl::gradient_edge(variable)); } else { next_edges.emplace_back(); } } void operator()(const Variable* variable) { operator()(*variable); } void operator()(const std::optional\u003cVariable\u003e\u0026 variable) { if (variable.has_value()) { operator()(*variable); } else { next_edges.emplace_back(); } } }; } // namespace detail inline void create_gradient_edge( Variable\u0026 variable, std::shared_ptr\u003cNode\u003e function) { // Copy before move. const auto input_nr = function-\u003eadd_input_metadata(variable); impl::set_gradient_edge(variable, {std::move(function), input_nr}); } inline bool any_variable_requires_grad(const variable_list\u0026 variables) { return std::any_of( variables.begin(), variables.end(), [](const Variable\u0026 variable) { return variable.defined() \u0026\u0026 variable.requires_grad(); }); } template \u003ctypename... Variables\u003e edge_list collect_next_edges(Variables\u0026\u0026... variables) { detail::MakeNextFunctionList make; make.apply(std::forward\u003cVariables\u003e(variables)...); return std::move(make.next_edges); } struct TypeAndSize { TypeAndSize() = default; /* implicit */ TypeAndSize(const at::Tensor\u0026 t) : sym_sizes(t.sym_sizes().vec()), options(t.options()) {} at::Tensor zeros(); std::vector\u003cc10::SymInt\u003e sym_sizes; at::TensorOptions options; }; } // namespace torch::autograd",
       "author": {
         "@type": "Organization",
         "name": "PyTorch Contributors",
         "url": "https://pytorch.org"
       },
       "image": "../_static/img/pytorch_seo.png",
       "mainEntityOfPage": {
         "@type": "WebPage",
         "@id": "/api/program_listing_file_torch_csrc_autograd_function.h.html"
       },
       "datePublished": "2023-01-01T00:00:00Z",
       "dateModified": "2023-01-01T00:00:00Z"
     }
 </script>
  <script>
    // Tutorials Call to action event tracking
    $("[data-behavior='call-to-action-event']").on('click', function () {
      fbq('trackCustom', "Download", {
        tutorialTitle: $('h1:first').text(),
        downloadLink: this.href,
        tutorialLink: window.location.href,
        downloadTitle: $(this).attr("data-response")
      });
      if (typeof gtag === 'function') {
        gtag('event', 'click', {
          'event_category': $(this).attr("data-response"),
          'event_label': $("h1").first().text(),
          'tutorial_link': window.location.href
        });
      }
    });
  </script>
  
  </body>
</html>