

<!DOCTYPE html>


<html lang="en" data-content_root="" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Program Listing for File custom_function.h &#8212; PyTorch main documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/theme.css" />
    <link rel="stylesheet" type="text/css" href="../_static/collapsible-lists/css/tree_view.css" />
    <link rel="stylesheet" type="text/css" href="../_static/cpp_theme.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/sphinx_highlight.js"></script>
    <script src="../_static/collapsible-lists/js/CollapsibleLists.compressed.js"></script>
    <script src="../_static/collapsible-lists/js/apply-collapsible-lists.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'api/program_listing_file_torch_csrc_autograd_custom_function.h';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="File custom_operator.h" href="file_torch_csrc_jit_runtime_custom_operator.h.html" />
    <link rel="prev" title="File custom_function.h" href="file_torch_csrc_autograd_custom_function.h.html" />

  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
<script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/2.3.1/list.min.js"></script>
<script>
  if (window.location.hostname === 'docs.pytorch.org' || window.location.hostname === 'docs-preview.pytorch.org') {
    const script = document.createElement('script');
    script.src = 'https://cmp.osano.com/16A0DbT9yDNIaQkvZ/31b1b91a-e0b6-47ea-bde2-7f2bd13dbe5c/osano.js?variant=one';
    document.head.appendChild(script);
  }
</script>
<script>
  // Cookie banner for non-LF projects
  document.addEventListener('DOMContentLoaded', function () {
    // Hide cookie banner on local environments and LF owned docs
    if (window.location.hostname === 'localhost' ||
      window.location.hostname === '0.0.0.0' ||
      window.location.hostname === '127.0.0.1' ||
      window.location.hostname === 'docs.pytorch.org' ||
      window.location.hostname === 'docs-preview.pytorch.org' ||
      window.location.hostname.startsWith('192.168.')) {
      const banner = document.querySelector('.cookie-banner-wrapper');
      if (banner) {
        banner.style.display = 'none';
      }
    }
  });
</script>
<!-- Conditional CSS for header and footer height adjustment -->

<link rel="stylesheet" type="text/css" href="../_static/css/theme.css" crossorigin="anonymous">
<script type="text/javascript" src="../_static/js/theme.js"></script>
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Montserrat:wght@300;400&display=swap" rel="stylesheet">
<meta property="og:image" content="../_static/img/pytorch_seo.png" />
<link rel="stylesheet" href="../_static/webfonts/all.min.css" crossorigin="anonymous">
<meta http-equiv="Content-Security-Policy"
  content="default-src * 'unsafe-inline' 'unsafe-eval' data: blob:; style-src * 'unsafe-inline'; script-src * 'unsafe-inline' 'unsafe-eval' blob:;">
<meta name="pytorch_project" content="">
<!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-T8XT4PS" height="0" width="0"
    style="display:none;visibility:hidden"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->
<!-- Google Tag Manager -->
<script>(function (w, d, s, l, i) {
    w[l] = w[l] || []; w[l].push({
      'gtm.start':
        new Date().getTime(), event: 'gtm.js'
    }); var f = d.getElementsByTagName(s)[0],
      j = d.createElement(s), dl = l != 'dataLayer' ? '&l=' + l : ''; j.async = true; j.src =
        'https://www.googletagmanager.com/gtm.js?id=' + i + dl; f.parentNode.insertBefore(j, f);
    j.onload = function () {
      window.dispatchEvent(new Event('gtm_loaded'));
      console.log('GTM loaded successfully');
    };
  })(window, document, 'script', 'dataLayer', 'GTM-T8XT4PS');
</script>
<!-- End Google Tag Manager -->
<!-- Facebook Pixel Code -->
<script>
  !function (f, b, e, v, n, t, s) {
    if (f.fbq) return; n = f.fbq = function () {
      n.callMethod ?
        n.callMethod.apply(n, arguments) : n.queue.push(arguments)
    };
    if (!f._fbq) f._fbq = n; n.push = n; n.loaded = !0; n.version = '2.0';
    n.queue = []; t = b.createElement(e); t.async = !0;
    t.src = v; s = b.getElementsByTagName(e)[0];
    s.parentNode.insertBefore(t, s)
  }(window, document, 'script',
    'https://connect.facebook.net/en_US/fbevents.js');
  fbq('init', '243028289693773');
  fbq('track', 'PageView');
</script>
<script>
  document.documentElement.setAttribute('data-version', 'main');
</script>
<noscript>
  <img height="1" width="1" src="https://www.facebook.com/tr?id=243028289693773&ev=PageView&noscript=1" />
</noscript>
<script>
  function gtag() {
    window.dataLayer.push(arguments);
  }
</script>
<!-- End Facebook Pixel Code -->
<!-- Script to Fix scrolling -->
<script>
  document.addEventListener('DOMContentLoaded', function () {
    // Fix anchor scrolling
    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
      anchor.addEventListener('click', function (e) {
        e.preventDefault();
        const targetId = this.getAttribute('href').substring(1);
        const targetElement = document.getElementById(targetId);

        if (targetElement) {
          const headerHeight =
            (document.querySelector('.header-holder') ? document.querySelector('.header-holder').offsetHeight : 0) +
            (document.querySelector('.bd-header') ? document.querySelector('.bd-header').offsetHeight : 0) + 20;

          const targetPosition = targetElement.getBoundingClientRect().top + window.pageYOffset - headerHeight;
          window.scrollTo({
            top: targetPosition,
            behavior: 'smooth'
          });

          // Update URL hash without scrolling
          history.pushState(null, null, '#' + targetId);
        }
      });
    });
  });
</script>

<script async src="https://cse.google.com/cse.js?cx=e65585f8c3ea1440e"></script>

<!--
   Search engines should not index the main version of documentation.
   Stable documentation are built without release == 'main'.
   -->
<meta name="robots" content="noindex">


  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>

  </head>

<body data-feedback-url="https://github.com/pytorch/pytorch" class="pytorch-body">
  
    <div class="container-fluid header-holder tutorials-header" id="header-holder">
   <div class="header-container-wrapper">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>

          <li class="main-menu-item">
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                <span>Learn</span>
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/get-started">
                  <span class=dropdown-title>Get Started</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials">
                  <span class="dropdown-title">Tutorials</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/basics/intro.html">
                  <span class="dropdown-title">Learn the Basics</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/recipes/recipes_index.html">
                  <span class="dropdown-title">PyTorch Recipes</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/introyt.html">
                  <span class="dropdown-title">Intro to PyTorch - YouTube Series</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/webinars/">
                  <span class="dropdown-title">Webinars</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
              <span>Community</span>
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://landscape.pytorch.org/" target="_blank">
                  <span class="dropdown-title">Landscape</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/join-ecosystem">
                  <span class="dropdown-title">Join the Ecosystem</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/community-hub/">
                  <span class="dropdown-title">Community Hub</span>
                </a>
                <a class="nav-dropdown-item" href="https://discuss.pytorch.org/">
                  <span class="dropdown-title">Forums</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/resources">
                  <span class=dropdown-title>Developer Resources</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/contributor-awards/">
                  <span class="dropdown-title">Contributor Awards</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/community-events/">
                  <span class="dropdown-title">Community Events</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/programs/ambassadors/">
                  <span class="dropdown-title">PyTorch Ambassadors</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
              <span>Projects</span>
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/projects/pytorch/">
                  <span class="dropdown-title">PyTorch</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/projects/vllm/">
                  <span class="dropdown-title">vLLM</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/projects/deepspeed/">
                  <span class="dropdown-title">DeepSpeed</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/projects/host-your-project/">
                  <span class="dropdown-title">Host Your Project</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
              <span> Docs</span>
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/docs/stable/index.html">
                  <span class="dropdown-title">PyTorch</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/pytorch-domains">
                  <span class="dropdown-title">Domains</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
              <span>Blogs & News</span>
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/blog/">
                  <span class="dropdown-title">Blog</span>
                </a>
                 <a class="nav-dropdown-item" href="https://pytorch.org/announcements">
                  <span class="dropdown-title">Announcements</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/case-studies/">
                  <span class="dropdown-title">Case Studies</span>
                <a class="nav-dropdown-item" href="https://pytorch.org/events">
                  <span class="dropdown-title">Events</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/newsletter">
                  <span class="dropdown-title">Newsletter</span>
                </a>
            </div>
          </li>

          <li class="main-menu-item">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
              <span>About</span>
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/foundation">
                  <span class="dropdown-title">PyTorch Foundation</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/members">
                  <span class="dropdown-title">Members</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/governing-board">
                  <span class="dropdown-title">Governing Board</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tac">
                  <span class="dropdown-title">Technical Advisory Council</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/credits">
                  <span class="dropdown-title">Cloud Credit Program</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/staff">
                  <span class="dropdown-title">Staff</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/contact">
                  <span class="dropdown-title">Contact</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <div class="no-dropdown main-menu-button">
              <a href="https://pytorch.org/join" data-cta="join">
                JOIN
              </a>
            </div>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu">
        <i class="fa-solid fa-ellipsis"></i>
      </a>
    </div>
  </div>
 </div>

 <!-- Begin Mobile Menu -->

<div class="mobile-main-menu">
  <div class="container-fluid">
    <div class="header-container-wrapper">
      <div class="mobile-main-menu-header-container">
        <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu">
        </a>
      </div>
    </div>
  </div>

  <div class="mobile-main-menu-links-container">
    <div class="main-menu">
      <ul>
         <li class="resources-mobile-menu-title">
           <a>Learn</a>
         </li>
         <ul class="resources-mobile-menu-items">
           <li>
             <a href="https://pytorch.org/get-started">Get Started</a>
           </li>
           <li>
             <a href="https://pytorch.org/tutorials">Tutorials</a>
           </li>
           <li>
             <a href="https://pytorch.org/tutorials/beginner/basics/intro.html">Learn the Basics</a>
           </li>
           <li>
             <a href="https://pytorch.org/tutorials/recipes/recipes_index.html">PyTorch Recipes</a>
           </li>
           <li>
             <a href="https://pytorch.org/tutorials/beginner/introyt.html">Introduction to PyTorch - YouTube Series</a>
           </li>
           <li>
            <a href="https://pytorch.org/webinars/">Webinars</a>
          </li>
        </ul>
         <li class="resources-mobile-menu-title">
           <a>Community</a>
         </li>
         <ul class="resources-mobile-menu-items">
          <li>
            <a href="https://landscape.pytorch.org/">Landscape</a>
          </li>
          <li>
             <a href="https://pytorch.org/join-ecosystem">Join the Ecosystem</a>
           </li>
           <li>
             <a href="https://pytorch.org/community-hub/">Community Hub</a>
           </li>
           <li>
             <a href="https://discuss.pytorch.org/">Forums</a>
           </li>
           <li>
             <a href="https://pytorch.org/resources">Developer Resources</a>
           </li>
           <li>
             <a href="https://pytorch.org/contributor-awards/">Contributor Awards</a>
           </li>
           <li>
            <a href="https://pytorch.org/community-events/">Community Events</a>
          </li>
          <li>
            <a href="https://pytorch.org/programs/ambassadors/">PyTorch Ambassadors</a>
          </li>
       </ul>

         <li class="resources-mobile-menu-title">
           <a>Projects</a>
         </li>

         <ul class="resources-mobile-menu-items">
           <li>
             <a href="https://pytorch.org/projects/pytorch/">PyTorch</a>
           </li>

           <li>
             <a href="https://pytorch.org/projects/vllm/">vLLM</a>
           </li>
           <li>
            <a href="https://pytorch.org/projects/deepspeed/">DeepSpeed</a>
          </li>
          <li>
             <a href="https://pytorch.org/projects/host-your-project/">Host Your Project</a>
           </li>
         </ul>

         <li class="resources-mobile-menu-title">
           <a>Docs</a>
         </li>

         <ul class="resources-mobile-menu-items">
          <li>
            <a href="https://pytorch.org/docs/stable/index.html">PyTorch</a>
          </li>

          <li>
            <a href="https://pytorch.org/pytorch-domains">Domains</a>
          </li>
        </ul>

        <li class="resources-mobile-menu-title">
          <a>Blog & News</a>
        </li>

         <ul class="resources-mobile-menu-items">
          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>
          <li>
            <a href="https://pytorch.org/announcements">Announcements</a>
          </li>

          <li>
            <a href="https://pytorch.org/case-studies/">Case Studies</a>
          </li>
          <li>
            <a href="https://pytorch.org/events">Events</a>
          </li>
          <li>
             <a href="https://pytorch.org/newsletter">Newsletter</a>
           </li>
        </ul>

        <li class="resources-mobile-menu-title">
          <a>About</a>
        </li>

        <ul class="resources-mobile-menu-items">
          <li>
            <a href="https://pytorch.org/foundation">PyTorch Foundation</a>
          </li>
          <li>
            <a href="https://pytorch.org/members">Members</a>
          </li>
          <li>
            <a href="https://pytorch.org/governing-board">Governing Board</a>
          </li>
          <li>
            <a href="https://pytorch.org/tac">Technical Advisory Council</a>
         </li>
         <li>
             <a href="https://pytorch.org/credits">Cloud Credit Program</a>
          </li>
          <li>
             <a href="https://pytorch.org/staff">Staff</a>
          </li>
          <li>
             <a href="https://pytorch.org/contact">Contact</a>
          </li>
        </ul>
      </ul>
    </div>
  </div>
</div>

<!-- End Mobile Menu -->
  
  
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
<div class="bd-header__inner bd-page-width">
  <button class="pst-navbar-icon sidebar-toggle primary-toggle" aria-label="Site navigation">
    <span class="fa-solid fa-bars"></span>
  </button>
  
  
  <div class=" navbar-header-items__start">
    
      <div class="navbar-item">
  <a href="../index.html" class="version">main</a>
</div>
    
  </div>
  
  <div class=" navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../installing.html">
    Installing C++ Distributions of PyTorch
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../frontend.html">
    The C++ Frontend
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="library_root.html">
    Library API
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../notes/faq.html">
    FAQ
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../notes/inference_mode.html">
    Inference Mode
  </a>
</li>

            <li class="nav-item dropdown">
                <button class="btn dropdown-toggle nav-item" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-controls="pst-nav-more-links">
                    More
                </button>
                <ul id="pst-nav-more-links" class="dropdown-menu">
                    
<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="../notes/maybe_owned.html">
    MaybeOwned<Tensor>
  </a>
</li>


<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="../notes/tensor_basics.html">
    Tensor Basics
  </a>
</li>


<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="../notes/tensor_creation.html">
    Tensor Creation API
  </a>
</li>


<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="../notes/tensor_cuda_stream.html">
    Tensor CUDA Stream API
  </a>
</li>


<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="../notes/tensor_indexing.html">
    Tensor Indexing API
  </a>
</li>


<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="../notes/versioning.html">
    Library Versioning
  </a>
</li>

                </ul>
            </li>
            
  </ul>
</nav></div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
      
        <div class="navbar-item">
  
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
</div>
      
        <div class="navbar-item">

<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
      
        <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://x.com/PyTorch" title="X" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-x-twitter fa-lg" aria-hidden="true"></i>
            <span class="sr-only">X</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/pytorch/pytorch" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://discuss.pytorch.org/" title="PyTorch Forum" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-discourse fa-lg" aria-hidden="true"></i>
            <span class="sr-only">PyTorch Forum</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://pypi.org/project/torch/" title="PyPi" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-python fa-lg" aria-hidden="true"></i>
            <span class="sr-only">PyPi</span></a>
        </li>
</ul></div>
      
    </div>
    
  </div>
  
  

  
    <button class="pst-navbar-icon sidebar-toggle secondary-toggle" aria-label="On this page">
      <span class="fa-solid fa-outdent"></span>
    </button>
  
</div>

    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
        
          
          
            <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../installing.html">
    Installing C++ Distributions of PyTorch
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../frontend.html">
    The C++ Frontend
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="library_root.html">
    Library API
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../notes/faq.html">
    FAQ
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../notes/inference_mode.html">
    Inference Mode
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../notes/maybe_owned.html">
    MaybeOwned<Tensor>
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../notes/tensor_basics.html">
    Tensor Basics
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../notes/tensor_creation.html">
    Tensor Creation API
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../notes/tensor_cuda_stream.html">
    Tensor CUDA Stream API
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../notes/tensor_indexing.html">
    Tensor Indexing API
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../notes/versioning.html">
    Library Versioning
  </a>
</li>

  </ul>
</nav></div>
          
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item">
  
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
</div>
        
          <div class="navbar-item">

<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
        
          <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://x.com/PyTorch" title="X" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-x-twitter fa-lg" aria-hidden="true"></i>
            <span class="sr-only">X</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/pytorch/pytorch" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://discuss.pytorch.org/" title="PyTorch Forum" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-discourse fa-lg" aria-hidden="true"></i>
            <span class="sr-only">PyTorch Forum</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://pypi.org/project/torch/" title="PyPi" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-python fa-lg" aria-hidden="true"></i>
            <span class="sr-only">PyPi</span></a>
        </li>
</ul></div>
        
      </div>
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
<nav class="bd-docs-nav bd-links"
     aria-label="Section Navigation">
  <p class="bd-links__title" role="heading" aria-level="1">Section Navigation</p>
  <div class="bd-toc-item navbar-nav"><ul class="current nav bd-sidenav">
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Program Listing for File custom_function.h</a></li>
</ul>
</div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">



<nav aria-label="Breadcrumb" class="d-print-none">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    
    <li class="breadcrumb-item"><a href="library_root.html" class="nav-link">Library API</a></li>
    
    
    <li class="breadcrumb-item"><a href="file_torch_csrc_autograd_custom_function.h.html" class="nav-link">File custom_function.h</a></li>
    
    <li class="breadcrumb-item active" aria-current="page">Program...</li>
  </ul>
</nav>
</div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">
<div class="rating">
    Rate this Page
    <div class="stars">
        
        <span class="star" data-behavior="tutorial-rating" data-count="1" data-value="1">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="2" data-value="2">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="3" data-value="3">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="4" data-value="4">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="5" data-value="5">★</span>
        
    </div>
</div>
</div>
      
    </div>
  
</div>
</div>
              
              
  
<div id="searchbox"></div>
  <article class="bd-article" id="pytorch-article">
    <!-- Hidden breadcrumb schema for SEO only -->
    <div style="display:none;" itemscope itemtype="https://schema.org/BreadcrumbList">
      
      <div itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
        <link itemprop="item" href="library_root.html">
        <meta itemprop="name" content="Library API">
        <meta itemprop="position" content="1">
      </div>
      
      <div itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
        <link itemprop="item" href="file_torch_csrc_autograd_custom_function.h.html">
        <meta itemprop="name" content="File custom_function.h">
        <meta itemprop="position" content="2">
      </div>
      
      <div itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
        <meta itemprop="name" content="Program Listing for File custom_function.h">
        <meta itemprop="position" content="3">
      </div>
    </div>

    

    
              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <div class="section" id="program-listing-for-file-custom-function-h">
<span id="program-listing-file-torch-csrc-autograd-custom-function-h"></span><h1>Program Listing for File custom_function.h<a class="headerlink" href="#program-listing-for-file-custom-function-h" title="Permalink to this heading">#</a></h1>
<p>↰ <a class="reference internal" href="file_torch_csrc_autograd_custom_function.h.html#file-torch-csrc-autograd-custom-function-h"><span class="std std-ref">Return to documentation for file</span></a> (<code class="docutils literal notranslate"><span class="pre">torch/csrc/autograd/custom_function.h</span></code>)</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="cp">#pragma once</span>

<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;ATen/core/ivalue.h&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;c10/core/SymInt.h&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;c10/util/flat_hash_map.h&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;c10/util/irange.h&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;torch/csrc/autograd/function.h&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;torch/csrc/autograd/variable.h&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;torch/csrc/autograd/variable_info.h&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;torch/csrc/dynamo/compiled_autograd.h&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;vector&gt;</span>

<span class="k">namespace</span><span class="w"> </span><span class="nn">torch</span><span class="o">::</span><span class="nn">autograd</span><span class="w"> </span><span class="p">{</span>

<span class="k">using</span><span class="w"> </span><span class="n">optional_variable_list</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">Variable</span><span class="o">&gt;&gt;</span><span class="p">;</span>
<span class="k">using</span><span class="w"> </span><span class="n">_jvp_fn_t</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">function</span><span class="o">&lt;</span><span class="n">variable_list</span><span class="p">(</span><span class="n">variable_list</span><span class="p">,</span><span class="w"> </span><span class="n">variable_list</span><span class="p">)</span><span class="o">&gt;</span><span class="p">;</span>
<span class="k">using</span><span class="w"> </span><span class="n">_view_as_self_fn_t</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">function</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="p">)</span><span class="o">&gt;</span><span class="p">;</span>

<span class="n">TORCH_API</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">Variable</span><span class="o">&gt;&gt;</span><span class="w"> </span><span class="n">_wrap_outputs</span><span class="p">(</span>
<span class="w">    </span><span class="k">const</span><span class="w"> </span><span class="n">variable_list</span><span class="o">&amp;</span><span class="w"> </span><span class="n">input_vars</span><span class="p">,</span>
<span class="w">    </span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">unordered_set</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">TensorImpl</span><span class="o">*&gt;&amp;</span><span class="w"> </span><span class="n">non_differentiable</span><span class="p">,</span>
<span class="w">    </span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">unordered_set</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">TensorImpl</span><span class="o">*&gt;&amp;</span><span class="w"> </span><span class="n">dirty_inputs</span><span class="p">,</span>
<span class="w">    </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">ArrayRef</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">Variable</span><span class="o">&gt;&gt;</span><span class="w"> </span><span class="n">raw_outputs</span><span class="p">,</span>
<span class="w">    </span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">Node</span><span class="o">&gt;&amp;</span><span class="w"> </span><span class="n">cdata</span><span class="p">,</span>
<span class="w">    </span><span class="k">const</span><span class="w"> </span><span class="n">_jvp_fn_t</span><span class="o">&amp;</span><span class="w"> </span><span class="n">jvp_user_function</span><span class="p">,</span>
<span class="w">    </span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">unordered_set</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">TensorImpl</span><span class="o">*&gt;&amp;</span><span class="w"> </span><span class="n">to_save_if_setup_context</span><span class="p">,</span>
<span class="w">    </span><span class="k">const</span><span class="w"> </span><span class="n">_view_as_self_fn_t</span><span class="o">&amp;</span><span class="w"> </span><span class="n">view_as_self_fn</span><span class="p">);</span>

<span class="n">TORCH_API</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="n">check_variable_result</span><span class="p">(</span>
<span class="w">    </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">TensorBase</span><span class="o">&amp;</span><span class="w"> </span><span class="n">original</span><span class="p">,</span>
<span class="w">    </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">TensorBase</span><span class="o">&amp;</span><span class="w"> </span><span class="n">result</span><span class="p">,</span>
<span class="w">    </span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="o">&amp;</span><span class="w"> </span><span class="n">hook_name</span><span class="p">);</span>

<span class="c1">// Get the return type of the forward function of the custom Function class X</span>
<span class="k">template</span><span class="w"> </span><span class="o">&lt;</span><span class="k">typename</span><span class="w"> </span><span class="nc">X</span><span class="p">,</span><span class="w"> </span><span class="k">typename</span><span class="p">...</span><span class="w"> </span><span class="n">Args</span><span class="o">&gt;</span>
<span class="k">using</span><span class="w"> </span><span class="n">forward_t</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">decltype</span><span class="p">(</span><span class="n">X</span><span class="o">::</span><span class="n">forward</span><span class="p">(</span><span class="k">nullptr</span><span class="p">,</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">declval</span><span class="o">&lt;</span><span class="n">Args</span><span class="o">&gt;</span><span class="p">()...));</span>

<span class="k">template</span><span class="w"> </span><span class="o">&lt;</span><span class="k">class</span><span class="w"> </span><span class="nc">T</span><span class="o">&gt;</span>
<span class="k">struct</span><span class="w"> </span><span class="nc">TORCH_API</span><span class="w"> </span><span class="n">Function</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="c1">// We need to use a different template parameter than T here because T will</span>
<span class="w">  </span><span class="c1">// inherit from Function, and when Function&lt;T&gt; is instantiated, T::forward</span>
<span class="w">  </span><span class="c1">// is not declared yet.</span>
<span class="w">  </span><span class="c1">// The enable_if check is to ensure that the user doesn&#39;t explicitly provide</span>
<span class="w">  </span><span class="c1">// the parameter X.</span>
<span class="w">  </span><span class="k">template</span><span class="w"> </span><span class="o">&lt;</span><span class="k">typename</span><span class="w"> </span><span class="nc">X</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">T</span><span class="p">,</span><span class="w"> </span><span class="k">typename</span><span class="p">...</span><span class="w"> </span><span class="n">Args</span><span class="o">&gt;</span>
<span class="w">  </span><span class="k">static</span><span class="w"> </span><span class="k">auto</span><span class="w"> </span><span class="n">apply</span><span class="p">(</span><span class="n">Args</span><span class="o">&amp;&amp;</span><span class="p">...</span><span class="w"> </span><span class="n">args</span><span class="p">)</span>
<span class="w">      </span><span class="o">-&gt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">enable_if_t</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">is_same_v</span><span class="o">&lt;</span><span class="n">X</span><span class="p">,</span><span class="w"> </span><span class="n">T</span><span class="o">&gt;</span><span class="p">,</span><span class="w"> </span><span class="n">forward_t</span><span class="o">&lt;</span><span class="n">X</span><span class="p">,</span><span class="w"> </span><span class="n">Args</span><span class="p">...</span><span class="o">&gt;&gt;</span><span class="p">;</span>

<span class="w">  </span><span class="c1">// This flag is for an experimental feature: compiled autograd. Not all</span>
<span class="w">  </span><span class="c1">// built-in APIs are supported at the moment e.g. mark_dirty and</span>
<span class="w">  </span><span class="c1">// mark_non_differentiable. Before setting this flag to enable tracing for</span>
<span class="w">  </span><span class="c1">// your custom function &lt;T&gt;, you need to ensure that the backward function is</span>
<span class="w">  </span><span class="c1">// traceable i.e. any variables accessed in the backward other than the input</span>
<span class="w">  </span><span class="c1">// arguments must be handled in a similar manner to built-ins in</span>
<span class="w">  </span><span class="c1">// CppNode::compiled_args and CppNode::apply_with_saved.</span>
<span class="w">  </span><span class="k">static</span><span class="w"> </span><span class="k">constexpr</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">is_traceable</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nb">false</span><span class="p">;</span>
<span class="p">};</span>

<span class="k">struct</span><span class="w"> </span><span class="nc">TORCH_API</span><span class="w"> </span><span class="n">AutogradContext</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">AutogradContext</span><span class="p">()</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">default</span><span class="p">;</span>
<span class="w">  </span><span class="n">AutogradContext</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">AutogradContext</span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">delete</span><span class="p">;</span>
<span class="w">  </span><span class="n">AutogradContext</span><span class="o">&amp;</span><span class="w"> </span><span class="k">operator</span><span class="o">=</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">AutogradContext</span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">delete</span><span class="p">;</span>
<span class="w">  </span><span class="n">AutogradContext</span><span class="p">(</span><span class="n">AutogradContext</span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">delete</span><span class="p">;</span>
<span class="w">  </span><span class="n">AutogradContext</span><span class="o">&amp;</span><span class="w"> </span><span class="k">operator</span><span class="o">=</span><span class="p">(</span><span class="n">AutogradContext</span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">delete</span><span class="p">;</span>
<span class="w">  </span><span class="o">~</span><span class="n">AutogradContext</span><span class="p">()</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">default</span><span class="p">;</span>

<span class="w">  </span><span class="n">AutogradContext</span><span class="p">(</span><span class="n">PackedArgs</span><span class="o">&amp;</span><span class="w"> </span><span class="n">packed_args</span><span class="p">);</span>

<span class="w">  </span><span class="n">ska</span><span class="o">::</span><span class="n">flat_hash_map</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="p">,</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">IValue</span><span class="o">&gt;</span><span class="w"> </span><span class="n">saved_data</span><span class="p">;</span>

<span class="w">  </span><span class="kt">void</span><span class="w"> </span><span class="nf">save_for_backward</span><span class="p">(</span><span class="n">variable_list</span><span class="w"> </span><span class="n">to_save</span><span class="p">);</span>
<span class="w">  </span><span class="kt">void</span><span class="w"> </span><span class="nf">mark_dirty</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">variable_list</span><span class="o">&amp;</span><span class="w"> </span><span class="n">inputs</span><span class="p">);</span>
<span class="w">  </span><span class="kt">void</span><span class="w"> </span><span class="nf">mark_non_differentiable</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">variable_list</span><span class="o">&amp;</span><span class="w"> </span><span class="n">outputs</span><span class="p">);</span>
<span class="w">  </span><span class="c1">// Sets whether undefined output grad tensors should be expanded to tensors</span>
<span class="w">  </span><span class="c1">// full of zeros before calling backward function. Default value is true.</span>
<span class="w">  </span><span class="kt">void</span><span class="w"> </span><span class="nf">set_materialize_grads</span><span class="p">(</span><span class="kt">bool</span><span class="w"> </span><span class="n">value</span><span class="p">);</span>

<span class="w">  </span><span class="n">variable_list</span><span class="w"> </span><span class="nf">get_saved_variables</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">unordered_set</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">TensorImpl</span><span class="o">*&gt;&amp;</span><span class="w"> </span><span class="n">get_and_bump_dirty</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">unordered_set</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">TensorImpl</span><span class="o">*&gt;&amp;</span><span class="w"> </span><span class="n">get_non_differentiable</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>

<span class="w">  </span><span class="kt">bool</span><span class="w"> </span><span class="nf">needs_input_grad</span><span class="p">(</span><span class="kt">size_t</span><span class="w"> </span><span class="n">output_edge_index</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="kt">bool</span><span class="w"> </span><span class="nf">needs_input_grad</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">initializer_list</span><span class="o">&lt;</span><span class="n">IndexRange</span><span class="o">&gt;</span><span class="w"> </span><span class="n">idxs</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>

<span class="w"> </span><span class="k">private</span><span class="o">:</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">unordered_set</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">TensorImpl</span><span class="o">*&gt;</span><span class="w"> </span><span class="n">non_differentiable_</span><span class="p">;</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">unordered_set</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">TensorImpl</span><span class="o">*&gt;</span><span class="w"> </span><span class="n">dirty_inputs_</span><span class="p">;</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">torch</span><span class="o">::</span><span class="n">autograd</span><span class="o">::</span><span class="n">SavedVariable</span><span class="o">&gt;</span><span class="w"> </span><span class="n">saved_variables_</span><span class="p">;</span>
<span class="w">  </span><span class="n">variable_list</span><span class="w"> </span><span class="n">to_save_</span><span class="p">;</span>
<span class="w">  </span><span class="kt">bool</span><span class="w"> </span><span class="n">materialize_grads_</span><span class="p">{</span><span class="nb">true</span><span class="p">};</span>

<span class="w">  </span><span class="c1">// The CppNode in the autograd graph that owns this AutogradContext. We need a</span>
<span class="w">  </span><span class="c1">// weak_ptr to avoid a refcycle. Since grad_fn_ owns this AutogradContext, it</span>
<span class="w">  </span><span class="c1">// will always be alive when we want to use it.</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">weak_ptr</span><span class="o">&lt;</span><span class="n">Node</span><span class="o">&gt;</span><span class="w"> </span><span class="n">grad_fn_</span><span class="p">;</span>
<span class="w">  </span><span class="kt">bool</span><span class="w"> </span><span class="n">has_freed_buffers_</span><span class="p">{</span><span class="nb">false</span><span class="p">};</span>

<span class="w">  </span><span class="c1">// Compiled autograd overrides saved_variables() and needs_input_grad().</span>
<span class="w">  </span><span class="c1">// We store the values we want to return here.</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">variable_list</span><span class="o">&gt;</span><span class="w"> </span><span class="n">saved_variables_override_</span><span class="p">;</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">bool</span><span class="o">&gt;&gt;</span><span class="w"> </span><span class="n">needs_input_grad_override_</span><span class="p">;</span>

<span class="w">  </span><span class="kt">void</span><span class="w"> </span><span class="nf">save_variables</span><span class="p">();</span>

<span class="w">  </span><span class="k">template</span><span class="w"> </span><span class="o">&lt;</span><span class="k">class</span><span class="w"> </span><span class="nc">T</span><span class="o">&gt;</span>
<span class="w">  </span><span class="k">friend</span><span class="w"> </span><span class="k">struct</span><span class="w"> </span><span class="nc">CppNode</span><span class="p">;</span>
<span class="w">  </span><span class="k">template</span><span class="w"> </span><span class="o">&lt;</span><span class="k">class</span><span class="w"> </span><span class="nc">T</span><span class="o">&gt;</span>
<span class="w">  </span><span class="k">friend</span><span class="w"> </span><span class="n">variable_list</span><span class="w"> </span><span class="n">CppNode_apply_functional</span><span class="p">(</span>
<span class="w">      </span><span class="n">variable_list</span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="n">inputs</span><span class="p">,</span>
<span class="w">      </span><span class="n">AutogradContext</span><span class="o">&amp;</span><span class="w"> </span><span class="n">ctx_</span><span class="p">,</span>
<span class="w">      </span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">bool</span><span class="o">&gt;&amp;</span><span class="w"> </span><span class="n">is_variable_input_</span><span class="p">,</span>
<span class="w">      </span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">VariableInfo</span><span class="o">&gt;&amp;</span><span class="w"> </span><span class="n">output_info_</span><span class="p">,</span>
<span class="w">      </span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="o">&amp;</span><span class="w"> </span><span class="n">name</span><span class="p">);</span>
<span class="p">};</span>

<span class="k">template</span><span class="w"> </span><span class="o">&lt;</span><span class="k">typename</span><span class="w"> </span><span class="nc">T</span><span class="o">&gt;</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">variable_list</span><span class="w"> </span><span class="n">CppNode_apply_functional</span><span class="p">(</span>
<span class="w">    </span><span class="c1">// NOLINTNEXTLINE(cppcoreguidelines-rvalue-reference-param-not-moved)</span>
<span class="w">    </span><span class="n">variable_list</span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="n">inputs</span><span class="p">,</span>
<span class="w">    </span><span class="n">AutogradContext</span><span class="o">&amp;</span><span class="w"> </span><span class="n">ctx_</span><span class="p">,</span>
<span class="w">    </span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">bool</span><span class="o">&gt;&amp;</span><span class="w"> </span><span class="n">is_variable_input_</span><span class="p">,</span>
<span class="w">    </span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">VariableInfo</span><span class="o">&gt;&amp;</span><span class="w"> </span><span class="n">output_info_</span><span class="p">,</span>
<span class="w">    </span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="o">&amp;</span><span class="w"> </span><span class="n">name</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">OptionalDeviceGuard</span><span class="w"> </span><span class="n">_device_guard</span><span class="p">;</span>

<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">num_inputs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">inputs</span><span class="p">.</span><span class="n">size</span><span class="p">();</span>
<span class="w">  </span><span class="n">variable_list</span><span class="w"> </span><span class="n">backward_inputs</span><span class="p">;</span>
<span class="w">  </span><span class="n">backward_inputs</span><span class="p">.</span><span class="n">reserve</span><span class="p">(</span><span class="n">num_inputs</span><span class="p">);</span>
<span class="w">  </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="k">auto</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">irange</span><span class="p">(</span><span class="n">num_inputs</span><span class="p">))</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">inputs</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="n">defined</span><span class="p">()</span><span class="w"> </span><span class="o">||</span><span class="w"> </span><span class="o">!</span><span class="n">ctx_</span><span class="p">.</span><span class="n">materialize_grads_</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="n">backward_inputs</span><span class="p">.</span><span class="n">emplace_back</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">move</span><span class="p">(</span><span class="n">inputs</span><span class="p">[</span><span class="n">i</span><span class="p">]));</span>
<span class="w">    </span><span class="p">}</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="n">backward_inputs</span><span class="p">.</span><span class="n">emplace_back</span><span class="p">(</span><span class="n">output_info_</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="n">zeros</span><span class="p">(</span><span class="n">_device_guard</span><span class="p">));</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">outputs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">T</span><span class="o">::</span><span class="n">backward</span><span class="p">(</span><span class="o">&amp;</span><span class="n">ctx_</span><span class="p">,</span><span class="w"> </span><span class="n">backward_inputs</span><span class="p">);</span>

<span class="w">  </span><span class="k">const</span><span class="w"> </span><span class="k">auto</span><span class="w"> </span><span class="n">num_forward_inputs</span><span class="w"> </span><span class="o">=</span>
<span class="w">      </span><span class="k">static_cast</span><span class="o">&lt;</span><span class="kt">int64_t</span><span class="o">&gt;</span><span class="p">(</span><span class="n">is_variable_input_</span><span class="p">.</span><span class="n">size</span><span class="p">());</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">num_outputs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">static_cast</span><span class="o">&lt;</span><span class="kt">int64_t</span><span class="o">&gt;</span><span class="p">(</span><span class="n">outputs</span><span class="p">.</span><span class="n">size</span><span class="p">());</span>
<span class="w">  </span><span class="c1">// Returning too many results is ok, but only as long as they&#39;re all</span>
<span class="w">  </span><span class="c1">// undefined. Truncate the result vector in that case.</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">num_outputs</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="n">num_forward_inputs</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="kt">bool</span><span class="w"> </span><span class="n">all_undef</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nb">true</span><span class="p">;</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="k">auto</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">irange</span><span class="p">(</span><span class="n">num_forward_inputs</span><span class="p">,</span><span class="w"> </span><span class="n">num_outputs</span><span class="p">))</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="n">all_undef</span><span class="w"> </span><span class="o">&amp;=</span><span class="w"> </span><span class="p">(</span><span class="o">!</span><span class="n">outputs</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="n">defined</span><span class="p">());</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">all_undef</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="n">outputs</span><span class="p">.</span><span class="n">resize</span><span class="p">(</span><span class="n">num_forward_inputs</span><span class="p">);</span>
<span class="w">      </span><span class="n">num_outputs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">num_forward_inputs</span><span class="p">;</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="n">TORCH_CHECK</span><span class="p">(</span>
<span class="w">      </span><span class="n">num_outputs</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="n">num_forward_inputs</span><span class="p">,</span>
<span class="w">      </span><span class="s">&quot;function &quot;</span><span class="p">,</span>
<span class="w">      </span><span class="n">name</span><span class="p">,</span>
<span class="w">      </span><span class="s">&quot; returned an incorrect number of gradients (expected &quot;</span><span class="p">,</span>
<span class="w">      </span><span class="n">num_forward_inputs</span><span class="p">,</span>
<span class="w">      </span><span class="s">&quot;, got &quot;</span><span class="p">,</span>
<span class="w">      </span><span class="n">num_outputs</span><span class="p">,</span>
<span class="w">      </span><span class="s">&quot;)&quot;</span><span class="p">);</span>

<span class="w">  </span><span class="n">variable_list</span><span class="w"> </span><span class="n">results</span><span class="p">;</span>
<span class="w">  </span><span class="n">results</span><span class="p">.</span><span class="n">reserve</span><span class="p">(</span><span class="n">num_outputs</span><span class="p">);</span>
<span class="w">  </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="k">auto</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">irange</span><span class="p">(</span><span class="n">num_outputs</span><span class="p">))</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="o">!</span><span class="n">is_variable_input_</span><span class="p">[</span><span class="n">i</span><span class="p">])</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="n">TORCH_CHECK</span><span class="p">(</span>
<span class="w">          </span><span class="n">outputs</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="n">defined</span><span class="p">()</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="nb">false</span><span class="p">,</span>
<span class="w">          </span><span class="s">&quot;function &quot;</span><span class="p">,</span>
<span class="w">          </span><span class="n">name</span><span class="p">,</span>
<span class="w">          </span><span class="s">&quot; returned a gradient different that is defined at position &quot;</span><span class="p">,</span>
<span class="w">          </span><span class="n">i</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span>
<span class="w">          </span><span class="s">&quot;, std the corresponding forward input was not a Variable&quot;</span><span class="p">);</span>
<span class="w">      </span><span class="k">continue</span><span class="p">;</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span><span class="n">results</span><span class="p">.</span><span class="n">emplace_back</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="n">i</span><span class="p">]);</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="n">results</span><span class="p">;</span>
<span class="p">}</span>

<span class="k">template</span><span class="w"> </span><span class="o">&lt;</span><span class="k">typename</span><span class="w"> </span><span class="nc">T</span><span class="o">&gt;</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">variable_list</span><span class="w"> </span><span class="n">CppNode_apply_functional_ivalue</span><span class="p">(</span>
<span class="w">    </span><span class="k">const</span><span class="w"> </span><span class="n">variable_list</span><span class="o">&amp;</span><span class="w"> </span><span class="n">inputs</span><span class="p">,</span>
<span class="w">    </span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">c10</span><span class="o">::</span><span class="n">IValue</span><span class="o">&gt;&amp;</span><span class="w"> </span><span class="n">args</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">packed_args</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">PackedArgs</span><span class="p">(</span><span class="n">args</span><span class="p">);</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">ctx</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">AutogradContext</span><span class="p">(</span><span class="n">packed_args</span><span class="p">);</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">output_info</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">packed_args</span><span class="p">.</span><span class="n">unpack</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">VariableInfo</span><span class="o">&gt;&gt;</span><span class="p">();</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">is_variable_input</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">packed_args</span><span class="p">.</span><span class="n">unpack</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">bool</span><span class="o">&gt;&gt;</span><span class="p">();</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">name</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">packed_args</span><span class="p">.</span><span class="n">unpack</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="o">&gt;</span><span class="p">();</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="n">CppNode_apply_functional</span><span class="o">&lt;</span><span class="n">T</span><span class="o">&gt;</span><span class="p">(</span>
<span class="w">      </span><span class="n">variable_list</span><span class="p">(</span><span class="n">inputs</span><span class="p">),</span><span class="w"> </span><span class="n">ctx</span><span class="p">,</span><span class="w"> </span><span class="n">is_variable_input</span><span class="p">,</span><span class="w"> </span><span class="n">output_info</span><span class="p">,</span><span class="w"> </span><span class="n">name</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// CppNode&lt;T&gt; is the Node in the autograd graph that represents the user defined</span>
<span class="c1">// backward function for Function&lt;T&gt;. Calls to CppNode::apply are forward to</span>
<span class="c1">// T::backward().</span>
<span class="k">template</span><span class="w"> </span><span class="o">&lt;</span><span class="k">class</span><span class="w"> </span><span class="nc">T</span><span class="o">&gt;</span>
<span class="k">struct</span><span class="w"> </span><span class="nc">CppNode</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="k">public</span><span class="w"> </span><span class="n">Node</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">variable_list</span><span class="w"> </span><span class="nf">apply</span><span class="p">(</span><span class="n">variable_list</span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="n">inputs</span><span class="p">)</span><span class="w"> </span><span class="k">override</span><span class="p">;</span>
<span class="w">  </span><span class="n">AutogradContext</span><span class="w"> </span><span class="n">ctx_</span><span class="p">;</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">bool</span><span class="o">&gt;</span><span class="w"> </span><span class="n">is_variable_input_</span><span class="p">;</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">VariableInfo</span><span class="o">&gt;</span><span class="w"> </span><span class="n">input_info_</span><span class="p">;</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">VariableInfo</span><span class="o">&gt;</span><span class="w"> </span><span class="n">output_info_</span><span class="p">;</span>

<span class="w">  </span><span class="kt">void</span><span class="w"> </span><span class="nf">release_variables</span><span class="p">()</span><span class="w"> </span><span class="k">override</span><span class="p">;</span>

<span class="w">  </span><span class="kt">void</span><span class="w"> </span><span class="nf">set_ctx_grad_fn</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">Node</span><span class="o">&gt;&amp;</span><span class="w"> </span><span class="n">node</span><span class="p">);</span>
<span class="w">  </span><span class="kt">void</span><span class="w"> </span><span class="nf">save_variables_to_ctx</span><span class="p">();</span>

<span class="w">  </span><span class="kt">void</span><span class="w"> </span><span class="nf">compiled_args</span><span class="p">(</span><span class="n">CompiledNodeArgs</span><span class="o">&amp;</span><span class="w"> </span><span class="n">args</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="k">override</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="c1">// although neither of the 2 methods below have uniqueness guarantees</span>
<span class="w">    </span><span class="c1">// it is unlikely for them to collide at the same time</span>
<span class="w">    </span><span class="n">args</span><span class="p">.</span><span class="n">collect</span><span class="p">(</span><span class="k">static_cast</span><span class="o">&lt;</span><span class="kt">uint64_t</span><span class="o">&gt;</span><span class="p">(</span><span class="k">typeid</span><span class="p">(</span><span class="n">T</span><span class="p">).</span><span class="n">hash_code</span><span class="p">()));</span>
<span class="w">    </span><span class="n">args</span><span class="p">.</span><span class="n">collect</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="p">(</span><span class="k">typeid</span><span class="p">(</span><span class="n">T</span><span class="p">).</span><span class="n">name</span><span class="p">()));</span>

<span class="w">    </span><span class="n">args</span><span class="p">.</span><span class="n">collect</span><span class="p">(</span><span class="n">ctx_</span><span class="p">.</span><span class="n">saved_data</span><span class="p">);</span>
<span class="w">    </span><span class="n">TORCH_INTERNAL_ASSERT</span><span class="p">(</span><span class="n">ctx_</span><span class="p">.</span><span class="n">non_differentiable_</span><span class="p">.</span><span class="n">empty</span><span class="p">());</span>
<span class="w">    </span><span class="n">TORCH_INTERNAL_ASSERT</span><span class="p">(</span><span class="n">ctx_</span><span class="p">.</span><span class="n">dirty_inputs_</span><span class="p">.</span><span class="n">empty</span><span class="p">());</span>
<span class="w">    </span><span class="n">args</span><span class="p">.</span><span class="n">collect</span><span class="p">(</span>
<span class="w">        </span><span class="n">ctx_</span><span class="p">.</span><span class="n">saved_variables_</span><span class="p">,</span><span class="w"> </span><span class="nb">true</span><span class="p">);</span><span class="w"> </span><span class="c1">// always unpacked as output in eager</span>
<span class="w">    </span><span class="n">TORCH_INTERNAL_ASSERT</span><span class="p">(</span><span class="n">ctx_</span><span class="p">.</span><span class="n">to_save_</span><span class="p">.</span><span class="n">empty</span><span class="p">());</span>
<span class="w">    </span><span class="n">args</span><span class="p">.</span><span class="n">collect</span><span class="p">(</span><span class="n">ctx_</span><span class="p">.</span><span class="n">materialize_grads_</span><span class="p">);</span>
<span class="w">    </span><span class="n">args</span><span class="p">.</span><span class="n">collect</span><span class="p">(</span><span class="n">ctx_</span><span class="p">.</span><span class="n">has_freed_buffers_</span><span class="p">);</span>
<span class="w">    </span><span class="n">args</span><span class="p">.</span><span class="n">collect</span><span class="p">(</span><span class="n">is_variable_input_</span><span class="p">);</span>
<span class="w">    </span><span class="n">args</span><span class="p">.</span><span class="n">collect</span><span class="p">(</span><span class="n">input_info_</span><span class="p">);</span>
<span class="w">    </span><span class="n">args</span><span class="p">.</span><span class="n">collect</span><span class="p">(</span><span class="n">output_info_</span><span class="p">);</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="n">variable_list</span><span class="w"> </span><span class="nf">apply_with_saved</span><span class="p">(</span>
<span class="w">      </span><span class="k">const</span><span class="w"> </span><span class="n">variable_list</span><span class="o">&amp;</span><span class="w"> </span><span class="n">inputs</span><span class="p">,</span>
<span class="w">      </span><span class="n">SwapSavedVariables</span><span class="o">&amp;</span><span class="w"> </span><span class="n">saved</span><span class="p">)</span><span class="w"> </span><span class="k">override</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">saved</span><span class="p">.</span><span class="n">before</span><span class="p">(</span><span class="n">ctx_</span><span class="p">.</span><span class="n">saved_data</span><span class="p">);</span>
<span class="w">    </span><span class="n">TORCH_INTERNAL_ASSERT</span><span class="p">(</span><span class="n">ctx_</span><span class="p">.</span><span class="n">non_differentiable_</span><span class="p">.</span><span class="n">empty</span><span class="p">());</span>
<span class="w">    </span><span class="n">TORCH_INTERNAL_ASSERT</span><span class="p">(</span><span class="n">ctx_</span><span class="p">.</span><span class="n">dirty_inputs_</span><span class="p">.</span><span class="n">empty</span><span class="p">());</span>
<span class="w">    </span><span class="n">saved</span><span class="p">.</span><span class="n">before</span><span class="p">(</span><span class="n">ctx_</span><span class="p">.</span><span class="n">saved_variables_</span><span class="p">);</span>
<span class="w">    </span><span class="n">TORCH_INTERNAL_ASSERT</span><span class="p">(</span><span class="n">ctx_</span><span class="p">.</span><span class="n">to_save_</span><span class="p">.</span><span class="n">empty</span><span class="p">());</span>
<span class="w">    </span><span class="n">saved</span><span class="p">.</span><span class="n">before</span><span class="p">(</span><span class="n">ctx_</span><span class="p">.</span><span class="n">materialize_grads_</span><span class="p">);</span>
<span class="w">    </span><span class="n">saved</span><span class="p">.</span><span class="n">before</span><span class="p">(</span><span class="n">ctx_</span><span class="p">.</span><span class="n">has_freed_buffers_</span><span class="p">);</span>
<span class="w">    </span><span class="n">saved</span><span class="p">.</span><span class="n">before</span><span class="p">(</span><span class="n">input_info_</span><span class="p">);</span>
<span class="w">    </span><span class="n">saved</span><span class="p">.</span><span class="n">before</span><span class="p">(</span><span class="n">output_info_</span><span class="p">);</span>

<span class="w">    </span><span class="n">PackedArgs</span><span class="w"> </span><span class="n">packed_args</span><span class="p">;</span>
<span class="w">    </span><span class="n">packed_args</span><span class="p">.</span><span class="n">pack_saved_data</span><span class="p">(</span><span class="n">ctx_</span><span class="p">.</span><span class="n">saved_data</span><span class="p">);</span>
<span class="w">    </span><span class="n">variable_list</span><span class="w"> </span><span class="n">saved_variables</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ctx_</span><span class="p">.</span><span class="n">get_saved_variables</span><span class="p">();</span>
<span class="w">    </span><span class="n">packed_args</span><span class="p">.</span><span class="n">pack</span><span class="p">(</span><span class="n">saved_variables</span><span class="p">);</span>
<span class="w">    </span><span class="n">packed_args</span><span class="p">.</span><span class="n">pack</span><span class="p">(</span><span class="n">ctx_</span><span class="p">.</span><span class="n">materialize_grads_</span><span class="p">);</span>
<span class="w">    </span><span class="n">packed_args</span><span class="p">.</span><span class="n">pack</span><span class="p">(</span><span class="n">ctx_</span><span class="p">.</span><span class="n">has_freed_buffers_</span><span class="p">);</span>

<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">bool</span><span class="o">&gt;</span><span class="w"> </span><span class="n">needs_input_grad</span><span class="p">;</span>
<span class="w">    </span><span class="p">{</span>
<span class="w">      </span><span class="k">auto</span><span class="w"> </span><span class="n">ptr</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ctx_</span><span class="p">.</span><span class="n">grad_fn_</span><span class="p">.</span><span class="n">lock</span><span class="p">();</span>
<span class="w">      </span><span class="n">TORCH_INTERNAL_ASSERT</span><span class="p">(</span><span class="n">ptr</span><span class="p">);</span>
<span class="w">      </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="k">auto</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">irange</span><span class="p">(</span><span class="n">ptr</span><span class="o">-&gt;</span><span class="n">next_edges</span><span class="p">().</span><span class="n">size</span><span class="p">()))</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="n">needs_input_grad</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">ptr</span><span class="o">-&gt;</span><span class="n">task_should_compute_output</span><span class="p">(</span><span class="n">i</span><span class="p">));</span>
<span class="w">      </span><span class="p">}</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span><span class="n">packed_args</span><span class="p">.</span><span class="n">pack</span><span class="p">(</span><span class="n">needs_input_grad</span><span class="p">);</span>

<span class="w">    </span><span class="n">packed_args</span><span class="p">.</span><span class="n">pack</span><span class="p">(</span><span class="n">output_info_</span><span class="p">);</span>
<span class="w">    </span><span class="n">packed_args</span><span class="p">.</span><span class="n">pack</span><span class="p">(</span><span class="n">is_variable_input_</span><span class="p">);</span>
<span class="w">    </span><span class="n">packed_args</span><span class="p">.</span><span class="n">pack</span><span class="p">(</span><span class="n">name</span><span class="p">());</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">args</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">move</span><span class="p">(</span><span class="n">packed_args</span><span class="p">).</span><span class="n">vec</span><span class="p">();</span>

<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">output_metadata</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">dynamo</span><span class="o">::</span><span class="n">autograd</span><span class="o">::</span>
<span class="w">        </span><span class="n">IValuePacker</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">InputMetadata</span><span class="o">&gt;&gt;&gt;::</span><span class="n">pack</span><span class="p">(</span>
<span class="w">            </span><span class="n">torch</span><span class="o">::</span><span class="n">dynamo</span><span class="o">::</span><span class="n">autograd</span><span class="o">::</span><span class="n">get_input_metadata</span><span class="p">(</span><span class="n">next_edges</span><span class="p">()));</span>

<span class="w">    </span><span class="k">const</span><span class="w"> </span><span class="k">auto</span><span class="o">&amp;</span><span class="w"> </span><span class="n">pyinterface</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">dynamo</span><span class="o">::</span><span class="n">autograd</span><span class="o">::</span><span class="n">getPyCompilerInterface</span><span class="p">();</span>

<span class="w">    </span><span class="c1">// Each time apply_with_saved is called, we bind a new function to Python.</span>
<span class="w">    </span><span class="c1">// This is because the schema might be different on compiled autograd cache</span>
<span class="w">    </span><span class="c1">// misses. An alternative is to pass the schema to Python so that it can be</span>
<span class="w">    </span><span class="c1">// an input to a function, but the schema can&#39;t be put into an FX graph</span>
<span class="w">    </span><span class="c1">// right now.</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">TypePtr</span><span class="o">&gt;</span><span class="w"> </span><span class="n">schema</span><span class="p">;</span>
<span class="w">    </span><span class="n">schema</span><span class="p">.</span><span class="n">reserve</span><span class="p">(</span><span class="n">args</span><span class="p">.</span><span class="n">size</span><span class="p">());</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="k">auto</span><span class="o">&amp;</span><span class="w"> </span><span class="n">ivalue</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="n">args</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">ivalue</span><span class="p">.</span><span class="n">isTensor</span><span class="p">())</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="n">schema</span><span class="p">.</span><span class="n">emplace_back</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">TensorType</span><span class="o">::</span><span class="n">get</span><span class="p">());</span>
<span class="w">      </span><span class="p">}</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="n">schema</span><span class="p">.</span><span class="n">emplace_back</span><span class="p">(</span><span class="n">ivalue</span><span class="p">.</span><span class="n">type</span><span class="p">());</span>
<span class="w">      </span><span class="p">}</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span><span class="k">static_assert</span><span class="p">(</span>
<span class="w">        </span><span class="n">std</span><span class="o">::</span><span class="n">is_same_v</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">remove_cv_t</span><span class="o">&lt;</span><span class="k">decltype</span><span class="p">(</span><span class="n">T</span><span class="o">::</span><span class="n">is_traceable</span><span class="p">)</span><span class="o">&gt;</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="o">&gt;</span><span class="p">);</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">fn_name</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">pyinterface</span><span class="o">-&gt;</span><span class="n">bind_function</span><span class="p">(</span>
<span class="w">        </span><span class="n">saved</span><span class="p">.</span><span class="n">get_py_compiler</span><span class="p">(),</span>
<span class="w">        </span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="p">(</span><span class="k">typeid</span><span class="p">(</span><span class="n">T</span><span class="p">).</span><span class="n">name</span><span class="p">()),</span>
<span class="w">        </span><span class="n">CppNode_apply_functional_ivalue</span><span class="o">&lt;</span><span class="n">T</span><span class="o">&gt;</span><span class="p">,</span>
<span class="w">        </span><span class="n">schema</span><span class="p">,</span>
<span class="w">        </span><span class="cm">/*is_custom_function*/</span><span class="w"> </span><span class="nb">true</span><span class="p">,</span>
<span class="w">        </span><span class="cm">/*is_traceable*/</span><span class="w"> </span><span class="n">T</span><span class="o">::</span><span class="n">is_traceable</span><span class="p">);</span>

<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">results</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">pyinterface</span><span class="o">-&gt;</span><span class="n">call_function</span><span class="p">(</span>
<span class="w">        </span><span class="n">saved</span><span class="p">.</span><span class="n">get_py_compiler</span><span class="p">(),</span>
<span class="w">        </span><span class="s">&quot;apply_functional&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="n">fn_name</span><span class="p">,</span>
<span class="w">        </span><span class="n">inputs</span><span class="p">,</span>
<span class="w">        </span><span class="n">args</span><span class="p">,</span>
<span class="w">        </span><span class="n">output_metadata</span><span class="p">);</span>

<span class="w">    </span><span class="n">saved</span><span class="p">.</span><span class="n">after</span><span class="p">(</span><span class="n">ctx_</span><span class="p">.</span><span class="n">saved_data</span><span class="p">);</span>
<span class="w">    </span><span class="n">TORCH_INTERNAL_ASSERT</span><span class="p">(</span><span class="n">ctx_</span><span class="p">.</span><span class="n">non_differentiable_</span><span class="p">.</span><span class="n">empty</span><span class="p">());</span>
<span class="w">    </span><span class="n">TORCH_INTERNAL_ASSERT</span><span class="p">(</span><span class="n">ctx_</span><span class="p">.</span><span class="n">dirty_inputs_</span><span class="p">.</span><span class="n">empty</span><span class="p">());</span>
<span class="w">    </span><span class="n">saved</span><span class="p">.</span><span class="n">after</span><span class="p">(</span><span class="n">ctx_</span><span class="p">.</span><span class="n">saved_variables_</span><span class="p">);</span>
<span class="w">    </span><span class="n">TORCH_INTERNAL_ASSERT</span><span class="p">(</span><span class="n">ctx_</span><span class="p">.</span><span class="n">to_save_</span><span class="p">.</span><span class="n">empty</span><span class="p">());</span>
<span class="w">    </span><span class="n">saved</span><span class="p">.</span><span class="n">after</span><span class="p">(</span><span class="n">ctx_</span><span class="p">.</span><span class="n">materialize_grads_</span><span class="p">);</span>
<span class="w">    </span><span class="n">saved</span><span class="p">.</span><span class="n">after</span><span class="p">(</span><span class="n">ctx_</span><span class="p">.</span><span class="n">has_freed_buffers_</span><span class="p">);</span>
<span class="w">    </span><span class="n">saved</span><span class="p">.</span><span class="n">after</span><span class="p">(</span><span class="n">input_info_</span><span class="p">);</span>
<span class="w">    </span><span class="n">saved</span><span class="p">.</span><span class="n">after</span><span class="p">(</span><span class="n">output_info_</span><span class="p">);</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">results</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>
<span class="p">};</span>

<span class="k">struct</span><span class="w"> </span><span class="nc">ExtractVariables</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="n">IterArgs</span><span class="o">&lt;</span><span class="n">ExtractVariables</span><span class="o">&gt;</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="c1">// NOLINTNEXTLINE(cppcoreguidelines-avoid-const-or-ref-data-members)</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">bool</span><span class="o">&gt;&amp;</span><span class="w"> </span><span class="n">is_var_</span><span class="p">;</span>
<span class="w">  </span><span class="c1">// NOLINTNEXTLINE(cppcoreguidelines-avoid-const-or-ref-data-members)</span>
<span class="w">  </span><span class="n">variable_list</span><span class="o">&amp;</span><span class="w"> </span><span class="n">list_</span><span class="p">;</span>
<span class="w">  </span><span class="n">ExtractVariables</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">bool</span><span class="o">&gt;&amp;</span><span class="w"> </span><span class="n">is_var</span><span class="p">,</span><span class="w"> </span><span class="n">variable_list</span><span class="o">&amp;</span><span class="w"> </span><span class="n">list</span><span class="p">)</span>
<span class="w">      </span><span class="o">:</span><span class="w"> </span><span class="n">is_var_</span><span class="p">(</span><span class="n">is_var</span><span class="p">),</span><span class="w"> </span><span class="n">list_</span><span class="p">(</span><span class="n">list</span><span class="p">)</span><span class="w"> </span><span class="p">{}</span>
<span class="w">  </span><span class="kt">void</span><span class="w"> </span><span class="k">operator</span><span class="p">()(</span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;&amp;</span><span class="w"> </span><span class="n">x</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">has_value</span><span class="p">()</span><span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="n">x</span><span class="p">.</span><span class="n">value</span><span class="p">().</span><span class="n">defined</span><span class="p">())</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="n">is_var_</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="nb">true</span><span class="p">);</span>
<span class="w">      </span><span class="n">list_</span><span class="p">.</span><span class="n">emplace_back</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">value</span><span class="p">());</span>
<span class="w">    </span><span class="p">}</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="n">is_var_</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="nb">false</span><span class="p">);</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="kt">void</span><span class="w"> </span><span class="k">operator</span><span class="p">()(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&amp;</span><span class="w"> </span><span class="n">x</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">is_var_</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="nb">true</span><span class="p">);</span>
<span class="w">    </span><span class="n">list_</span><span class="p">.</span><span class="n">emplace_back</span><span class="p">(</span><span class="n">x</span><span class="p">);</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="kt">void</span><span class="w"> </span><span class="k">operator</span><span class="p">()(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">TensorList</span><span class="o">&amp;</span><span class="w"> </span><span class="n">list</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&amp;</span><span class="w"> </span><span class="n">x</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="n">list</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="n">is_var_</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="nb">true</span><span class="p">);</span>
<span class="w">      </span><span class="n">list_</span><span class="p">.</span><span class="n">emplace_back</span><span class="p">(</span><span class="n">x</span><span class="p">);</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="k">template</span><span class="w"> </span><span class="o">&lt;</span><span class="k">typename</span><span class="w"> </span><span class="nc">T</span><span class="o">&gt;</span>
<span class="w">  </span><span class="kt">void</span><span class="w"> </span><span class="k">operator</span><span class="p">()(</span><span class="k">const</span><span class="w"> </span><span class="n">T</span><span class="o">&amp;</span><span class="w"> </span><span class="n">x</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">is_var_</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="nb">false</span><span class="p">);</span>
<span class="w">  </span><span class="p">}</span>
<span class="p">};</span>

<span class="k">template</span><span class="w"> </span><span class="o">&lt;</span><span class="k">typename</span><span class="p">...</span><span class="w"> </span><span class="n">Args</span><span class="o">&gt;</span>
<span class="kr">inline</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="n">extract_vars</span><span class="p">(</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">bool</span><span class="o">&gt;&amp;</span><span class="w"> </span><span class="n">is_var</span><span class="p">,</span>
<span class="w">    </span><span class="n">variable_list</span><span class="o">&amp;</span><span class="w"> </span><span class="n">list</span><span class="p">,</span>
<span class="w">    </span><span class="n">Args</span><span class="o">&amp;&amp;</span><span class="p">...</span><span class="w"> </span><span class="n">args</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">ExtractVariables</span><span class="p">(</span><span class="n">is_var</span><span class="p">,</span><span class="w"> </span><span class="n">list</span><span class="p">).</span><span class="n">apply</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">forward</span><span class="o">&lt;</span><span class="n">Args</span><span class="o">&gt;</span><span class="p">(</span><span class="n">args</span><span class="p">)...);</span>
<span class="p">}</span>

<span class="k">template</span><span class="w"> </span><span class="o">&lt;</span><span class="k">typename</span><span class="w"> </span><span class="nc">T</span><span class="o">&gt;</span>
<span class="n">std</span><span class="o">::</span><span class="n">enable_if_t</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">is_same_v</span><span class="o">&lt;</span><span class="n">T</span><span class="p">,</span><span class="w"> </span><span class="n">variable_list</span><span class="o">&gt;</span><span class="p">,</span><span class="w"> </span><span class="n">T</span><span class="o">&gt;</span><span class="w"> </span><span class="n">to_output_type</span><span class="p">(</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">Variable</span><span class="o">&gt;&gt;&amp;</span><span class="w"> </span><span class="n">output_list</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">variable_list</span><span class="w"> </span><span class="n">result</span><span class="p">;</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">transform</span><span class="p">(</span>
<span class="w">      </span><span class="n">output_list</span><span class="p">.</span><span class="n">begin</span><span class="p">(),</span>
<span class="w">      </span><span class="n">output_list</span><span class="p">.</span><span class="n">end</span><span class="p">(),</span>
<span class="w">      </span><span class="n">std</span><span class="o">::</span><span class="n">back_inserter</span><span class="p">(</span><span class="n">result</span><span class="p">),</span>
<span class="w">      </span><span class="p">[](</span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">Variable</span><span class="o">&gt;&amp;</span><span class="w"> </span><span class="n">var</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="k">return</span><span class="w"> </span><span class="o">*</span><span class="n">var</span><span class="p">;</span><span class="w"> </span><span class="p">});</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="n">result</span><span class="p">;</span>
<span class="p">}</span>

<span class="k">template</span><span class="w"> </span><span class="o">&lt;</span><span class="k">typename</span><span class="w"> </span><span class="nc">T</span><span class="o">&gt;</span>
<span class="n">std</span><span class="o">::</span><span class="n">enable_if_t</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">is_same_v</span><span class="o">&lt;</span><span class="n">T</span><span class="p">,</span><span class="w"> </span><span class="n">Variable</span><span class="o">&gt;</span><span class="p">,</span><span class="w"> </span><span class="n">T</span><span class="o">&gt;</span><span class="w"> </span><span class="n">to_output_type</span><span class="p">(</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">Variable</span><span class="o">&gt;&gt;&amp;</span><span class="w"> </span><span class="n">output_list</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="o">*</span><span class="n">output_list</span><span class="p">[</span><span class="mi">0</span><span class="p">];</span>
<span class="p">}</span>

<span class="kr">inline</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">Variable</span><span class="o">&gt;&gt;</span><span class="w"> </span><span class="n">to_optional</span><span class="p">(</span><span class="n">Variable</span><span class="o">&amp;</span><span class="w"> </span><span class="n">output</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">Variable</span><span class="o">&gt;&gt;</span><span class="p">{</span><span class="n">output</span><span class="p">};</span>
<span class="p">}</span>

<span class="kr">inline</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">Variable</span><span class="o">&gt;&gt;</span><span class="w"> </span><span class="n">to_optional</span><span class="p">(</span><span class="n">variable_list</span><span class="o">&amp;</span><span class="w"> </span><span class="n">output</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">Variable</span><span class="o">&gt;&gt;</span><span class="w"> </span><span class="n">result</span><span class="p">;</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">transform</span><span class="p">(</span>
<span class="w">      </span><span class="n">output</span><span class="p">.</span><span class="n">begin</span><span class="p">(),</span>
<span class="w">      </span><span class="n">output</span><span class="p">.</span><span class="n">end</span><span class="p">(),</span>
<span class="w">      </span><span class="n">std</span><span class="o">::</span><span class="n">back_inserter</span><span class="p">(</span><span class="n">result</span><span class="p">),</span>
<span class="w">      </span><span class="p">[](</span><span class="k">const</span><span class="w"> </span><span class="n">Variable</span><span class="o">&amp;</span><span class="w"> </span><span class="n">var</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="k">return</span><span class="w"> </span><span class="n">var</span><span class="p">;</span><span class="w"> </span><span class="p">});</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="n">result</span><span class="p">;</span>
<span class="p">}</span>

<span class="k">template</span><span class="w"> </span><span class="o">&lt;</span><span class="k">class</span><span class="w"> </span><span class="nc">T</span><span class="o">&gt;</span>
<span class="k">template</span><span class="w"> </span><span class="o">&lt;</span><span class="k">typename</span><span class="w"> </span><span class="nc">X</span><span class="p">,</span><span class="w"> </span><span class="k">typename</span><span class="p">...</span><span class="w"> </span><span class="n">Args</span><span class="o">&gt;</span>
<span class="k">auto</span><span class="w"> </span><span class="n">Function</span><span class="o">&lt;</span><span class="n">T</span><span class="o">&gt;::</span><span class="n">apply</span><span class="p">(</span><span class="n">Args</span><span class="o">&amp;&amp;</span><span class="p">...</span><span class="w"> </span><span class="n">args</span><span class="p">)</span>
<span class="w">    </span><span class="o">-&gt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">enable_if_t</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">is_same_v</span><span class="o">&lt;</span><span class="n">X</span><span class="p">,</span><span class="w"> </span><span class="n">T</span><span class="o">&gt;</span><span class="p">,</span><span class="w"> </span><span class="n">forward_t</span><span class="o">&lt;</span><span class="n">X</span><span class="p">,</span><span class="w"> </span><span class="n">Args</span><span class="p">...</span><span class="o">&gt;&gt;</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="k">const</span><span class="w"> </span><span class="k">auto</span><span class="o">&amp;</span><span class="w"> </span><span class="n">functorch_tls</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">functorch</span><span class="o">::</span><span class="n">functorchTLSAccessor</span><span class="p">();</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">functorch_tls</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="c1">// Function support for functorch is handled in Python.</span>
<span class="w">    </span><span class="c1">// Here we are dealing with a (C++) Function, which is not supported.</span>
<span class="w">    </span><span class="c1">// Let&#39;s raise an error instead of being silently incorrect.</span>
<span class="w">    </span><span class="n">functorch_tls</span><span class="o">-&gt;</span><span class="n">checkSupportsCppAutogradFunction</span><span class="p">();</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">CppNode</span><span class="o">&lt;</span><span class="n">T</span><span class="o">&gt;&gt;</span><span class="w"> </span><span class="n">node</span><span class="p">(</span><span class="k">new</span><span class="w"> </span><span class="n">CppNode</span><span class="o">&lt;</span><span class="n">T</span><span class="o">&gt;</span><span class="p">(),</span><span class="w"> </span><span class="n">deleteNode</span><span class="p">);</span>
<span class="w">  </span><span class="n">variable_list</span><span class="w"> </span><span class="n">input_vars</span><span class="p">;</span>

<span class="w">  </span><span class="k">const</span><span class="w"> </span><span class="kt">size_t</span><span class="w"> </span><span class="n">num_inputs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">sizeof</span><span class="p">...(</span><span class="n">Args</span><span class="p">);</span>
<span class="w">  </span><span class="n">input_vars</span><span class="p">.</span><span class="n">reserve</span><span class="p">(</span><span class="n">num_inputs</span><span class="p">);</span>
<span class="w">  </span><span class="n">node</span><span class="o">-&gt;</span><span class="n">is_variable_input_</span><span class="p">.</span><span class="n">reserve</span><span class="p">(</span><span class="n">num_inputs</span><span class="p">);</span>
<span class="w">  </span><span class="c1">// TODO Add tracing here</span>
<span class="w">  </span><span class="n">extract_vars</span><span class="p">(</span><span class="n">node</span><span class="o">-&gt;</span><span class="n">is_variable_input_</span><span class="p">,</span><span class="w"> </span><span class="n">input_vars</span><span class="p">,</span><span class="w"> </span><span class="n">args</span><span class="p">...);</span>

<span class="w">  </span><span class="kt">bool</span><span class="w"> </span><span class="n">is_executable</span><span class="w"> </span><span class="o">=</span>
<span class="w">      </span><span class="n">GradMode</span><span class="o">::</span><span class="n">is_enabled</span><span class="p">()</span><span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="n">any_variable_requires_grad</span><span class="p">(</span><span class="n">input_vars</span><span class="p">);</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">next_edges</span><span class="w"> </span><span class="o">=</span>
<span class="w">      </span><span class="p">(</span><span class="n">is_executable</span><span class="w"> </span><span class="o">?</span><span class="w"> </span><span class="n">collect_next_edges</span><span class="p">(</span><span class="n">input_vars</span><span class="p">)</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="n">edge_list</span><span class="p">());</span>
<span class="w">  </span><span class="n">node</span><span class="o">-&gt;</span><span class="n">set_ctx_grad_fn</span><span class="p">(</span><span class="n">node</span><span class="p">);</span>
<span class="w">  </span><span class="n">node</span><span class="o">-&gt;</span><span class="n">set_next_edges</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">move</span><span class="p">(</span><span class="n">next_edges</span><span class="p">));</span>
<span class="w">  </span><span class="n">node</span><span class="o">-&gt;</span><span class="n">clear_input_metadata</span><span class="p">();</span>

<span class="w">  </span><span class="n">node</span><span class="o">-&gt;</span><span class="n">input_info_</span><span class="p">.</span><span class="n">reserve</span><span class="p">(</span><span class="n">input_vars</span><span class="p">.</span><span class="n">size</span><span class="p">());</span>
<span class="w">  </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="k">auto</span><span class="o">&amp;</span><span class="w"> </span><span class="n">var</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="n">input_vars</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">node</span><span class="o">-&gt;</span><span class="n">input_info_</span><span class="p">.</span><span class="n">emplace_back</span><span class="p">(</span><span class="n">var</span><span class="p">);</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="k">using</span><span class="w"> </span><span class="n">forward_return_t</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">forward_t</span><span class="o">&lt;</span><span class="n">X</span><span class="p">,</span><span class="w"> </span><span class="n">Args</span><span class="p">...</span><span class="o">&gt;</span><span class="p">;</span>
<span class="w">  </span><span class="n">forward_return_t</span><span class="w"> </span><span class="n">outputs</span><span class="p">;</span>
<span class="w">  </span><span class="p">{</span>
<span class="w">    </span><span class="n">AutoGradMode</span><span class="w"> </span><span class="nf">grad_mode</span><span class="p">(</span><span class="nb">false</span><span class="p">);</span>
<span class="w">    </span><span class="n">outputs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">T</span><span class="o">::</span><span class="n">forward</span><span class="p">(</span><span class="o">&amp;</span><span class="n">node</span><span class="o">-&gt;</span><span class="n">ctx_</span><span class="p">,</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">forward</span><span class="o">&lt;</span><span class="n">Args</span><span class="o">&gt;</span><span class="p">(</span><span class="n">args</span><span class="p">)...);</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="n">_jvp_fn_t</span><span class="w"> </span><span class="n">jvp_fn</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">[](</span><span class="k">const</span><span class="w"> </span><span class="n">variable_list</span><span class="o">&amp;</span><span class="w"> </span><span class="n">inputs</span><span class="p">,</span>
<span class="w">                        </span><span class="k">const</span><span class="w"> </span><span class="n">variable_list</span><span class="o">&amp;</span><span class="w"> </span><span class="n">gI</span><span class="p">)</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="n">variable_list</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">TORCH_CHECK</span><span class="p">(</span>
<span class="w">        </span><span class="nb">false</span><span class="p">,</span>
<span class="w">        </span><span class="s">&quot;jvp is not implemented for the c++ API of custom Function yet.&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="s">&quot;Please open a feature request on GitHub if you need this.&quot;</span><span class="p">);</span>
<span class="w">  </span><span class="p">};</span>

<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">view_as_self_fn</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">[](</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&amp;</span><span class="w"> </span><span class="n">x</span><span class="p">)</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">x</span><span class="p">.</span><span class="n">view_as</span><span class="p">(</span><span class="n">x</span><span class="p">);</span>
<span class="w">  </span><span class="p">};</span>

<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">wrapped_outputs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">_wrap_outputs</span><span class="p">(</span>
<span class="w">      </span><span class="n">input_vars</span><span class="p">,</span>
<span class="w">      </span><span class="n">node</span><span class="o">-&gt;</span><span class="n">ctx_</span><span class="p">.</span><span class="n">get_non_differentiable</span><span class="p">(),</span>
<span class="w">      </span><span class="n">node</span><span class="o">-&gt;</span><span class="n">ctx_</span><span class="p">.</span><span class="n">get_and_bump_dirty</span><span class="p">(),</span>
<span class="w">      </span><span class="n">to_optional</span><span class="p">(</span><span class="n">outputs</span><span class="p">),</span>
<span class="w">      </span><span class="n">is_executable</span><span class="w"> </span><span class="o">?</span><span class="w"> </span><span class="n">node</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="k">nullptr</span><span class="p">,</span>
<span class="w">      </span><span class="n">jvp_fn</span><span class="p">,</span>
<span class="w">      </span><span class="p">{},</span>
<span class="w">      </span><span class="n">view_as_self_fn</span><span class="p">);</span>

<span class="w">  </span><span class="n">node</span><span class="o">-&gt;</span><span class="n">output_info_</span><span class="p">.</span><span class="n">reserve</span><span class="p">(</span><span class="n">wrapped_outputs</span><span class="p">.</span><span class="n">size</span><span class="p">());</span>
<span class="w">  </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="k">auto</span><span class="o">&amp;</span><span class="w"> </span><span class="n">output</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="n">wrapped_outputs</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">is_executable</span><span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="n">output</span><span class="p">.</span><span class="n">has_value</span><span class="p">())</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="n">node</span><span class="o">-&gt;</span><span class="n">output_info_</span><span class="p">.</span><span class="n">emplace_back</span><span class="p">(</span><span class="n">output</span><span class="p">.</span><span class="n">value</span><span class="p">());</span>
<span class="w">    </span><span class="p">}</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">is_executable</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="n">node</span><span class="o">-&gt;</span><span class="n">output_info_</span><span class="p">.</span><span class="n">emplace_back</span><span class="p">();</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">is_executable</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">node</span><span class="o">-&gt;</span><span class="n">save_variables_to_ctx</span><span class="p">();</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="c1">// wrapped_outputs will be a variable_list so, convert it to the correct</span>
<span class="w">  </span><span class="c1">// return type. Only Variable and variable_list are accepted as return types.</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="n">to_output_type</span><span class="o">&lt;</span><span class="n">forward_return_t</span><span class="o">&gt;</span><span class="p">(</span><span class="n">wrapped_outputs</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// The logic here is the same as PyNode::apply, so changes to it should be done</span>
<span class="c1">// in both the places</span>
<span class="k">template</span><span class="w"> </span><span class="o">&lt;</span><span class="k">class</span><span class="w"> </span><span class="nc">T</span><span class="o">&gt;</span>
<span class="c1">// NOLINTNEXTLINE(cppcoreguidelines-rvalue-reference-param-not-moved)</span>
<span class="n">variable_list</span><span class="w"> </span><span class="n">CppNode</span><span class="o">&lt;</span><span class="n">T</span><span class="o">&gt;::</span><span class="n">apply</span><span class="p">(</span><span class="n">variable_list</span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="n">inputs</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="c1">// Acquire lock to here protect thread safety on custom C++ Autograd Node</span>
<span class="w">  </span><span class="c1">// This is needed for the custom Autograd Node since we don&#39;t know if the</span>
<span class="w">  </span><span class="c1">// user defined Node will write to the shared data during backward.</span>
<span class="w">  </span><span class="c1">// see Note [Thread Safety on Autograd Node]</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">lock_guard</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">mutex</span><span class="o">&gt;</span><span class="w"> </span><span class="n">lock</span><span class="p">(</span><span class="n">mutex_</span><span class="p">);</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="n">CppNode_apply_functional</span><span class="o">&lt;</span><span class="n">T</span><span class="o">&gt;</span><span class="p">(</span>
<span class="w">      </span><span class="n">std</span><span class="o">::</span><span class="n">move</span><span class="p">(</span><span class="n">inputs</span><span class="p">),</span><span class="w"> </span><span class="n">ctx_</span><span class="p">,</span><span class="w"> </span><span class="n">is_variable_input_</span><span class="p">,</span><span class="w"> </span><span class="n">output_info_</span><span class="p">,</span><span class="w"> </span><span class="n">name</span><span class="p">());</span>
<span class="p">}</span>

<span class="k">template</span><span class="w"> </span><span class="o">&lt;</span><span class="k">class</span><span class="w"> </span><span class="nc">T</span><span class="o">&gt;</span>
<span class="kt">void</span><span class="w"> </span><span class="n">CppNode</span><span class="o">&lt;</span><span class="n">T</span><span class="o">&gt;::</span><span class="n">release_variables</span><span class="p">()</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="c1">// lock to ensure thread safety, see [Thread Safety on Autograd Node]</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">lock_guard</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">mutex</span><span class="o">&gt;</span><span class="w"> </span><span class="n">lock</span><span class="p">(</span><span class="n">mutex_</span><span class="p">);</span>
<span class="w">  </span><span class="n">ctx_</span><span class="p">.</span><span class="n">saved_variables_</span><span class="p">.</span><span class="n">clear</span><span class="p">();</span>
<span class="w">  </span><span class="n">ctx_</span><span class="p">.</span><span class="n">has_freed_buffers_</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nb">true</span><span class="p">;</span>
<span class="p">}</span>

<span class="k">template</span><span class="w"> </span><span class="o">&lt;</span><span class="k">class</span><span class="w"> </span><span class="nc">T</span><span class="o">&gt;</span>
<span class="kt">void</span><span class="w"> </span><span class="n">CppNode</span><span class="o">&lt;</span><span class="n">T</span><span class="o">&gt;::</span><span class="n">save_variables_to_ctx</span><span class="p">()</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">ctx_</span><span class="p">.</span><span class="n">save_variables</span><span class="p">();</span>
<span class="p">}</span>

<span class="k">template</span><span class="w"> </span><span class="o">&lt;</span><span class="k">class</span><span class="w"> </span><span class="nc">T</span><span class="o">&gt;</span>
<span class="kt">void</span><span class="w"> </span><span class="n">CppNode</span><span class="o">&lt;</span><span class="n">T</span><span class="o">&gt;::</span><span class="n">set_ctx_grad_fn</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">Node</span><span class="o">&gt;&amp;</span><span class="w"> </span><span class="n">node</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">ctx_</span><span class="p">.</span><span class="n">grad_fn_</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">node</span><span class="p">;</span>
<span class="p">}</span>

<span class="p">}</span><span class="w"> </span><span class="c1">// namespace torch::autograd</span>
</pre></div>
</div>
</div>


                </article>
              
  </article>
  
              
              
                <footer class="bd-footer-article">
                  <div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item">
<div class="feedback">
  
<div class="rating">
    Rate this Page
    <div class="stars">
        
        <span class="star" data-behavior="tutorial-rating" data-count="1" data-value="1">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="2" data-value="2">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="3" data-value="3">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="4" data-value="4">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="5" data-value="5">★</span>
        
    </div>
</div>

  <div class="feedback-send">
    <button class="feedback-btn"
            onclick="openGitHubIssue()"
            data-bs-title="Create a GitHub Issue"
            data-bs-placement="bottom"
            data-bs-toggle="tooltip"
            data-gtm="feedback-btn-click">Send Feedback
    </button>
  </div>
</div>

<div class="prev-next-area">
    <a class="left-prev"
       href="file_torch_csrc_autograd_custom_function.h.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">File custom_function.h</p>
      </div>
    </a>
    <a class="right-next"
       href="file_torch_csrc_jit_runtime_custom_operator.h.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">File custom_operator.h</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>

<div class="footer-info">
  <p class="copyright">
    
      
        © Copyright PyTorch Contributors.
      
      <br/>
    
  </p>

  <p class="theme-version">
    Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.15.4.
  </p>
</div>
</div>
  
</div>
                </footer>
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="file_torch_csrc_autograd_custom_function.h.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">File custom_function.h</p>
      </div>
    </a>
    <a class="right-next"
       href="file_torch_csrc_jit_runtime_custom_operator.h.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">File custom_operator.h</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc">
<div class="sidebar-secondary-items sidebar-secondary__inner">
    
       <div class="sidebar-secondary-item">
    <div class="tocsection sourcelink">
      <a href="../_sources/api/program_listing_file_torch_csrc_autograd_custom_function.h.rst.txt">
        <i class="fa-solid fa-file-lines"></i> Show Source
      </a>
    </div>
</div>
    




<div class="sidebar-secondary-item">
  <div class="sidebar-heading">PyTorch Libraries</div>
  <ul style="list-style-type: none; padding: 0; padding-bottom: 80px;">
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/ao" style="color: var(--pst-color-text-muted)">torchao</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/torchrec" style="color: var(--pst-color-text-muted)">torchrec</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/torchft" style="color: var(--pst-color-text-muted)">torchft</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/torchcodec" style="color: var(--pst-color-text-muted)">TorchCodec</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/vision" style="color: var(--pst-color-text-muted)">torchvision</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/executorch" style="color: var(--pst-color-text-muted)">ExecuTorch</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/xla" style="color: var(--pst-color-text-muted)">PyTorch on XLA Devices</a></li>
  
  </ul>
</div>

</div>
</div>
              
            
          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
  <div class="container">
    <div class="row">
      <div class="col-md-4">
        <h2>Docs</h2>
        <p>Access comprehensive developer documentation for PyTorch</p>
        <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
      </div>

      <div class="col-md-4">
        <h2>Tutorials</h2>
        <p>Get in-depth tutorials for beginners and advanced developers</p>
        <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
      </div>

      <div class="col-md-4">
        <h2>Resources</h2>
        <p>Find development resources and get your questions answered</p>
        <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
      </div>
    </div>
  </div>
</div>

<footer class="site-footer">
  <div class="container footer-container">

    
    <div class="newsletter" id="newsletter">

      <p class="newsletter__title is-style-max-width-800"><strong>Stay in touch</strong> for updates, event info, and
        the latest news</p>


      <script charset="utf-8" type="text/javascript" src="//js.hsforms.net/forms/embed/v2.js"></script>
      <script>
        hbspt.forms.create({
          region: "na1",
          portalId: "8112310",
          formId: "2fb2231c-000b-4ec5-88a0-1ab242549c9e"
        });
      </script>


      <p class="newsletter__privacy">By submitting this form, I consent to receive marketing emails from the LF and its
        projects regarding their events, training, research, developments, and related announcements. I understand that
        I can unsubscribe at any time using the links in the footers of the emails I receive. <a
          href="https://www.linuxfoundation.org/privacy/">Privacy Policy</a>.</p>

    </div>
    

    <div class="lf-grid">
      <ul class="social-links">
        <li><a href="https://www.facebook.com/pytorch" target="_blank" title="PyTorch on Facebook">
            <svg xmlns="http://www.w3.org/2000/svg" viewbox="-0.51 -0.26 26.45 26.45" aria-label="Facebook">
              <path fill="currentColor"
                d="M25.497 13.075c0-2.45-.698-4.848-2.011-6.911a12.765 12.765 0 0 0-5.398-4.73A12.671 12.671 0 0 0 11.008.38a12.705 12.705 0 0 0-6.529 2.95A12.827 12.827 0 0 0 .563 9.358a12.896 12.896 0 0 0-.07 7.201 12.831 12.831 0 0 0 3.801 6.103 12.709 12.709 0 0 0 6.471 3.078v-8.957H7.53v-3.708h3.235v-2.824c0-3.213 1.903-4.988 4.813-4.988.956.014 1.909.097 2.852.25V8.67h-1.607a1.83 1.83 0 0 0-1.518.497 1.854 1.854 0 0 0-.561 1.505v2.404h3.535l-.563 3.708h-2.97v8.957a12.725 12.725 0 0 0 7.697-4.337 12.87 12.87 0 0 0 3.054-8.328z" />
            </svg>
          </a></li>
        <li><a href="https://twitter.com/pytorch" target="_blank" title="PyTorch on X">
            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 300 300" aria-label="X">
              <path fill="currentColor"
                d="M178.57 127.15 290.27 0h-26.46l-97.03 110.38L89.34 0H0l117.13 166.93L0 300.25h26.46l102.4-116.59 81.8 116.59h89.34M36.01 19.54H76.66l187.13 262.13h-40.66" />
            </svg>
          </a></li>
        <li><a href="https://www.youtube.com/pytorch" target="_blank" title="PyTorch on YouTube">
            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0.21 0.27 34.45 25.07" aria-label="YouTube">
              <path fill="currentColor"
                d="M33.729 6.084s-.327-2.33-1.317-3.356a4.691 4.691 0 0 0-3.32-1.432c-4.634-.34-11.589-.34-11.589-.34h-.014s-6.954 0-11.59.342a4.692 4.692 0 0 0-3.32 1.432c-.993 1.025-1.315 3.354-1.315 3.354a52.189 52.189 0 0 0-.331 5.473v2.566c.014 1.829.125 3.656.331 5.472 0 0 .322 2.33 1.316 3.36 1.26 1.345 2.916 1.3 3.653 1.445 2.65.26 11.263.34 11.263.34s6.96-.01 11.597-.353a4.691 4.691 0 0 0 3.32-1.432c.993-1.026 1.316-3.356 1.316-3.356.206-1.817.316-3.644.33-5.473v-2.57a52.26 52.26 0 0 0-.33-5.472zM14.076 17.232V7.729l8.951 4.768-8.95 4.735z" />
            </svg>
          </a></li>
        <li><a href="https://www.linkedin.com/company/pytorch" target="_blank" title="PyTorch on LinkedIn">
            <svg xmlns="http://www.w3.org/2000/svg" viewbox="-10.23 -10.23 531.96 531.96" aria-label="LinkedIn">
              <rect width="512" height="512" rx="0" fill="currentColor" />
              <circle fill="#000" cx="142" cy="138" r="37" />
              <path stroke="#000" stroke-width="66" d="M244 194v198M142 194v198" />
              <path fill="#000"
                d="M276 282c0-20 13-40 36-40 24 0 33 18 33 45v105h66V279c0-61-32-89-76-89-34 0-51 19-59 32" />
            </svg>
          </a></li>
        <li><a href="https://pytorch.slack.com" target="_blank" title="PyTorch Slack">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0.16 -0.03 21.19 21.19" aria-label="Slack">
              <path fill="currentColor"
                d="M4.896 13.27a2.147 2.147 0 0 1-2.141 2.142A2.147 2.147 0 0 1 .613 13.27c0-1.178.963-2.141 2.142-2.141h2.141v2.141zm1.08 0c0-1.178.962-2.141 2.141-2.141s2.142.963 2.142 2.141v5.363a2.147 2.147 0 0 1-2.142 2.141 2.147 2.147 0 0 1-2.141-2.142V13.27zm2.141-8.6a2.147 2.147 0 0 1-2.141-2.14c0-1.18.962-2.142 2.141-2.142s2.142.963 2.142 2.141v2.142H8.117zm0 1.08c1.179 0 2.141.962 2.141 2.141a2.147 2.147 0 0 1-2.141 2.142H2.755A2.147 2.147 0 0 1 .613 7.89c0-1.179.963-2.141 2.142-2.141h5.362zm8.599 2.141c0-1.179.963-2.141 2.141-2.141 1.179 0 2.143.962 2.143 2.14a2.147 2.147 0 0 1-2.142 2.142h-2.141V7.89zm-1.08 0a2.147 2.147 0 0 1-2.141 2.142 2.147 2.147 0 0 1-2.141-2.142V2.53c0-1.178.962-2.141 2.141-2.141s2.142.963 2.142 2.141v5.362zm-2.141 8.6c1.179 0 2.142.962 2.142 2.14a2.147 2.147 0 0 1-2.142 2.142 2.147 2.147 0 0 1-2.141-2.141V16.49h2.141zm0-1.08a2.147 2.147 0 0 1-2.141-2.141c0-1.179.962-2.142 2.141-2.142h5.362c1.179 0 2.142.963 2.142 2.142a2.147 2.147 0 0 1-2.142 2.142h-5.362z">
              </path>
            </svg>
          </a></li>
        <li><a href="https://pytorch.org/wechat" title="PyTorch on WeChat">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0.14 -0.17 38.02 33.02" aria-label="WeChat">
              <path fill="currentColor"
                d="M26.289 10.976a12.972 12.972 0 0 0-8.742 3.53 10.386 10.386 0 0 0-3.224 8.795c-1.326-.164-2.535-.345-3.75-.448a2.332 2.332 0 0 0-1.273.216c-1.18.666-2.311 1.418-3.652 2.255.246-1.112.405-2.087.687-3.024a1.15 1.15 0 0 0-.523-1.52C1.737 17.902.02 13.601 1.307 9.165c1.189-4.1 4.11-6.587 8.077-7.884A13.54 13.54 0 0 1 24.18 5.617a10.135 10.135 0 0 1 2.109 5.359zM10.668 9.594a1.564 1.564 0 0 0-2.095-1.472 1.52 1.52 0 0 0-.895 1.964 1.502 1.502 0 0 0 1.391.966 1.545 1.545 0 0 0 1.598-1.46v.002zm8.15-1.566a1.567 1.567 0 0 0-1.528 1.543 1.528 1.528 0 0 0 1.571 1.492 1.52 1.52 0 0 0 1.375-2.117 1.518 1.518 0 0 0-1.415-.919l-.003.001z">
              </path>
              <path fill="currentColor"
                d="M33.914 32.137c-1.075-.478-2.062-1.196-3.11-1.306-1.049-.11-2.145.494-3.24.605a10.821 10.821 0 0 1-8.781-2.864c-4.682-4.33-4.013-10.97 1.403-14.518 4.811-3.154 11.874-2.102 15.268 2.273a8.671 8.671 0 0 1-1.002 12.095c-1.046.929-1.422 1.693-.751 2.917.102.257.174.525.213.798zM21.68 20.292a1.264 1.264 0 1 0 .01-2.528 1.264 1.264 0 0 0-.01 2.528zm7.887-2.526a1.266 1.266 0 0 0-1.256 1.21 1.247 1.247 0 1 0 1.256-1.21z">
              </path>
            </svg>
          </a></li>
      </ul>
    </div>

    <div class="privacy-policy">
      <div class="copyright">
        
        <p>
          &copy; PyTorch. Copyright © The Linux Foundation®. All rights reserved. The Linux Foundation has registered
          trademarks and uses trademarks. For more information, including terms of use, privacy policy, and trademark
          usage, please see our <a href="https://www.linuxfoundation.org/legal/policies">Policies</a> page. <a
            href="https://www.linuxfoundation.org/trademark-usage">Trademark Usage</a>. <a
            href="http://www.linuxfoundation.org/privacy">Privacy Policy</a>.
        </p>
        
      </div>
    </div>


  </div>
</footer>

<div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../_static/img/pytorch-x.svg">
  </div>
</div>
  
  <footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
  
    <div class="footer-items__start">
      
        <div class="footer-item">

  <p class="copyright">
    
      © Copyright PyTorch Contributors.
      <br/>
    
  </p>
</div>
      
        <div class="footer-item">

  <p class="sphinx-version">
    Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 5.3.0.
    <br/>
  </p>
</div>
      
    </div>
  
  
  
    <div class="footer-items__end">
      
        <div class="footer-item">
<p class="theme-version">
  Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.15.4.
</p></div>
      
    </div>
  
</div>

  </footer>
  <script type="application/ld+json">
    {
       "@context": "https://schema.org",
       "@type": "Article",
       "name": "Program Listing for File custom_function.h",
       "headline": "Program Listing for File custom_function.h",
       "description": "PyTorch Documentation. Explore PyTorch, an open-source machine learning library that accelerates the path from research prototyping to production deployment. Discover tutorials, API references, and guides to help you build and deploy deep learning models efficiently.",
       "url": "/api/program_listing_file_torch_csrc_autograd_custom_function.h.html",
       "articleBody": "Program Listing for File custom_function.h# \u21b0 Return to documentation for file (torch/csrc/autograd/custom_function.h) #pragma once #include \u003cATen/core/ivalue.h\u003e #include \u003cc10/core/SymInt.h\u003e #include \u003cc10/util/flat_hash_map.h\u003e #include \u003cc10/util/irange.h\u003e #include \u003ctorch/csrc/autograd/function.h\u003e #include \u003ctorch/csrc/autograd/variable.h\u003e #include \u003ctorch/csrc/autograd/variable_info.h\u003e #include \u003ctorch/csrc/dynamo/compiled_autograd.h\u003e #include \u003cvector\u003e namespace torch::autograd { using optional_variable_list = std::vector\u003cstd::optional\u003cVariable\u003e\u003e; using _jvp_fn_t = std::function\u003cvariable_list(variable_list, variable_list)\u003e; using _view_as_self_fn_t = std::function\u003cat::Tensor(at::Tensor)\u003e; TORCH_API std::vector\u003cstd::optional\u003cVariable\u003e\u003e _wrap_outputs( const variable_list\u0026 input_vars, const std::unordered_set\u003cat::TensorImpl*\u003e\u0026 non_differentiable, const std::unordered_set\u003cat::TensorImpl*\u003e\u0026 dirty_inputs, const at::ArrayRef\u003cstd::optional\u003cVariable\u003e\u003e raw_outputs, const std::shared_ptr\u003cNode\u003e\u0026 cdata, const _jvp_fn_t\u0026 jvp_user_function, const std::unordered_set\u003cat::TensorImpl*\u003e\u0026 to_save_if_setup_context, const _view_as_self_fn_t\u0026 view_as_self_fn); TORCH_API void check_variable_result( const at::TensorBase\u0026 original, const at::TensorBase\u0026 result, const std::string\u0026 hook_name); // Get the return type of the forward function of the custom Function class X template \u003ctypename X, typename... Args\u003e using forward_t = decltype(X::forward(nullptr, std::declval\u003cArgs\u003e()...)); template \u003cclass T\u003e struct TORCH_API Function { // We need to use a different template parameter than T here because T will // inherit from Function, and when Function\u003cT\u003e is instantiated, T::forward // is not declared yet. // The enable_if check is to ensure that the user doesn\u0027t explicitly provide // the parameter X. template \u003ctypename X = T, typename... Args\u003e static auto apply(Args\u0026\u0026... args) -\u003e std::enable_if_t\u003cstd::is_same_v\u003cX, T\u003e, forward_t\u003cX, Args...\u003e\u003e; // This flag is for an experimental feature: compiled autograd. Not all // built-in APIs are supported at the moment e.g. mark_dirty and // mark_non_differentiable. Before setting this flag to enable tracing for // your custom function \u003cT\u003e, you need to ensure that the backward function is // traceable i.e. any variables accessed in the backward other than the input // arguments must be handled in a similar manner to built-ins in // CppNode::compiled_args and CppNode::apply_with_saved. static constexpr bool is_traceable = false; }; struct TORCH_API AutogradContext { AutogradContext() = default; AutogradContext(const AutogradContext\u0026 other) = delete; AutogradContext\u0026 operator=(const AutogradContext\u0026 other) = delete; AutogradContext(AutogradContext\u0026\u0026 other) = delete; AutogradContext\u0026 operator=(AutogradContext\u0026\u0026 other) = delete; ~AutogradContext() = default; AutogradContext(PackedArgs\u0026 packed_args); ska::flat_hash_map\u003cstd::string, at::IValue\u003e saved_data; void save_for_backward(variable_list to_save); void mark_dirty(const variable_list\u0026 inputs); void mark_non_differentiable(const variable_list\u0026 outputs); // Sets whether undefined output grad tensors should be expanded to tensors // full of zeros before calling backward function. Default value is true. void set_materialize_grads(bool value); variable_list get_saved_variables() const; const std::unordered_set\u003cat::TensorImpl*\u003e\u0026 get_and_bump_dirty() const; const std::unordered_set\u003cat::TensorImpl*\u003e\u0026 get_non_differentiable() const; bool needs_input_grad(size_t output_edge_index) const; bool needs_input_grad(std::initializer_list\u003cIndexRange\u003e idxs) const; private: std::unordered_set\u003cat::TensorImpl*\u003e non_differentiable_; std::unordered_set\u003cat::TensorImpl*\u003e dirty_inputs_; std::vector\u003ctorch::autograd::SavedVariable\u003e saved_variables_; variable_list to_save_; bool materialize_grads_{true}; // The CppNode in the autograd graph that owns this AutogradContext. We need a // weak_ptr to avoid a refcycle. Since grad_fn_ owns this AutogradContext, it // will always be alive when we want to use it. std::weak_ptr\u003cNode\u003e grad_fn_; bool has_freed_buffers_{false}; // Compiled autograd overrides saved_variables() and needs_input_grad(). // We store the values we want to return here. std::optional\u003cvariable_list\u003e saved_variables_override_; std::optional\u003cstd::vector\u003cbool\u003e\u003e needs_input_grad_override_; void save_variables(); template \u003cclass T\u003e friend struct CppNode; template \u003cclass T\u003e friend variable_list CppNode_apply_functional( variable_list\u0026\u0026 inputs, AutogradContext\u0026 ctx_, const std::vector\u003cbool\u003e\u0026 is_variable_input_, const std::vector\u003cVariableInfo\u003e\u0026 output_info_, const std::string\u0026 name); }; template \u003ctypename T\u003e inline variable_list CppNode_apply_functional( // NOLINTNEXTLINE(cppcoreguidelines-rvalue-reference-param-not-moved) variable_list\u0026\u0026 inputs, AutogradContext\u0026 ctx_, const std::vector\u003cbool\u003e\u0026 is_variable_input_, const std::vector\u003cVariableInfo\u003e\u0026 output_info_, const std::string\u0026 name) { at::OptionalDeviceGuard _device_guard; auto num_inputs = inputs.size(); variable_list backward_inputs; backward_inputs.reserve(num_inputs); for (const auto i : c10::irange(num_inputs)) { if (inputs[i].defined() || !ctx_.materialize_grads_) { backward_inputs.emplace_back(std::move(inputs[i])); } else { backward_inputs.emplace_back(output_info_[i].zeros(_device_guard)); } } auto outputs = T::backward(\u0026ctx_, backward_inputs); const auto num_forward_inputs = static_cast\u003cint64_t\u003e(is_variable_input_.size()); auto num_outputs = static_cast\u003cint64_t\u003e(outputs.size()); // Returning too many results is ok, but only as long as they\u0027re all // undefined. Truncate the result vector in that case. if (num_outputs \u003e num_forward_inputs) { bool all_undef = true; for (const auto i : c10::irange(num_forward_inputs, num_outputs)) { all_undef \u0026= (!outputs[i].defined()); } if (all_undef) { outputs.resize(num_forward_inputs); num_outputs = num_forward_inputs; } } TORCH_CHECK( num_outputs == num_forward_inputs, \"function \", name, \" returned an incorrect number of gradients (expected \", num_forward_inputs, \", got \", num_outputs, \")\"); variable_list results; results.reserve(num_outputs); for (const auto i : c10::irange(num_outputs)) { if (!is_variable_input_[i]) { TORCH_CHECK( outputs[i].defined() == false, \"function \", name, \" returned a gradient different that is defined at position \", i + 1, \", std the corresponding forward input was not a Variable\"); continue; } results.emplace_back(outputs[i]); } return results; } template \u003ctypename T\u003e inline variable_list CppNode_apply_functional_ivalue( const variable_list\u0026 inputs, const std::vector\u003cc10::IValue\u003e\u0026 args) { auto packed_args = PackedArgs(args); auto ctx = AutogradContext(packed_args); auto output_info = packed_args.unpack\u003cstd::vector\u003cVariableInfo\u003e\u003e(); auto is_variable_input = packed_args.unpack\u003cstd::vector\u003cbool\u003e\u003e(); auto name = packed_args.unpack\u003cstd::string\u003e(); return CppNode_apply_functional\u003cT\u003e( variable_list(inputs), ctx, is_variable_input, output_info, name); } // CppNode\u003cT\u003e is the Node in the autograd graph that represents the user defined // backward function for Function\u003cT\u003e. Calls to CppNode::apply are forward to // T::backward(). template \u003cclass T\u003e struct CppNode : public Node { variable_list apply(variable_list\u0026\u0026 inputs) override; AutogradContext ctx_; std::vector\u003cbool\u003e is_variable_input_; std::vector\u003cVariableInfo\u003e input_info_; std::vector\u003cVariableInfo\u003e output_info_; void release_variables() override; void set_ctx_grad_fn(const std::shared_ptr\u003cNode\u003e\u0026 node); void save_variables_to_ctx(); void compiled_args(CompiledNodeArgs\u0026 args) const override { // although neither of the 2 methods below have uniqueness guarantees // it is unlikely for them to collide at the same time args.collect(static_cast\u003cuint64_t\u003e(typeid(T).hash_code())); args.collect(std::string(typeid(T).name())); args.collect(ctx_.saved_data); TORCH_INTERNAL_ASSERT(ctx_.non_differentiable_.empty()); TORCH_INTERNAL_ASSERT(ctx_.dirty_inputs_.empty()); args.collect( ctx_.saved_variables_, true); // always unpacked as output in eager TORCH_INTERNAL_ASSERT(ctx_.to_save_.empty()); args.collect(ctx_.materialize_grads_); args.collect(ctx_.has_freed_buffers_); args.collect(is_variable_input_); args.collect(input_info_); args.collect(output_info_); } variable_list apply_with_saved( const variable_list\u0026 inputs, SwapSavedVariables\u0026 saved) override { saved.before(ctx_.saved_data); TORCH_INTERNAL_ASSERT(ctx_.non_differentiable_.empty()); TORCH_INTERNAL_ASSERT(ctx_.dirty_inputs_.empty()); saved.before(ctx_.saved_variables_); TORCH_INTERNAL_ASSERT(ctx_.to_save_.empty()); saved.before(ctx_.materialize_grads_); saved.before(ctx_.has_freed_buffers_); saved.before(input_info_); saved.before(output_info_); PackedArgs packed_args; packed_args.pack_saved_data(ctx_.saved_data); variable_list saved_variables = ctx_.get_saved_variables(); packed_args.pack(saved_variables); packed_args.pack(ctx_.materialize_grads_); packed_args.pack(ctx_.has_freed_buffers_); std::vector\u003cbool\u003e needs_input_grad; { auto ptr = ctx_.grad_fn_.lock(); TORCH_INTERNAL_ASSERT(ptr); for (const auto i : c10::irange(ptr-\u003enext_edges().size())) { needs_input_grad.push_back(ptr-\u003etask_should_compute_output(i)); } } packed_args.pack(needs_input_grad); packed_args.pack(output_info_); packed_args.pack(is_variable_input_); packed_args.pack(name()); auto args = std::move(packed_args).vec(); auto output_metadata = torch::dynamo::autograd:: IValuePacker\u003cstd::vector\u003cstd::optional\u003cInputMetadata\u003e\u003e\u003e::pack( torch::dynamo::autograd::get_input_metadata(next_edges())); const auto\u0026 pyinterface = torch::dynamo::autograd::getPyCompilerInterface(); // Each time apply_with_saved is called, we bind a new function to Python. // This is because the schema might be different on compiled autograd cache // misses. An alternative is to pass the schema to Python so that it can be // an input to a function, but the schema can\u0027t be put into an FX graph // right now. std::vector\u003cat::TypePtr\u003e schema; schema.reserve(args.size()); for (const auto\u0026 ivalue : args) { if (ivalue.isTensor()) { schema.emplace_back(at::TensorType::get()); } else { schema.emplace_back(ivalue.type()); } } static_assert( std::is_same_v\u003cstd::remove_cv_t\u003cdecltype(T::is_traceable)\u003e, bool\u003e); auto fn_name = pyinterface-\u003ebind_function( saved.get_py_compiler(), std::string(typeid(T).name()), CppNode_apply_functional_ivalue\u003cT\u003e, schema, /*is_custom_function*/ true, /*is_traceable*/ T::is_traceable); auto results = pyinterface-\u003ecall_function( saved.get_py_compiler(), \"apply_functional\", fn_name, inputs, args, output_metadata); saved.after(ctx_.saved_data); TORCH_INTERNAL_ASSERT(ctx_.non_differentiable_.empty()); TORCH_INTERNAL_ASSERT(ctx_.dirty_inputs_.empty()); saved.after(ctx_.saved_variables_); TORCH_INTERNAL_ASSERT(ctx_.to_save_.empty()); saved.after(ctx_.materialize_grads_); saved.after(ctx_.has_freed_buffers_); saved.after(input_info_); saved.after(output_info_); return results; } }; struct ExtractVariables : IterArgs\u003cExtractVariables\u003e { // NOLINTNEXTLINE(cppcoreguidelines-avoid-const-or-ref-data-members) std::vector\u003cbool\u003e\u0026 is_var_; // NOLINTNEXTLINE(cppcoreguidelines-avoid-const-or-ref-data-members) variable_list\u0026 list_; ExtractVariables(std::vector\u003cbool\u003e\u0026 is_var, variable_list\u0026 list) : is_var_(is_var), list_(list) {} void operator()(const std::optional\u003cat::Tensor\u003e\u0026 x) { if (x.has_value() \u0026\u0026 x.value().defined()) { is_var_.push_back(true); list_.emplace_back(x.value()); } else { is_var_.push_back(false); } } void operator()(const at::Tensor\u0026 x) { is_var_.push_back(true); list_.emplace_back(x); } void operator()(const at::TensorList\u0026 list) { for (const at::Tensor\u0026 x : list) { is_var_.push_back(true); list_.emplace_back(x); } } template \u003ctypename T\u003e void operator()(const T\u0026 x) { is_var_.push_back(false); } }; template \u003ctypename... Args\u003e inline void extract_vars( std::vector\u003cbool\u003e\u0026 is_var, variable_list\u0026 list, Args\u0026\u0026... args) { ExtractVariables(is_var, list).apply(std::forward\u003cArgs\u003e(args)...); } template \u003ctypename T\u003e std::enable_if_t\u003cstd::is_same_v\u003cT, variable_list\u003e, T\u003e to_output_type( std::vector\u003cstd::optional\u003cVariable\u003e\u003e\u0026 output_list) { variable_list result; std::transform( output_list.begin(), output_list.end(), std::back_inserter(result), [](const std::optional\u003cVariable\u003e\u0026 var) { return *var; }); return result; } template \u003ctypename T\u003e std::enable_if_t\u003cstd::is_same_v\u003cT, Variable\u003e, T\u003e to_output_type( std::vector\u003cstd::optional\u003cVariable\u003e\u003e\u0026 output_list) { return *output_list[0]; } inline std::vector\u003cstd::optional\u003cVariable\u003e\u003e to_optional(Variable\u0026 output) { return std::vector\u003cstd::optional\u003cVariable\u003e\u003e{output}; } inline std::vector\u003cstd::optional\u003cVariable\u003e\u003e to_optional(variable_list\u0026 output) { std::vector\u003cstd::optional\u003cVariable\u003e\u003e result; std::transform( output.begin(), output.end(), std::back_inserter(result), [](const Variable\u0026 var) { return var; }); return result; } template \u003cclass T\u003e template \u003ctypename X, typename... Args\u003e auto Function\u003cT\u003e::apply(Args\u0026\u0026... args) -\u003e std::enable_if_t\u003cstd::is_same_v\u003cX, T\u003e, forward_t\u003cX, Args...\u003e\u003e { const auto\u0026 functorch_tls = at::functorch::functorchTLSAccessor(); if (functorch_tls) { // Function support for functorch is handled in Python. // Here we are dealing with a (C++) Function, which is not supported. // Let\u0027s raise an error instead of being silently incorrect. functorch_tls-\u003echeckSupportsCppAutogradFunction(); } std::shared_ptr\u003cCppNode\u003cT\u003e\u003e node(new CppNode\u003cT\u003e(), deleteNode); variable_list input_vars; const size_t num_inputs = sizeof...(Args); input_vars.reserve(num_inputs); node-\u003eis_variable_input_.reserve(num_inputs); // TODO Add tracing here extract_vars(node-\u003eis_variable_input_, input_vars, args...); bool is_executable = GradMode::is_enabled() \u0026\u0026 any_variable_requires_grad(input_vars); auto next_edges = (is_executable ? collect_next_edges(input_vars) : edge_list()); node-\u003eset_ctx_grad_fn(node); node-\u003eset_next_edges(std::move(next_edges)); node-\u003eclear_input_metadata(); node-\u003einput_info_.reserve(input_vars.size()); for (auto\u0026 var : input_vars) { node-\u003einput_info_.emplace_back(var); } using forward_return_t = forward_t\u003cX, Args...\u003e; forward_return_t outputs; { AutoGradMode grad_mode(false); outputs = T::forward(\u0026node-\u003ectx_, std::forward\u003cArgs\u003e(args)...); } _jvp_fn_t jvp_fn = [](const variable_list\u0026 inputs, const variable_list\u0026 gI) -\u003e variable_list { TORCH_CHECK( false, \"jvp is not implemented for the c++ API of custom Function yet.\", \"Please open a feature request on GitHub if you need this.\"); }; auto view_as_self_fn = [](const at::Tensor\u0026 x) -\u003e at::Tensor { return x.view_as(x); }; auto wrapped_outputs = _wrap_outputs( input_vars, node-\u003ectx_.get_non_differentiable(), node-\u003ectx_.get_and_bump_dirty(), to_optional(outputs), is_executable ? node : nullptr, jvp_fn, {}, view_as_self_fn); node-\u003eoutput_info_.reserve(wrapped_outputs.size()); for (auto\u0026 output : wrapped_outputs) { if (is_executable \u0026\u0026 output.has_value()) { node-\u003eoutput_info_.emplace_back(output.value()); } else if (is_executable) { node-\u003eoutput_info_.emplace_back(); } } if (is_executable) { node-\u003esave_variables_to_ctx(); } // wrapped_outputs will be a variable_list so, convert it to the correct // return type. Only Variable and variable_list are accepted as return types. return to_output_type\u003cforward_return_t\u003e(wrapped_outputs); } // The logic here is the same as PyNode::apply, so changes to it should be done // in both the places template \u003cclass T\u003e // NOLINTNEXTLINE(cppcoreguidelines-rvalue-reference-param-not-moved) variable_list CppNode\u003cT\u003e::apply(variable_list\u0026\u0026 inputs) { // Acquire lock to here protect thread safety on custom C++ Autograd Node // This is needed for the custom Autograd Node since we don\u0027t know if the // user defined Node will write to the shared data during backward. // see Note [Thread Safety on Autograd Node] std::lock_guard\u003cstd::mutex\u003e lock(mutex_); return CppNode_apply_functional\u003cT\u003e( std::move(inputs), ctx_, is_variable_input_, output_info_, name()); } template \u003cclass T\u003e void CppNode\u003cT\u003e::release_variables() { // lock to ensure thread safety, see [Thread Safety on Autograd Node] std::lock_guard\u003cstd::mutex\u003e lock(mutex_); ctx_.saved_variables_.clear(); ctx_.has_freed_buffers_ = true; } template \u003cclass T\u003e void CppNode\u003cT\u003e::save_variables_to_ctx() { ctx_.save_variables(); } template \u003cclass T\u003e void CppNode\u003cT\u003e::set_ctx_grad_fn(const std::shared_ptr\u003cNode\u003e\u0026 node) { ctx_.grad_fn_ = node; } } // namespace torch::autograd",
       "author": {
         "@type": "Organization",
         "name": "PyTorch Contributors",
         "url": "https://pytorch.org"
       },
       "image": "../_static/img/pytorch_seo.png",
       "mainEntityOfPage": {
         "@type": "WebPage",
         "@id": "/api/program_listing_file_torch_csrc_autograd_custom_function.h.html"
       },
       "datePublished": "2023-01-01T00:00:00Z",
       "dateModified": "2023-01-01T00:00:00Z"
     }
 </script>
  <script>
    // Tutorials Call to action event tracking
    $("[data-behavior='call-to-action-event']").on('click', function () {
      fbq('trackCustom', "Download", {
        tutorialTitle: $('h1:first').text(),
        downloadLink: this.href,
        tutorialLink: window.location.href,
        downloadTitle: $(this).attr("data-response")
      });
      if (typeof gtag === 'function') {
        gtag('event', 'click', {
          'event_category': $(this).attr("data-response"),
          'event_label': $("h1").first().text(),
          'tutorial_link': window.location.href
        });
      }
    });
  </script>
  
  </body>
</html>