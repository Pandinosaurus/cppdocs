

<!DOCTYPE html>


<html lang="en" data-content_root="" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Program Listing for File variable.h &#8212; PyTorch main documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/theme.css" />
    <link rel="stylesheet" type="text/css" href="../_static/collapsible-lists/css/tree_view.css" />
    <link rel="stylesheet" type="text/css" href="../_static/cpp_theme.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/sphinx_highlight.js"></script>
    <script src="../_static/collapsible-lists/js/CollapsibleLists.compressed.js"></script>
    <script src="../_static/collapsible-lists/js/apply-collapsible-lists.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'api/program_listing_file_torch_csrc_autograd_variable.h';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="File variable_factories.h" href="file_torch_csrc_autograd_generated_variable_factories.h.html" />
    <link rel="prev" title="File variable.h" href="file_torch_csrc_autograd_variable.h.html" />

  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
<script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/2.3.1/list.min.js"></script>
<script>
  if (window.location.hostname === 'docs.pytorch.org' || window.location.hostname === 'docs-preview.pytorch.org') {
    const script = document.createElement('script');
    script.src = 'https://cmp.osano.com/16A0DbT9yDNIaQkvZ/31b1b91a-e0b6-47ea-bde2-7f2bd13dbe5c/osano.js?variant=one';
    document.head.appendChild(script);
  }
</script>
<script>
  // Cookie banner for non-LF projects
  document.addEventListener('DOMContentLoaded', function () {
    // Hide cookie banner on local environments and LF owned docs
    if (window.location.hostname === 'localhost' ||
      window.location.hostname === '0.0.0.0' ||
      window.location.hostname === '127.0.0.1' ||
      window.location.hostname === 'docs.pytorch.org' ||
      window.location.hostname === 'docs-preview.pytorch.org' ||
      window.location.hostname.startsWith('192.168.')) {
      const banner = document.querySelector('.cookie-banner-wrapper');
      if (banner) {
        banner.style.display = 'none';
      }
    }
  });
</script>
<!-- Conditional CSS for header and footer height adjustment -->

<link rel="stylesheet" type="text/css" href="../_static/css/theme.css" crossorigin="anonymous">
<script type="text/javascript" src="../_static/js/theme.js"></script>
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Montserrat:wght@300;400&display=swap" rel="stylesheet">
<meta property="og:image" content="../_static/img/pytorch_seo.png" />
<link rel="stylesheet" href="../_static/webfonts/all.min.css" crossorigin="anonymous">
<meta http-equiv="Content-Security-Policy"
  content="default-src * 'unsafe-inline' 'unsafe-eval' data: blob:; style-src * 'unsafe-inline'; script-src * 'unsafe-inline' 'unsafe-eval' blob:;">
<meta name="pytorch_project" content="">
<!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-T8XT4PS" height="0" width="0"
    style="display:none;visibility:hidden"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->
<!-- Google Tag Manager -->
<script>(function (w, d, s, l, i) {
    w[l] = w[l] || []; w[l].push({
      'gtm.start':
        new Date().getTime(), event: 'gtm.js'
    }); var f = d.getElementsByTagName(s)[0],
      j = d.createElement(s), dl = l != 'dataLayer' ? '&l=' + l : ''; j.async = true; j.src =
        'https://www.googletagmanager.com/gtm.js?id=' + i + dl; f.parentNode.insertBefore(j, f);
    j.onload = function () {
      window.dispatchEvent(new Event('gtm_loaded'));
      console.log('GTM loaded successfully');
    };
  })(window, document, 'script', 'dataLayer', 'GTM-T8XT4PS');
</script>
<!-- End Google Tag Manager -->
<!-- Facebook Pixel Code -->
<script>
  !function (f, b, e, v, n, t, s) {
    if (f.fbq) return; n = f.fbq = function () {
      n.callMethod ?
        n.callMethod.apply(n, arguments) : n.queue.push(arguments)
    };
    if (!f._fbq) f._fbq = n; n.push = n; n.loaded = !0; n.version = '2.0';
    n.queue = []; t = b.createElement(e); t.async = !0;
    t.src = v; s = b.getElementsByTagName(e)[0];
    s.parentNode.insertBefore(t, s)
  }(window, document, 'script',
    'https://connect.facebook.net/en_US/fbevents.js');
  fbq('init', '243028289693773');
  fbq('track', 'PageView');
</script>
<script>
  document.documentElement.setAttribute('data-version', 'main');
</script>
<noscript>
  <img height="1" width="1" src="https://www.facebook.com/tr?id=243028289693773&ev=PageView&noscript=1" />
</noscript>
<script>
  function gtag() {
    window.dataLayer.push(arguments);
  }
</script>
<!-- End Facebook Pixel Code -->
<!-- Script to Fix scrolling -->
<script>
  document.addEventListener('DOMContentLoaded', function () {
    // Fix anchor scrolling
    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
      anchor.addEventListener('click', function (e) {
        e.preventDefault();
        const targetId = this.getAttribute('href').substring(1);
        const targetElement = document.getElementById(targetId);

        if (targetElement) {
          const headerHeight =
            (document.querySelector('.header-holder') ? document.querySelector('.header-holder').offsetHeight : 0) +
            (document.querySelector('.bd-header') ? document.querySelector('.bd-header').offsetHeight : 0) + 20;

          const targetPosition = targetElement.getBoundingClientRect().top + window.pageYOffset - headerHeight;
          window.scrollTo({
            top: targetPosition,
            behavior: 'smooth'
          });

          // Update URL hash without scrolling
          history.pushState(null, null, '#' + targetId);
        }
      });
    });
  });
</script>

<script async src="https://cse.google.com/cse.js?cx=e65585f8c3ea1440e"></script>

<!--
   Search engines should not index the main version of documentation.
   Stable documentation are built without release == 'main'.
   -->
<meta name="robots" content="noindex">


  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>

  </head>

<body data-feedback-url="https://github.com/pytorch/pytorch" class="pytorch-body">
  
    <div class="container-fluid header-holder tutorials-header" id="header-holder">
   <div class="header-container-wrapper">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>

          <li class="main-menu-item">
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                <span>Learn</span>
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/get-started">
                  <span class=dropdown-title>Get Started</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials">
                  <span class="dropdown-title">Tutorials</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/basics/intro.html">
                  <span class="dropdown-title">Learn the Basics</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/recipes/recipes_index.html">
                  <span class="dropdown-title">PyTorch Recipes</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/introyt.html">
                  <span class="dropdown-title">Intro to PyTorch - YouTube Series</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/webinars/">
                  <span class="dropdown-title">Webinars</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
              <span>Community</span>
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://landscape.pytorch.org/" target="_blank">
                  <span class="dropdown-title">Landscape</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/join-ecosystem">
                  <span class="dropdown-title">Join the Ecosystem</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/community-hub/">
                  <span class="dropdown-title">Community Hub</span>
                </a>
                <a class="nav-dropdown-item" href="https://discuss.pytorch.org/">
                  <span class="dropdown-title">Forums</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/resources">
                  <span class=dropdown-title>Developer Resources</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/contributor-awards/">
                  <span class="dropdown-title">Contributor Awards</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/community-events/">
                  <span class="dropdown-title">Community Events</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/programs/ambassadors/">
                  <span class="dropdown-title">PyTorch Ambassadors</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
              <span>Projects</span>
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/projects/pytorch/">
                  <span class="dropdown-title">PyTorch</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/projects/vllm/">
                  <span class="dropdown-title">vLLM</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/projects/deepspeed/">
                  <span class="dropdown-title">DeepSpeed</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/projects/host-your-project/">
                  <span class="dropdown-title">Host Your Project</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
              <span> Docs</span>
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/docs/stable/index.html">
                  <span class="dropdown-title">PyTorch</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/pytorch-domains">
                  <span class="dropdown-title">Domains</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
              <span>Blogs & News</span>
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/blog/">
                  <span class="dropdown-title">Blog</span>
                </a>
                 <a class="nav-dropdown-item" href="https://pytorch.org/announcements">
                  <span class="dropdown-title">Announcements</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/case-studies/">
                  <span class="dropdown-title">Case Studies</span>
                <a class="nav-dropdown-item" href="https://pytorch.org/events">
                  <span class="dropdown-title">Events</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/newsletter">
                  <span class="dropdown-title">Newsletter</span>
                </a>
            </div>
          </li>

          <li class="main-menu-item">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
              <span>About</span>
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/foundation">
                  <span class="dropdown-title">PyTorch Foundation</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/members">
                  <span class="dropdown-title">Members</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/governing-board">
                  <span class="dropdown-title">Governing Board</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tac">
                  <span class="dropdown-title">Technical Advisory Council</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/credits">
                  <span class="dropdown-title">Cloud Credit Program</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/staff">
                  <span class="dropdown-title">Staff</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/contact">
                  <span class="dropdown-title">Contact</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <div class="no-dropdown main-menu-button">
              <a href="https://pytorch.org/join" data-cta="join">
                JOIN
              </a>
            </div>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu">
        <i class="fa-solid fa-ellipsis"></i>
      </a>
    </div>
  </div>
 </div>

 <!-- Begin Mobile Menu -->

<div class="mobile-main-menu">
  <div class="container-fluid">
    <div class="header-container-wrapper">
      <div class="mobile-main-menu-header-container">
        <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu">
        </a>
      </div>
    </div>
  </div>

  <div class="mobile-main-menu-links-container">
    <div class="main-menu">
      <ul>
         <li class="resources-mobile-menu-title">
           <a>Learn</a>
         </li>
         <ul class="resources-mobile-menu-items">
           <li>
             <a href="https://pytorch.org/get-started">Get Started</a>
           </li>
           <li>
             <a href="https://pytorch.org/tutorials">Tutorials</a>
           </li>
           <li>
             <a href="https://pytorch.org/tutorials/beginner/basics/intro.html">Learn the Basics</a>
           </li>
           <li>
             <a href="https://pytorch.org/tutorials/recipes/recipes_index.html">PyTorch Recipes</a>
           </li>
           <li>
             <a href="https://pytorch.org/tutorials/beginner/introyt.html">Introduction to PyTorch - YouTube Series</a>
           </li>
           <li>
            <a href="https://pytorch.org/webinars/">Webinars</a>
          </li>
        </ul>
         <li class="resources-mobile-menu-title">
           <a>Community</a>
         </li>
         <ul class="resources-mobile-menu-items">
          <li>
            <a href="https://landscape.pytorch.org/">Landscape</a>
          </li>
          <li>
             <a href="https://pytorch.org/join-ecosystem">Join the Ecosystem</a>
           </li>
           <li>
             <a href="https://pytorch.org/community-hub/">Community Hub</a>
           </li>
           <li>
             <a href="https://discuss.pytorch.org/">Forums</a>
           </li>
           <li>
             <a href="https://pytorch.org/resources">Developer Resources</a>
           </li>
           <li>
             <a href="https://pytorch.org/contributor-awards/">Contributor Awards</a>
           </li>
           <li>
            <a href="https://pytorch.org/community-events/">Community Events</a>
          </li>
          <li>
            <a href="https://pytorch.org/programs/ambassadors/">PyTorch Ambassadors</a>
          </li>
       </ul>

         <li class="resources-mobile-menu-title">
           <a>Projects</a>
         </li>

         <ul class="resources-mobile-menu-items">
           <li>
             <a href="https://pytorch.org/projects/pytorch/">PyTorch</a>
           </li>

           <li>
             <a href="https://pytorch.org/projects/vllm/">vLLM</a>
           </li>
           <li>
            <a href="https://pytorch.org/projects/deepspeed/">DeepSpeed</a>
          </li>
          <li>
             <a href="https://pytorch.org/projects/host-your-project/">Host Your Project</a>
           </li>
         </ul>

         <li class="resources-mobile-menu-title">
           <a>Docs</a>
         </li>

         <ul class="resources-mobile-menu-items">
          <li>
            <a href="https://pytorch.org/docs/stable/index.html">PyTorch</a>
          </li>

          <li>
            <a href="https://pytorch.org/pytorch-domains">Domains</a>
          </li>
        </ul>

        <li class="resources-mobile-menu-title">
          <a>Blog & News</a>
        </li>

         <ul class="resources-mobile-menu-items">
          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>
          <li>
            <a href="https://pytorch.org/announcements">Announcements</a>
          </li>

          <li>
            <a href="https://pytorch.org/case-studies/">Case Studies</a>
          </li>
          <li>
            <a href="https://pytorch.org/events">Events</a>
          </li>
          <li>
             <a href="https://pytorch.org/newsletter">Newsletter</a>
           </li>
        </ul>

        <li class="resources-mobile-menu-title">
          <a>About</a>
        </li>

        <ul class="resources-mobile-menu-items">
          <li>
            <a href="https://pytorch.org/foundation">PyTorch Foundation</a>
          </li>
          <li>
            <a href="https://pytorch.org/members">Members</a>
          </li>
          <li>
            <a href="https://pytorch.org/governing-board">Governing Board</a>
          </li>
          <li>
            <a href="https://pytorch.org/tac">Technical Advisory Council</a>
         </li>
         <li>
             <a href="https://pytorch.org/credits">Cloud Credit Program</a>
          </li>
          <li>
             <a href="https://pytorch.org/staff">Staff</a>
          </li>
          <li>
             <a href="https://pytorch.org/contact">Contact</a>
          </li>
        </ul>
      </ul>
    </div>
  </div>
</div>

<!-- End Mobile Menu -->
  
  
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
<div class="bd-header__inner bd-page-width">
  <button class="pst-navbar-icon sidebar-toggle primary-toggle" aria-label="Site navigation">
    <span class="fa-solid fa-bars"></span>
  </button>
  
  
  <div class=" navbar-header-items__start">
    
      <div class="navbar-item">
  <a href="../index.html" class="version">main</a>
</div>
    
  </div>
  
  <div class=" navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../installing.html">
    Installing C++ Distributions of PyTorch
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../frontend.html">
    The C++ Frontend
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="library_root.html">
    Library API
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../notes/faq.html">
    FAQ
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../notes/inference_mode.html">
    Inference Mode
  </a>
</li>

            <li class="nav-item dropdown">
                <button class="btn dropdown-toggle nav-item" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-controls="pst-nav-more-links">
                    More
                </button>
                <ul id="pst-nav-more-links" class="dropdown-menu">
                    
<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="../notes/maybe_owned.html">
    MaybeOwned<Tensor>
  </a>
</li>


<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="../notes/tensor_basics.html">
    Tensor Basics
  </a>
</li>


<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="../notes/tensor_creation.html">
    Tensor Creation API
  </a>
</li>


<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="../notes/tensor_cuda_stream.html">
    Tensor CUDA Stream API
  </a>
</li>


<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="../notes/tensor_indexing.html">
    Tensor Indexing API
  </a>
</li>


<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="../notes/versioning.html">
    Library Versioning
  </a>
</li>

                </ul>
            </li>
            
  </ul>
</nav></div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
      
        <div class="navbar-item">
  
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
</div>
      
        <div class="navbar-item">

<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
      
        <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://x.com/PyTorch" title="X" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-x-twitter fa-lg" aria-hidden="true"></i>
            <span class="sr-only">X</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/pytorch/pytorch" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://discuss.pytorch.org/" title="PyTorch Forum" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-discourse fa-lg" aria-hidden="true"></i>
            <span class="sr-only">PyTorch Forum</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://pypi.org/project/torch/" title="PyPi" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-python fa-lg" aria-hidden="true"></i>
            <span class="sr-only">PyPi</span></a>
        </li>
</ul></div>
      
    </div>
    
  </div>
  
  

  
    <button class="pst-navbar-icon sidebar-toggle secondary-toggle" aria-label="On this page">
      <span class="fa-solid fa-outdent"></span>
    </button>
  
</div>

    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
        
          
          
            <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../installing.html">
    Installing C++ Distributions of PyTorch
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../frontend.html">
    The C++ Frontend
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="library_root.html">
    Library API
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../notes/faq.html">
    FAQ
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../notes/inference_mode.html">
    Inference Mode
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../notes/maybe_owned.html">
    MaybeOwned<Tensor>
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../notes/tensor_basics.html">
    Tensor Basics
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../notes/tensor_creation.html">
    Tensor Creation API
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../notes/tensor_cuda_stream.html">
    Tensor CUDA Stream API
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../notes/tensor_indexing.html">
    Tensor Indexing API
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../notes/versioning.html">
    Library Versioning
  </a>
</li>

  </ul>
</nav></div>
          
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item">
  
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
</div>
        
          <div class="navbar-item">

<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
        
          <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://x.com/PyTorch" title="X" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-x-twitter fa-lg" aria-hidden="true"></i>
            <span class="sr-only">X</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/pytorch/pytorch" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://discuss.pytorch.org/" title="PyTorch Forum" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-discourse fa-lg" aria-hidden="true"></i>
            <span class="sr-only">PyTorch Forum</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://pypi.org/project/torch/" title="PyPi" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-python fa-lg" aria-hidden="true"></i>
            <span class="sr-only">PyPi</span></a>
        </li>
</ul></div>
        
      </div>
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
<nav class="bd-docs-nav bd-links"
     aria-label="Section Navigation">
  <p class="bd-links__title" role="heading" aria-level="1">Section Navigation</p>
  <div class="bd-toc-item navbar-nav"><ul class="current nav bd-sidenav">
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Program Listing for File variable.h</a></li>
</ul>
</div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">



<nav aria-label="Breadcrumb" class="d-print-none">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    
    <li class="breadcrumb-item"><a href="library_root.html" class="nav-link">Library API</a></li>
    
    
    <li class="breadcrumb-item"><a href="file_torch_csrc_autograd_variable.h.html" class="nav-link">File variable.h</a></li>
    
    <li class="breadcrumb-item active" aria-current="page">Program...</li>
  </ul>
</nav>
</div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">
<div class="rating">
    Rate this Page
    <div class="stars">
        
        <span class="star" data-behavior="tutorial-rating" data-count="1" data-value="1">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="2" data-value="2">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="3" data-value="3">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="4" data-value="4">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="5" data-value="5">★</span>
        
    </div>
</div>
</div>
      
    </div>
  
</div>
</div>
              
              
  
<div id="searchbox"></div>
  <article class="bd-article" id="pytorch-article">
    <!-- Hidden breadcrumb schema for SEO only -->
    <div style="display:none;" itemscope itemtype="https://schema.org/BreadcrumbList">
      
      <div itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
        <link itemprop="item" href="library_root.html">
        <meta itemprop="name" content="Library API">
        <meta itemprop="position" content="1">
      </div>
      
      <div itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
        <link itemprop="item" href="file_torch_csrc_autograd_variable.h.html">
        <meta itemprop="name" content="File variable.h">
        <meta itemprop="position" content="2">
      </div>
      
      <div itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
        <meta itemprop="name" content="Program Listing for File variable.h">
        <meta itemprop="position" content="3">
      </div>
    </div>

    

    
              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <div class="section" id="program-listing-for-file-variable-h">
<span id="program-listing-file-torch-csrc-autograd-variable-h"></span><h1>Program Listing for File variable.h<a class="headerlink" href="#program-listing-for-file-variable-h" title="Permalink to this heading">#</a></h1>
<p>↰ <a class="reference internal" href="file_torch_csrc_autograd_variable.h.html#file-torch-csrc-autograd-variable-h"><span class="std std-ref">Return to documentation for file</span></a> (<code class="docutils literal notranslate"><span class="pre">torch/csrc/autograd/variable.h</span></code>)</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="cp">#pragma once</span>

<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;torch/csrc/utils/python_stub.h&gt;</span>

<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;torch/csrc/Export.h&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;torch/csrc/autograd/cpp_hook.h&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;torch/csrc/autograd/edge.h&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;torch/csrc/autograd/forward_grad.h&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;torch/csrc/autograd/function_hook.h&gt;</span>

<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;ATen/NamedTensorUtils.h&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;ATen/core/Tensor.h&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;ATen/core/VariableHooksInterface.h&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;c10/util/Exception.h&gt;</span>

<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;cstdint&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;memory&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;mutex&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;string&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;utility&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;vector&gt;</span>

<span class="k">namespace</span><span class="w"> </span><span class="nn">torch</span><span class="o">::</span><span class="nn">autograd</span><span class="w"> </span><span class="p">{</span>

<span class="k">using</span><span class="w"> </span><span class="n">Variable</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="p">;</span>

<span class="p">}</span><span class="w"> </span><span class="c1">// namespace torch::autograd</span>

<span class="c1">// The following are all internal APIs and should not be shown in libtorch docs.</span>
<span class="c1">// Therefore, we wrap the following code with `#ifndef DOXYGEN_SHOULD_SKIP_THIS</span>
<span class="c1">// ... #endif`</span>

<span class="cp">#ifndef DOXYGEN_SHOULD_SKIP_THIS</span>

<span class="k">namespace</span><span class="w"> </span><span class="nn">torch</span><span class="o">::</span><span class="nn">autograd</span><span class="w"> </span><span class="p">{</span>

<span class="kr">inline</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="nf">isDifferentiableType</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">ScalarType</span><span class="w"> </span><span class="n">t</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="n">isFloatingType</span><span class="p">(</span><span class="n">t</span><span class="p">)</span><span class="w"> </span><span class="o">||</span><span class="w"> </span><span class="n">isComplexType</span><span class="p">(</span><span class="n">t</span><span class="p">);</span>
<span class="p">}</span>

<span class="k">struct</span><span class="w"> </span><span class="nc">Node</span><span class="p">;</span>


<span class="k">struct</span><span class="w"> </span><span class="nc">AutogradMeta</span><span class="p">;</span>
<span class="k">struct</span><span class="w"> </span><span class="nc">DifferentiableViewMeta</span><span class="p">;</span>

<span class="c1">// Private-ish functions for manipulating variables; we don&#39;t want to put them</span>
<span class="c1">// on Tensor proper</span>
<span class="k">namespace</span><span class="w"> </span><span class="nn">impl</span><span class="w"> </span><span class="p">{</span>

<span class="c1">// WARNING: This may return a nullptr.  If you require AutogradMeta to return</span>
<span class="c1">// a materialized structure, use materialize_autograd_meta instead.</span>
<span class="n">TORCH_API</span><span class="w"> </span><span class="n">AutogradMeta</span><span class="o">*</span><span class="w"> </span><span class="n">get_autograd_meta</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">TensorBase</span><span class="o">&amp;</span><span class="p">);</span>

<span class="c1">// WARNING: This will return a nullptr if the Tensor is not a view.</span>
<span class="n">TORCH_API</span><span class="w"> </span><span class="n">DifferentiableViewMeta</span><span class="o">*</span><span class="w"> </span><span class="n">get_view_autograd_meta</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">TensorBase</span><span class="o">&amp;</span><span class="p">);</span>

<span class="c1">// Returns the current autograd meta, materializing it if it was previously</span>
<span class="c1">// none.  This counts as a *mutating* operation, so do not call it on</span>
<span class="c1">// &quot;read-only&quot; operators; in particular, this is NOT thread safe</span>
<span class="n">TORCH_API</span><span class="w"> </span><span class="n">AutogradMeta</span><span class="o">*</span><span class="w"> </span><span class="n">materialize_autograd_meta</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">TensorBase</span><span class="o">&amp;</span><span class="p">);</span>

<span class="n">TORCH_API</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="n">set_grad_accumulator</span><span class="p">(</span>
<span class="w">    </span><span class="k">const</span><span class="w"> </span><span class="n">Variable</span><span class="o">&amp;</span><span class="p">,</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">weak_ptr</span><span class="o">&lt;</span><span class="n">Node</span><span class="o">&gt;</span><span class="w"> </span><span class="n">grad_accumulator</span><span class="p">);</span>

<span class="n">TORCH_API</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">Node</span><span class="o">&gt;</span><span class="w"> </span><span class="n">try_get_grad_accumulator</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">Variable</span><span class="o">&amp;</span><span class="p">);</span>

<span class="n">TORCH_API</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">Node</span><span class="o">&gt;</span><span class="w"> </span><span class="n">grad_accumulator</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">Variable</span><span class="o">&amp;</span><span class="p">);</span>

<span class="n">TORCH_API</span><span class="w"> </span><span class="n">Edge</span><span class="w"> </span><span class="n">gradient_edge</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">Variable</span><span class="o">&amp;</span><span class="p">);</span>

<span class="n">TORCH_API</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="n">set_gradient_edge</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">Variable</span><span class="o">&amp;</span><span class="p">,</span><span class="w"> </span><span class="n">Edge</span><span class="w"> </span><span class="n">edge</span><span class="p">);</span>

<span class="c1">// Autograd Graph Interaction</span>
<span class="c1">//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>

<span class="n">TORCH_API</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="n">rebase_history</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">Variable</span><span class="o">&amp;</span><span class="p">,</span><span class="w"> </span><span class="n">Edge</span><span class="w"> </span><span class="n">gradient_edge</span><span class="p">);</span>

<span class="n">TORCH_API</span><span class="w"> </span><span class="n">Node</span><span class="o">*</span><span class="w"> </span><span class="n">grad_fn_unsafe</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">Variable</span><span class="o">&amp;</span><span class="p">);</span>

<span class="n">TORCH_API</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="n">bump_version</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">Variable</span><span class="o">&amp;</span><span class="p">);</span>
<span class="n">TORCH_API</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="n">set_version_counter</span><span class="p">(</span>
<span class="w">    </span><span class="k">const</span><span class="w"> </span><span class="n">Variable</span><span class="o">&amp;</span><span class="p">,</span>
<span class="w">    </span><span class="k">const</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">VariableVersion</span><span class="o">&amp;</span><span class="w"> </span><span class="n">version_counter</span><span class="p">);</span>

<span class="n">TORCH_API</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">VariableVersion</span><span class="o">&amp;</span><span class="w"> </span><span class="n">version_counter</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">Variable</span><span class="o">&amp;</span><span class="p">);</span>

<span class="n">TORCH_API</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="n">set_name</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">Variable</span><span class="o">&amp;</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="o">&amp;</span><span class="w"> </span><span class="n">name</span><span class="p">);</span>

<span class="n">TORCH_API</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="n">add_hook</span><span class="p">(</span>
<span class="w">    </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">TensorBase</span><span class="o">&amp;</span><span class="p">,</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">unique_ptr</span><span class="o">&lt;</span><span class="n">FunctionPreHook</span><span class="o">&gt;</span><span class="w"> </span><span class="n">hook</span><span class="p">);</span>
<span class="n">TORCH_API</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">unique_ptr</span><span class="o">&lt;</span><span class="n">FunctionPreHook</span><span class="o">&gt;&gt;&amp;</span><span class="w"> </span><span class="n">hooks</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">Variable</span><span class="o">&amp;</span><span class="p">);</span>
<span class="n">TORCH_API</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="n">clear_hooks</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">TensorBase</span><span class="o">&amp;</span><span class="p">);</span>

<span class="n">TORCH_API</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="n">set_post_acc_grad_hooks</span><span class="p">(</span>
<span class="w">    </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">TensorBase</span><span class="o">&amp;</span><span class="p">,</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">unique_ptr</span><span class="o">&lt;</span><span class="n">PostAccumulateGradHook</span><span class="o">&gt;</span><span class="w"> </span><span class="n">dict</span><span class="p">);</span>
<span class="n">TORCH_API</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">unique_ptr</span><span class="o">&lt;</span><span class="n">PostAccumulateGradHook</span><span class="o">&gt;&amp;</span><span class="w"> </span><span class="n">post_acc_grad_hooks</span><span class="p">(</span>
<span class="w">    </span><span class="k">const</span><span class="w"> </span><span class="n">Variable</span><span class="o">&amp;</span><span class="p">);</span>

<span class="n">TORCH_API</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="n">create_cpp_hook</span><span class="p">(</span>
<span class="w">    </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">TensorBase</span><span class="o">&amp;</span><span class="p">,</span>
<span class="w">    </span><span class="kt">bool</span><span class="w"> </span><span class="n">is_retains_grad_hooks</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nb">false</span><span class="p">);</span>
<span class="p">}</span><span class="w"> </span><span class="c1">// namespace impl</span>

<span class="c1">//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
<span class="c1">//                            AutogradMeta</span>
<span class="c1">//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>


<span class="k">struct</span><span class="w"> </span><span class="nc">TORCH_API</span><span class="w"> </span><span class="n">AutogradMeta</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="k">public</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">AutogradMetaInterface</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="w"> </span><span class="n">name_</span><span class="p">;</span>

<span class="w">  </span><span class="n">Variable</span><span class="w"> </span><span class="n">grad_</span><span class="p">;</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">Node</span><span class="o">&gt;</span><span class="w"> </span><span class="n">grad_fn_</span><span class="p">;</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">weak_ptr</span><span class="o">&lt;</span><span class="n">Node</span><span class="o">&gt;</span><span class="w"> </span><span class="n">grad_accumulator_</span><span class="p">;</span>

<span class="w">  </span><span class="c1">// This field is used to store all the forward AD gradients</span>
<span class="w">  </span><span class="c1">// associated with this AutogradMeta (and the Tensor it corresponds to)</span>
<span class="w">  </span><span class="c1">// There is a semantic 1:1 correspondence between AutogradMeta and</span>
<span class="w">  </span><span class="c1">// ForwardGrad but:</span>
<span class="w">  </span><span class="c1">//   - This field is lazily populated.</span>
<span class="w">  </span><span class="c1">//   - This field is a shared_ptr but it must never be</span>
<span class="w">  </span><span class="c1">//     shared by multiple Tensors. See Note [ Using ForwardGrad ]</span>
<span class="w">  </span><span class="c1">// Any transition from not_initialized to initialized</span>
<span class="w">  </span><span class="c1">// must be protected by mutex_</span>
<span class="w">  </span><span class="k">mutable</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">ForwardGrad</span><span class="o">&gt;</span><span class="w"> </span><span class="n">fw_grad_</span><span class="p">;</span>

<span class="w">  </span><span class="c1">// The hooks_ field is actually reused by both python and cpp logic</span>
<span class="w">  </span><span class="c1">// For both cases, we have a data structure, cpp_hooks_list_ (cpp)</span>
<span class="w">  </span><span class="c1">// or dict (python) which is the canonical copy.</span>
<span class="w">  </span><span class="c1">// Then, for both cases, we always register a single hook to</span>
<span class="w">  </span><span class="c1">// hooks_ which wraps all the hooks in the list/dict.</span>
<span class="w">  </span><span class="c1">// And, again in both cases, if the grad_fn exists on that tensor</span>
<span class="w">  </span><span class="c1">// we will additionally register a single hook to the grad_fn.</span>
<span class="w">  </span><span class="c1">//</span>
<span class="w">  </span><span class="c1">// Note that the cpp and python use cases aren&#39;t actually aware of</span>
<span class="w">  </span><span class="c1">// each other, so using both is not defined behavior.</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">unique_ptr</span><span class="o">&lt;</span><span class="n">FunctionPreHook</span><span class="o">&gt;&gt;</span><span class="w"> </span><span class="n">hooks_</span><span class="p">;</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">hooks_list</span><span class="o">&gt;</span><span class="w"> </span><span class="n">cpp_hooks_list_</span><span class="p">;</span>

<span class="w">  </span><span class="c1">// The post_acc_grad_hooks_ field stores only Python hooks</span>
<span class="w">  </span><span class="c1">// (PyFunctionTensorPostAccGradHooks) that are called after the</span>
<span class="w">  </span><span class="c1">// .grad field has been accumulated into. This is less complicated</span>
<span class="w">  </span><span class="c1">// than the hooks_ field, which encapsulates a lot more.</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">unique_ptr</span><span class="o">&lt;</span><span class="n">PostAccumulateGradHook</span><span class="o">&gt;</span><span class="w"> </span><span class="n">post_acc_grad_hooks_</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">nullptr</span><span class="p">;</span>

<span class="w">  </span><span class="c1">// Only meaningful on leaf variables (must be false otherwise)</span>
<span class="w">  </span><span class="kt">bool</span><span class="w"> </span><span class="n">requires_grad_</span><span class="p">{</span><span class="nb">false</span><span class="p">};</span>

<span class="w">  </span><span class="c1">// Only meaningful on non-leaf variables (must be false otherwise)</span>
<span class="w">  </span><span class="kt">bool</span><span class="w"> </span><span class="n">retains_grad_</span><span class="p">{</span><span class="nb">false</span><span class="p">};</span>

<span class="w">  </span><span class="kt">bool</span><span class="w"> </span><span class="n">is_view_</span><span class="p">{</span><span class="nb">false</span><span class="p">};</span>

<span class="w">  </span><span class="c1">// The &quot;output number&quot; of this variable; e.g., if this variable</span>
<span class="w">  </span><span class="c1">// was the second output of a function, then output_nr == 1.</span>
<span class="w">  </span><span class="c1">// We use this to make sure we can setup the backwards trace</span>
<span class="w">  </span><span class="c1">// correctly when this variable is passed to another function.</span>
<span class="w">  </span><span class="kt">uint32_t</span><span class="w"> </span><span class="n">output_nr_</span><span class="p">;</span>

<span class="w">  </span><span class="c1">// Mutex to ensure that concurrent read operations that modify internal</span>
<span class="w">  </span><span class="c1">// state are still thread-safe. Used by grad_fn(), grad_accumulator(),</span>
<span class="w">  </span><span class="c1">// fw_grad() and set_fw_grad()</span>
<span class="w">  </span><span class="c1">// This is mutable because we need to be able to acquire this from const</span>
<span class="w">  </span><span class="c1">// version of this class for the functions above</span>
<span class="w">  </span><span class="k">mutable</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">mutex</span><span class="w"> </span><span class="n">mutex_</span><span class="p">;</span>

<span class="w">  </span><span class="kt">void</span><span class="w"> </span><span class="nf">set_requires_grad</span><span class="p">(</span><span class="kt">bool</span><span class="w"> </span><span class="n">requires_grad</span><span class="p">,</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">TensorImpl</span><span class="o">*</span><span class="w"> </span><span class="n">self_impl</span><span class="p">)</span><span class="w"> </span><span class="k">final</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">TORCH_CHECK</span><span class="p">(</span>
<span class="w">        </span><span class="o">!</span><span class="n">requires_grad</span><span class="w"> </span><span class="o">||</span>
<span class="w">            </span><span class="n">isDifferentiableType</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">typeMetaToScalarType</span><span class="p">(</span><span class="n">self_impl</span><span class="o">-&gt;</span><span class="n">dtype</span><span class="p">())),</span>
<span class="w">        </span><span class="s">&quot;Only Tensors of floating point and complex dtype can require gradients&quot;</span><span class="p">);</span>
<span class="w">    </span><span class="n">requires_grad_</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">requires_grad</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="kt">bool</span><span class="w"> </span><span class="nf">requires_grad</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="k">override</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">requires_grad_</span><span class="w"> </span><span class="o">||</span><span class="w"> </span><span class="n">grad_fn_</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="n">Variable</span><span class="o">&amp;</span><span class="w"> </span><span class="nf">mutable_grad</span><span class="p">()</span><span class="w"> </span><span class="k">override</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">grad_</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="k">const</span><span class="w"> </span><span class="n">Variable</span><span class="o">&amp;</span><span class="w"> </span><span class="nf">grad</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="k">override</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">grad_</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="k">const</span><span class="w"> </span><span class="n">Variable</span><span class="o">&amp;</span><span class="w"> </span><span class="nf">fw_grad</span><span class="p">(</span><span class="kt">uint64_t</span><span class="w"> </span><span class="n">level</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">TensorBase</span><span class="o">&amp;</span><span class="w"> </span><span class="n">self</span><span class="p">)</span>
<span class="w">      </span><span class="k">const</span><span class="w"> </span><span class="k">override</span><span class="p">;</span>

<span class="w">  </span><span class="kt">void</span><span class="w"> </span><span class="nf">set_fw_grad</span><span class="p">(</span>
<span class="w">      </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">TensorBase</span><span class="o">&amp;</span><span class="w"> </span><span class="n">new_grad</span><span class="p">,</span>
<span class="w">      </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">TensorBase</span><span class="o">&amp;</span><span class="w"> </span><span class="n">self</span><span class="p">,</span>
<span class="w">      </span><span class="kt">uint64_t</span><span class="w"> </span><span class="n">level</span><span class="p">,</span>
<span class="w">      </span><span class="kt">bool</span><span class="w"> </span><span class="n">is_inplace_op</span><span class="p">)</span><span class="w"> </span><span class="k">override</span><span class="p">;</span>

<span class="w">  </span><span class="n">AutogradMeta</span><span class="p">(</span>
<span class="w">      </span><span class="n">at</span><span class="o">::</span><span class="n">TensorImpl</span><span class="o">*</span><span class="w"> </span><span class="n">self_impl</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">nullptr</span><span class="p">,</span>
<span class="w">      </span><span class="kt">bool</span><span class="w"> </span><span class="n">requires_grad</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nb">false</span><span class="p">,</span>
<span class="w">      </span><span class="n">Edge</span><span class="w"> </span><span class="n">gradient_edge</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Edge</span><span class="p">())</span>
<span class="w">      </span><span class="o">:</span><span class="w"> </span><span class="n">grad_fn_</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">move</span><span class="p">(</span><span class="n">gradient_edge</span><span class="p">.</span><span class="n">function</span><span class="p">)),</span>

<span class="w">        </span><span class="n">output_nr_</span><span class="p">(</span><span class="n">gradient_edge</span><span class="p">.</span><span class="n">input_nr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="c1">// set_requires_grad also checks error conditions.</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">requires_grad</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="n">TORCH_INTERNAL_ASSERT</span><span class="p">(</span><span class="n">self_impl</span><span class="p">);</span>
<span class="w">      </span><span class="n">set_requires_grad</span><span class="p">(</span><span class="n">requires_grad</span><span class="p">,</span><span class="w"> </span><span class="n">self_impl</span><span class="p">);</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span><span class="n">TORCH_CHECK</span><span class="p">(</span>
<span class="w">        </span><span class="o">!</span><span class="n">grad_fn_</span><span class="w"> </span><span class="o">||</span><span class="w"> </span><span class="o">!</span><span class="n">requires_grad_</span><span class="p">,</span>
<span class="w">        </span><span class="s">&quot;requires_grad should be false if grad_fn is set&quot;</span><span class="p">);</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="o">~</span><span class="n">AutogradMeta</span><span class="p">()</span><span class="w"> </span><span class="k">override</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="c1">// If AutogradMeta is being destroyed, it means that there is no other</span>
<span class="w">    </span><span class="c1">// reference to its corresponding Tensor. It implies that no other thread</span>
<span class="w">    </span><span class="c1">// can be using this object and so there is no need to lock mutex_ here to</span>
<span class="w">    </span><span class="c1">// guard the check if fw_grad_ is populated.</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">fw_grad_</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="c1">// See note [ Using ForwardGrad ]</span>
<span class="w">      </span><span class="n">fw_grad_</span><span class="o">-&gt;</span><span class="n">clear</span><span class="p">();</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">  </span><span class="p">}</span>
<span class="p">};</span>

<span class="k">struct</span><span class="w"> </span><span class="nc">TORCH_API</span><span class="w"> </span><span class="n">ViewFunc</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="k">virtual</span><span class="w"> </span><span class="o">~</span><span class="n">ViewFunc</span><span class="p">()</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">default</span><span class="p">;</span>
<span class="w">  </span><span class="k">virtual</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">c10</span><span class="o">::</span><span class="n">SymInt</span><span class="o">&gt;</span><span class="w"> </span><span class="n">get_symints</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="p">{};</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="k">virtual</span><span class="w"> </span><span class="kt">size_t</span><span class="w"> </span><span class="n">num_symints</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="k">virtual</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">get_tensors</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="p">{};</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="k">virtual</span><span class="w"> </span><span class="kt">size_t</span><span class="w"> </span><span class="n">num_tensors</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="k">virtual</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="k">operator</span><span class="p">()(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&amp;</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>
<span class="w">  </span><span class="k">virtual</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">unique_ptr</span><span class="o">&lt;</span><span class="n">ViewFunc</span><span class="o">&gt;</span><span class="w"> </span><span class="n">clone_and_set</span><span class="p">(</span>
<span class="w">      </span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">c10</span><span class="o">::</span><span class="n">SymInt</span><span class="o">&gt;&gt;</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">,</span>
<span class="w">      </span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;&gt;</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>

<span class="w"> </span><span class="k">protected</span><span class="o">:</span>
<span class="w">  </span><span class="k">virtual</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="n">set_symints</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">c10</span><span class="o">::</span><span class="n">SymInt</span><span class="o">&gt;</span><span class="p">)</span><span class="w"> </span><span class="p">{}</span>
<span class="w">  </span><span class="k">virtual</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="n">set_tensors</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="p">)</span><span class="w"> </span><span class="p">{}</span>
<span class="p">};</span>

<span class="k">struct</span><span class="w"> </span><span class="nc">ChainedViewFunc</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="k">public</span><span class="w"> </span><span class="n">ViewFunc</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">ChainedViewFunc</span><span class="p">(</span>
<span class="w">      </span><span class="n">std</span><span class="o">::</span><span class="n">unique_ptr</span><span class="o">&lt;</span><span class="n">ViewFunc</span><span class="o">&gt;</span><span class="w"> </span><span class="n">first</span><span class="p">,</span>
<span class="w">      </span><span class="n">std</span><span class="o">::</span><span class="n">unique_ptr</span><span class="o">&lt;</span><span class="n">ViewFunc</span><span class="o">&gt;</span><span class="w"> </span><span class="n">second</span><span class="p">)</span>
<span class="w">      </span><span class="o">:</span><span class="w"> </span><span class="n">first</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">move</span><span class="p">(</span><span class="n">first</span><span class="p">)),</span><span class="w"> </span><span class="n">second</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">move</span><span class="p">(</span><span class="n">second</span><span class="p">))</span><span class="w"> </span><span class="p">{}</span>
<span class="w">  </span><span class="o">~</span><span class="n">ChainedViewFunc</span><span class="p">()</span><span class="w"> </span><span class="k">override</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">default</span><span class="p">;</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">c10</span><span class="o">::</span><span class="n">SymInt</span><span class="o">&gt;</span><span class="w"> </span><span class="n">get_symints</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="k">override</span><span class="p">;</span>
<span class="w">  </span><span class="kt">size_t</span><span class="w"> </span><span class="nf">num_symints</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="k">override</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">first</span><span class="o">-&gt;</span><span class="n">num_symints</span><span class="p">()</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">second</span><span class="o">-&gt;</span><span class="n">num_symints</span><span class="p">();</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">get_tensors</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="k">override</span><span class="p">;</span>
<span class="w">  </span><span class="kt">size_t</span><span class="w"> </span><span class="nf">num_tensors</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="k">override</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">first</span><span class="o">-&gt;</span><span class="n">num_tensors</span><span class="p">()</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">second</span><span class="o">-&gt;</span><span class="n">num_tensors</span><span class="p">();</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">operator</span><span class="p">()(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&amp;</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="k">override</span><span class="p">;</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">unique_ptr</span><span class="o">&lt;</span><span class="n">ViewFunc</span><span class="o">&gt;</span><span class="w"> </span><span class="n">clone_and_set</span><span class="p">(</span>
<span class="w">      </span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">c10</span><span class="o">::</span><span class="n">SymInt</span><span class="o">&gt;&gt;</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">,</span>
<span class="w">      </span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;&gt;</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="k">override</span><span class="p">;</span>

<span class="w"> </span><span class="k">private</span><span class="o">:</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">unique_ptr</span><span class="o">&lt;</span><span class="n">ViewFunc</span><span class="o">&gt;</span><span class="w"> </span><span class="n">first</span><span class="p">;</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">unique_ptr</span><span class="o">&lt;</span><span class="n">ViewFunc</span><span class="o">&gt;</span><span class="w"> </span><span class="n">second</span><span class="p">;</span>
<span class="p">};</span>

<span class="k">struct</span><span class="w"> </span><span class="nc">ErroringViewFunc</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="k">public</span><span class="w"> </span><span class="n">ViewFunc</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">ErroringViewFunc</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="w"> </span><span class="n">error_msg</span><span class="p">)</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="n">error_msg</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">move</span><span class="p">(</span><span class="n">error_msg</span><span class="p">))</span><span class="w"> </span><span class="p">{}</span>
<span class="w">  </span><span class="o">~</span><span class="n">ErroringViewFunc</span><span class="p">()</span><span class="w"> </span><span class="k">override</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">default</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">operator</span><span class="p">()(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&amp;</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="k">override</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">TORCH_CHECK</span><span class="p">(</span><span class="nb">false</span><span class="p">,</span><span class="w"> </span><span class="n">error_msg</span><span class="p">);</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">unique_ptr</span><span class="o">&lt;</span><span class="n">ViewFunc</span><span class="o">&gt;</span><span class="w"> </span><span class="n">clone_and_set</span><span class="p">(</span>
<span class="w">      </span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">c10</span><span class="o">::</span><span class="n">SymInt</span><span class="o">&gt;&gt;</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">,</span>
<span class="w">      </span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;&gt;</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="k">override</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">make_unique</span><span class="o">&lt;</span><span class="n">ErroringViewFunc</span><span class="o">&gt;</span><span class="p">(</span><span class="n">error_msg</span><span class="p">);</span>
<span class="w">  </span><span class="p">}</span>

<span class="w"> </span><span class="k">private</span><span class="o">:</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="w"> </span><span class="n">error_msg</span><span class="p">;</span>
<span class="p">};</span>

<span class="k">struct</span><span class="w"> </span><span class="nc">TORCH_API</span><span class="w"> </span><span class="n">ViewInfo</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">Variable</span><span class="w"> </span><span class="n">base_</span><span class="p">;</span>

<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">unique_ptr</span><span class="o">&lt;</span><span class="n">ViewFunc</span><span class="o">&gt;</span><span class="w"> </span><span class="n">view_fn_</span><span class="p">;</span>

<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">function</span><span class="o">&lt;</span><span class="n">Variable</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">Variable</span><span class="o">&amp;</span><span class="p">)</span><span class="o">&gt;</span><span class="w"> </span><span class="n">rev_view_fn_</span><span class="p">;</span>

<span class="w">  </span><span class="kt">bool</span><span class="w"> </span><span class="nf">has_view_fn</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="c1">// assume either BOTH or NEITHER of view_fn_ and rev_view_fn_ exist</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">view_fn_</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="k">nullptr</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="k">const</span><span class="w"> </span><span class="n">ViewFunc</span><span class="o">&amp;</span><span class="w"> </span><span class="nf">view_fn</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">TORCH_CHECK</span><span class="p">(</span>
<span class="w">        </span><span class="n">has_view_fn</span><span class="p">(),</span><span class="w"> </span><span class="s">&quot;Can only access the view function if it exists.&quot;</span><span class="p">);</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="o">*</span><span class="n">view_fn_</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">function</span><span class="o">&lt;</span><span class="n">Variable</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">Variable</span><span class="o">&amp;</span><span class="p">)</span><span class="o">&gt;</span><span class="w"> </span><span class="n">rev_view_fn</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">TORCH_CHECK</span><span class="p">(</span>
<span class="w">        </span><span class="n">has_view_fn</span><span class="p">(),</span>
<span class="w">        </span><span class="s">&quot;Can only access the reverse view function if it exists.&quot;</span><span class="p">);</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">rev_view_fn_</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="n">ViewInfo</span><span class="w"> </span><span class="n">chain</span><span class="p">(</span>
<span class="w">      </span><span class="k">const</span><span class="w"> </span><span class="n">Variable</span><span class="o">&amp;</span><span class="w"> </span><span class="n">base</span><span class="p">,</span>
<span class="w">      </span><span class="k">const</span><span class="w"> </span><span class="n">Variable</span><span class="o">&amp;</span><span class="w"> </span><span class="n">tensor</span><span class="p">,</span>
<span class="w">      </span><span class="n">std</span><span class="o">::</span><span class="n">unique_ptr</span><span class="o">&lt;</span><span class="n">ViewFunc</span><span class="o">&gt;</span><span class="w"> </span><span class="n">view_func</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">nullptr</span><span class="p">,</span>
<span class="w">      </span><span class="n">std</span><span class="o">::</span><span class="n">function</span><span class="o">&lt;</span><span class="n">Variable</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">Variable</span><span class="o">&amp;</span><span class="p">)</span><span class="o">&gt;</span><span class="w"> </span><span class="n">rev_view_func</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>

<span class="w">  </span><span class="n">ViewInfo</span><span class="p">(</span>
<span class="w">      </span><span class="n">Variable</span><span class="w"> </span><span class="n">base</span><span class="p">,</span>
<span class="w">      </span><span class="n">std</span><span class="o">::</span><span class="n">unique_ptr</span><span class="o">&lt;</span><span class="n">ViewFunc</span><span class="o">&gt;</span><span class="w"> </span><span class="n">view_fn</span><span class="p">,</span>
<span class="w">      </span><span class="n">std</span><span class="o">::</span><span class="n">function</span><span class="o">&lt;</span><span class="n">Variable</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">Variable</span><span class="o">&amp;</span><span class="p">)</span><span class="o">&gt;</span><span class="w"> </span><span class="n">rev_view_fn</span><span class="p">)</span>
<span class="w">      </span><span class="o">:</span><span class="w"> </span><span class="n">base_</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">move</span><span class="p">(</span><span class="n">base</span><span class="p">)),</span>
<span class="w">        </span><span class="n">view_fn_</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">move</span><span class="p">(</span><span class="n">view_fn</span><span class="p">)),</span>
<span class="w">        </span><span class="n">rev_view_fn_</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">move</span><span class="p">(</span><span class="n">rev_view_fn</span><span class="p">))</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">TORCH_CHECK</span><span class="p">(</span><span class="n">base_</span><span class="p">.</span><span class="n">defined</span><span class="p">(),</span><span class="w"> </span><span class="s">&quot;base is undefined&quot;</span><span class="p">);</span>
<span class="w">  </span><span class="p">}</span>
<span class="p">};</span>

<span class="c1">//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
<span class="c1">//                     DifferentiableViewMeta</span>
<span class="c1">//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>



<span class="k">enum</span><span class="w"> </span><span class="k">class</span><span class="w"> </span><span class="nc">CreationMeta</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="kt">uint8_t</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">DEFAULT</span><span class="p">,</span>
<span class="w">  </span><span class="n">IN_CUSTOM_FUNCTION</span><span class="p">,</span>
<span class="w">  </span><span class="n">MULTI_OUTPUT_NODE</span><span class="p">,</span>
<span class="w">  </span><span class="n">NO_GRAD_MODE</span><span class="p">,</span>
<span class="w">  </span><span class="n">INFERENCE_MODE</span>
<span class="p">};</span>

<span class="kr">inline</span><span class="w"> </span><span class="n">CreationMeta</span><span class="w"> </span><span class="nf">propagate_creation_meta</span><span class="p">(</span>
<span class="w">    </span><span class="n">CreationMeta</span><span class="w"> </span><span class="n">prev_view_creation_meta</span><span class="p">,</span>
<span class="w">    </span><span class="n">CreationMeta</span><span class="w"> </span><span class="n">new_view_creation_meta</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="p">(</span><span class="n">new_view_creation_meta</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="n">CreationMeta</span><span class="o">::</span><span class="n">DEFAULT</span><span class="p">)</span>
<span class="w">      </span><span class="o">?</span><span class="w"> </span><span class="n">prev_view_creation_meta</span>
<span class="w">      </span><span class="o">:</span><span class="w"> </span><span class="p">(</span><span class="n">prev_view_creation_meta</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="n">CreationMeta</span><span class="o">::</span><span class="n">INFERENCE_MODE</span>
<span class="w">             </span><span class="o">?</span><span class="w"> </span><span class="n">prev_view_creation_meta</span>
<span class="w">             </span><span class="o">:</span><span class="w"> </span><span class="n">new_view_creation_meta</span><span class="p">);</span>
<span class="p">}</span>

<span class="n">TORCH_API</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="n">handle_view_on_rebase</span><span class="p">(</span>
<span class="w">    </span><span class="n">DifferentiableViewMeta</span><span class="o">*</span><span class="w"> </span><span class="n">diff_view_meta</span><span class="p">,</span>
<span class="w">    </span><span class="kt">bool</span><span class="w"> </span><span class="n">indirect</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nb">false</span><span class="p">);</span>

<span class="k">struct</span><span class="w"> </span><span class="nc">TORCH_API</span><span class="w"> </span><span class="n">DifferentiableViewMeta</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="k">public</span><span class="w"> </span><span class="n">AutogradMeta</span><span class="w"> </span><span class="p">{</span>
<span class="w"> </span><span class="k">private</span><span class="o">:</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">ViewInfo</span><span class="o">&gt;</span><span class="w"> </span><span class="n">backward_info_</span><span class="p">;</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">ViewInfo</span><span class="o">&gt;</span><span class="w"> </span><span class="n">forward_info_</span><span class="p">;</span>

<span class="w">  </span><span class="c1">// Optimization to reduce the number of ViewInfo we create.</span>
<span class="w">  </span><span class="c1">// In the (very common) case where backward_info_ == forward_info_, we only</span>
<span class="w">  </span><span class="c1">// populate backward_info_ (that should be used as both the forward and</span>
<span class="w">  </span><span class="c1">// backward view information) and set shared_view_info_ = true. Invariants:</span>
<span class="w">  </span><span class="c1">//   - If shared_view_info_ is false, there is no special constraints on</span>
<span class="w">  </span><span class="c1">//     backward_info_ and forward_info_</span>
<span class="w">  </span><span class="c1">//   - If shared_view_info_ is true, we must have:</span>
<span class="w">  </span><span class="c1">//      - backward_info_.has_value() == true</span>
<span class="w">  </span><span class="c1">//      - forward_info_.has_value() == false</span>
<span class="w">  </span><span class="kt">bool</span><span class="w"> </span><span class="n">shared_view_info_</span><span class="p">;</span>


<span class="w">  </span><span class="kt">uint32_t</span><span class="w"> </span><span class="n">attr_version_</span><span class="p">;</span>
<span class="w">  </span><span class="n">CreationMeta</span><span class="w"> </span><span class="n">creation_meta_</span><span class="p">;</span>

<span class="w"> </span><span class="k">public</span><span class="o">:</span>
<span class="w">  </span><span class="kt">bool</span><span class="w"> </span><span class="n">requires_grad</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="k">override</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">requires_grad_</span><span class="w"> </span><span class="o">||</span><span class="w"> </span><span class="n">grad_fn_</span><span class="w"> </span><span class="o">||</span>
<span class="w">        </span><span class="p">(</span><span class="n">has_bw_view</span><span class="p">()</span><span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="n">get_backward_view</span><span class="p">().</span><span class="n">base_</span><span class="p">.</span><span class="n">requires_grad</span><span class="p">());</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="kt">bool</span><span class="w"> </span><span class="n">shared_view_info</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">shared_view_info_</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="kt">bool</span><span class="w"> </span><span class="n">has_bw_view</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">backward_info_</span><span class="p">.</span><span class="n">has_value</span><span class="p">();</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="k">const</span><span class="w"> </span><span class="n">ViewInfo</span><span class="o">&amp;</span><span class="w"> </span><span class="n">get_backward_view</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">TORCH_CHECK</span><span class="p">(</span>
<span class="w">        </span><span class="n">has_bw_view</span><span class="p">(),</span><span class="w"> </span><span class="s">&quot;backward view info can only exist for backward views.&quot;</span><span class="p">);</span>
<span class="w">    </span><span class="c1">// NOLINTNEXTLINE(bugprone-unchecked-optional-access)</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">backward_info_</span><span class="p">.</span><span class="n">value</span><span class="p">();</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="kt">uint32_t</span><span class="w"> </span><span class="n">get_attr_version</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">TORCH_CHECK</span><span class="p">(</span>
<span class="w">        </span><span class="n">has_bw_view</span><span class="p">(),</span><span class="w"> </span><span class="s">&quot;attr_version can only exist for backward views.&quot;</span><span class="p">);</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">attr_version_</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="kt">void</span><span class="w"> </span><span class="n">set_attr_version</span><span class="p">(</span><span class="kt">uint32_t</span><span class="w"> </span><span class="n">new_attr_version</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">TORCH_CHECK</span><span class="p">(</span>
<span class="w">        </span><span class="n">has_bw_view</span><span class="p">(),</span><span class="w"> </span><span class="s">&quot;attr_version can only exist for backward views.&quot;</span><span class="p">);</span>
<span class="w">    </span><span class="n">attr_version_</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">new_attr_version</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="n">CreationMeta</span><span class="w"> </span><span class="n">get_creation_meta</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">TORCH_CHECK</span><span class="p">(</span>
<span class="w">        </span><span class="n">has_bw_view</span><span class="p">(),</span><span class="w"> </span><span class="s">&quot;creation_meta can only exist for backward views.&quot;</span><span class="p">);</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">creation_meta_</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="kt">void</span><span class="w"> </span><span class="n">set_creation_meta</span><span class="p">(</span><span class="n">CreationMeta</span><span class="w"> </span><span class="n">new_creation_meta</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">TORCH_CHECK</span><span class="p">(</span>
<span class="w">        </span><span class="n">has_bw_view</span><span class="p">(),</span><span class="w"> </span><span class="s">&quot;creation_meta can only exist for backward views.&quot;</span><span class="p">);</span>
<span class="w">    </span><span class="n">creation_meta_</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">new_creation_meta</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="kt">bool</span><span class="w"> </span><span class="n">has_fw_view</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">shared_view_info_</span><span class="w"> </span><span class="o">||</span><span class="w"> </span><span class="n">forward_info_</span><span class="p">.</span><span class="n">has_value</span><span class="p">();</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="k">const</span><span class="w"> </span><span class="n">ViewInfo</span><span class="o">&amp;</span><span class="w"> </span><span class="n">get_forward_view</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">TORCH_CHECK</span><span class="p">(</span>
<span class="w">        </span><span class="n">has_fw_view</span><span class="p">(),</span><span class="w"> </span><span class="s">&quot;forward view info can only exist for forward views.&quot;</span><span class="p">);</span>
<span class="w">    </span><span class="n">TORCH_CHECK</span><span class="p">(</span>
<span class="w">        </span><span class="o">!</span><span class="n">shared_view_info_</span><span class="w"> </span><span class="o">||</span><span class="w"> </span><span class="n">has_bw_view</span><span class="p">(),</span>
<span class="w">        </span><span class="s">&quot;forward view info can only exist for forward views.&quot;</span><span class="p">);</span>
<span class="w">    </span><span class="c1">// NOLINTNEXTLINE(bugprone-unchecked-optional-access)</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">shared_view_info_</span><span class="w"> </span><span class="o">?</span><span class="w"> </span><span class="n">backward_info_</span><span class="p">.</span><span class="n">value</span><span class="p">()</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="n">forward_info_</span><span class="p">.</span><span class="n">value</span><span class="p">();</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="n">DifferentiableViewMeta</span><span class="p">(</span>
<span class="w">      </span><span class="n">at</span><span class="o">::</span><span class="n">TensorImpl</span><span class="o">*</span><span class="w"> </span><span class="n">self_impl</span><span class="p">,</span>
<span class="w">      </span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">ViewInfo</span><span class="o">&gt;</span><span class="w"> </span><span class="n">backward_info</span><span class="p">,</span>
<span class="w">      </span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">ViewInfo</span><span class="o">&gt;</span><span class="w"> </span><span class="n">forward_info</span><span class="p">,</span>
<span class="w">      </span><span class="kt">bool</span><span class="w"> </span><span class="n">shared_view_info</span><span class="p">,</span>
<span class="w">      </span><span class="n">CreationMeta</span><span class="w"> </span><span class="n">creation_meta</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">CreationMeta</span><span class="o">::</span><span class="n">DEFAULT</span><span class="p">);</span>
<span class="p">};</span>

<span class="c1">//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
<span class="c1">//                        Variable Implementation</span>
<span class="c1">//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>

<span class="c1">// Factory Functions</span>
<span class="c1">//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>



<span class="c1">// See NOTE [ Autograd View Variables ] for details.</span>
<span class="c1">// Differentiable view. Track history with DifferentiableViewMeta.</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">Variable</span><span class="w"> </span><span class="nf">make_variable_differentiable_view</span><span class="p">(</span>
<span class="w">    </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&amp;</span><span class="w"> </span><span class="n">data</span><span class="p">,</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">ViewInfo</span><span class="o">&gt;</span><span class="w"> </span><span class="n">backward_info</span><span class="p">,</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">ViewInfo</span><span class="o">&gt;</span><span class="w"> </span><span class="n">forward_info</span><span class="p">,</span>
<span class="w">    </span><span class="kt">bool</span><span class="w"> </span><span class="n">shared_view_info</span><span class="p">,</span>
<span class="w">    </span><span class="n">CreationMeta</span><span class="w"> </span><span class="n">creation_meta</span><span class="p">,</span>
<span class="w">    </span><span class="kt">bool</span><span class="w"> </span><span class="n">allow_tensor_metadata_change</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nb">true</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">data</span><span class="p">.</span><span class="n">defined</span><span class="p">())</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">TORCH_CHECK</span><span class="p">(</span>
<span class="w">        </span><span class="n">data</span><span class="p">.</span><span class="n">getIntrusivePtr</span><span class="p">()</span><span class="o">-&gt;</span><span class="n">autograd_meta</span><span class="p">()</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">,</span>
<span class="w">        </span><span class="s">&quot;Attempted to make a tensor into a differentiable view, but the &quot;</span>
<span class="w">        </span><span class="s">&quot;tensor already had autograd metadata associated with it.  If you are &quot;</span>
<span class="w">        </span><span class="s">&quot;using a __torch_dispatch__ mode, the most common cause for this &quot;</span>
<span class="w">        </span><span class="s">&quot;problem is that you used torch.overrides.enable_reentrant_dispatch() &quot;</span>
<span class="w">        </span><span class="s">&quot;improperly; tensors created within the extent of reentrant dispatch &quot;</span>
<span class="w">        </span><span class="s">&quot;MUST NOT be directly returned from __torch_dispatch__; instead, they &quot;</span>
<span class="w">        </span><span class="s">&quot;must be wrapped into fresh tensors that serve as the output.  If you &quot;</span>
<span class="w">        </span><span class="s">&quot;are not using wrappers, you probably don&#39;t need reentrant dispatch.  &quot;</span>
<span class="w">        </span><span class="s">&quot;If this doesn&#39;t seem applicable, please file a bug to PyTorch.&quot;</span><span class="p">);</span>
<span class="w">    </span><span class="n">at</span><span class="o">::</span><span class="n">TensorImpl</span><span class="o">*</span><span class="w"> </span><span class="n">data_impl</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">data</span><span class="p">.</span><span class="n">unsafeGetTensorImpl</span><span class="p">();</span>
<span class="w">    </span><span class="n">data_impl</span><span class="o">-&gt;</span><span class="n">set_allow_tensor_metadata_change</span><span class="p">(</span><span class="n">allow_tensor_metadata_change</span><span class="p">);</span>
<span class="w">    </span><span class="n">data_impl</span><span class="o">-&gt;</span><span class="n">set_autograd_meta</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">make_unique</span><span class="o">&lt;</span><span class="n">DifferentiableViewMeta</span><span class="o">&gt;</span><span class="p">(</span>
<span class="w">        </span><span class="n">data_impl</span><span class="p">,</span>
<span class="w">        </span><span class="n">std</span><span class="o">::</span><span class="n">move</span><span class="p">(</span><span class="n">backward_info</span><span class="p">),</span>
<span class="w">        </span><span class="n">std</span><span class="o">::</span><span class="n">move</span><span class="p">(</span><span class="n">forward_info</span><span class="p">),</span>
<span class="w">        </span><span class="n">shared_view_info</span><span class="p">,</span>
<span class="w">        </span><span class="n">creation_meta</span><span class="p">));</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">data</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="n">Variable</span><span class="p">();</span>
<span class="p">}</span>

<span class="c1">// See NOTE [ Autograd View Variables ] for details.</span>
<span class="c1">// Non-differentiable view. Just share version counter.</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">Variable</span><span class="w"> </span><span class="nf">make_variable_non_differentiable_view</span><span class="p">(</span>
<span class="w">    </span><span class="k">const</span><span class="w"> </span><span class="n">Variable</span><span class="o">&amp;</span><span class="w"> </span><span class="n">base</span><span class="p">,</span>
<span class="w">    </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&amp;</span><span class="w"> </span><span class="n">data</span><span class="p">,</span>
<span class="w">    </span><span class="kt">bool</span><span class="w"> </span><span class="n">allow_tensor_metadata_change</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nb">true</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">data</span><span class="p">.</span><span class="n">defined</span><span class="p">())</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="c1">// Currently all of non-differentiable view ops(detach/_indices/_values)</span>
<span class="w">    </span><span class="c1">// share the same TensorImpl as their base Tensor. Thus a new TensorImpl</span>
<span class="w">    </span><span class="c1">// allocation here is required.</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">data_impl_copy</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">data</span><span class="p">.</span><span class="n">getIntrusivePtr</span><span class="p">()</span><span class="o">-&gt;</span><span class="n">shallow_copy_and_detach</span><span class="p">(</span>
<span class="w">        </span><span class="cm">/*version_counter=*/</span><span class="n">impl</span><span class="o">::</span><span class="n">version_counter</span><span class="p">(</span><span class="n">base</span><span class="p">),</span>
<span class="w">        </span><span class="cm">/*allow_tensor_metadata_change=*/</span><span class="n">allow_tensor_metadata_change</span><span class="p">);</span>
<span class="w">    </span><span class="n">data_impl_copy</span><span class="o">-&gt;</span><span class="n">set_autograd_meta</span><span class="p">(</span><span class="k">nullptr</span><span class="p">);</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">Variable</span><span class="p">(</span><span class="n">data_impl_copy</span><span class="p">);</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="n">Variable</span><span class="p">();</span>
<span class="p">}</span>

<span class="kr">inline</span><span class="w"> </span><span class="n">Variable</span><span class="w"> </span><span class="nf">make_variable</span><span class="p">(</span>
<span class="w">    </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">data</span><span class="p">,</span>
<span class="w">    </span><span class="kt">bool</span><span class="w"> </span><span class="n">requires_grad</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nb">false</span><span class="p">,</span>
<span class="w">    </span><span class="kt">bool</span><span class="w"> </span><span class="n">allow_tensor_metadata_change</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nb">true</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">data</span><span class="p">.</span><span class="n">defined</span><span class="p">())</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">data</span><span class="p">.</span><span class="n">getIntrusivePtr</span><span class="p">().</span><span class="n">use_count</span><span class="p">()</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="o">&amp;&amp;</span>
<span class="w">        </span><span class="n">data</span><span class="p">.</span><span class="n">getIntrusivePtr</span><span class="p">()</span><span class="o">-&gt;</span><span class="n">unique_version</span><span class="p">())</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="k">auto</span><span class="w"> </span><span class="n">data_impl</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">data</span><span class="p">.</span><span class="n">unsafeReleaseIntrusivePtr</span><span class="p">();</span>
<span class="w">      </span><span class="n">data_impl</span><span class="o">-&gt;</span><span class="n">set_allow_tensor_metadata_change</span><span class="p">(</span><span class="n">allow_tensor_metadata_change</span><span class="p">);</span>
<span class="w">      </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">requires_grad</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="n">data_impl</span><span class="o">-&gt;</span><span class="n">set_autograd_meta</span><span class="p">(</span>
<span class="w">            </span><span class="n">std</span><span class="o">::</span><span class="n">make_unique</span><span class="o">&lt;</span><span class="n">AutogradMeta</span><span class="o">&gt;</span><span class="p">(</span><span class="n">data_impl</span><span class="p">.</span><span class="n">get</span><span class="p">(),</span><span class="w"> </span><span class="n">requires_grad</span><span class="p">));</span>
<span class="w">      </span><span class="p">}</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="n">data_impl</span><span class="o">-&gt;</span><span class="n">set_autograd_meta</span><span class="p">(</span><span class="k">nullptr</span><span class="p">);</span>
<span class="w">      </span><span class="p">}</span>
<span class="w">      </span><span class="k">return</span><span class="w"> </span><span class="n">Variable</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">move</span><span class="p">(</span><span class="n">data_impl</span><span class="p">));</span>
<span class="w">    </span><span class="p">}</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="k">auto</span><span class="w"> </span><span class="n">data_impl_copy</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">data</span><span class="p">.</span><span class="n">getIntrusivePtr</span><span class="p">()</span><span class="o">-&gt;</span><span class="n">shallow_copy_and_detach</span><span class="p">(</span>
<span class="w">          </span><span class="cm">/*version_counter=*/</span><span class="mi">0</span><span class="p">,</span>
<span class="w">          </span><span class="cm">/*allow_tensor_metadata_change=*/</span><span class="n">allow_tensor_metadata_change</span><span class="p">);</span>
<span class="w">      </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">requires_grad</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="n">data_impl_copy</span><span class="o">-&gt;</span><span class="n">set_autograd_meta</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">make_unique</span><span class="o">&lt;</span><span class="n">AutogradMeta</span><span class="o">&gt;</span><span class="p">(</span>
<span class="w">            </span><span class="n">data_impl_copy</span><span class="p">.</span><span class="n">get</span><span class="p">(),</span><span class="w"> </span><span class="n">requires_grad</span><span class="p">));</span>
<span class="w">      </span><span class="p">}</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="n">data_impl_copy</span><span class="o">-&gt;</span><span class="n">set_autograd_meta</span><span class="p">(</span><span class="k">nullptr</span><span class="p">);</span>
<span class="w">      </span><span class="p">}</span>
<span class="w">      </span><span class="k">return</span><span class="w"> </span><span class="n">Variable</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">move</span><span class="p">(</span><span class="n">data_impl_copy</span><span class="p">));</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="n">Variable</span><span class="p">();</span>
<span class="p">}</span>

<span class="kr">inline</span><span class="w"> </span><span class="n">Variable</span><span class="w"> </span><span class="nf">make_variable</span><span class="p">(</span>
<span class="w">    </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&amp;</span><span class="w"> </span><span class="n">data</span><span class="p">,</span>
<span class="w">    </span><span class="n">Edge</span><span class="w"> </span><span class="n">gradient_edge</span><span class="p">,</span>
<span class="w">    </span><span class="kt">bool</span><span class="w"> </span><span class="n">allow_tensor_metadata_change</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nb">true</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">data</span><span class="p">.</span><span class="n">defined</span><span class="p">())</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">data_impl_copy</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">data</span><span class="p">.</span><span class="n">getIntrusivePtr</span><span class="p">()</span><span class="o">-&gt;</span><span class="n">shallow_copy_and_detach</span><span class="p">(</span>
<span class="w">        </span><span class="cm">/*version_counter=*/</span><span class="mi">0</span><span class="p">,</span>
<span class="w">        </span><span class="cm">/*allow_tensor_metadata_change=*/</span><span class="n">allow_tensor_metadata_change</span><span class="p">);</span>
<span class="w">    </span><span class="n">data_impl_copy</span><span class="o">-&gt;</span><span class="n">set_autograd_meta</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">make_unique</span><span class="o">&lt;</span><span class="n">AutogradMeta</span><span class="o">&gt;</span><span class="p">(</span>
<span class="w">        </span><span class="n">data_impl_copy</span><span class="p">.</span><span class="n">get</span><span class="p">(),</span><span class="w"> </span><span class="nb">false</span><span class="p">,</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">move</span><span class="p">(</span><span class="n">gradient_edge</span><span class="p">)));</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">Variable</span><span class="p">(</span><span class="n">data_impl_copy</span><span class="p">);</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="n">Variable</span><span class="p">();</span>
<span class="p">}</span>

<span class="k">struct</span><span class="w"> </span><span class="nc">VariableHooks</span><span class="w"> </span><span class="k">final</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">impl</span><span class="o">::</span><span class="n">VariableHooksInterface</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">TensorBase</span><span class="w"> </span><span class="nf">tensor_data</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">TensorBase</span><span class="o">&amp;</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="k">override</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">TensorBase</span><span class="w"> </span><span class="nf">variable_data</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">TensorBase</span><span class="o">&amp;</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="k">override</span><span class="p">;</span>
<span class="w">  </span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">torch</span><span class="o">::</span><span class="n">autograd</span><span class="o">::</span><span class="n">Node</span><span class="o">&gt;&amp;</span><span class="w"> </span><span class="n">grad_fn</span><span class="p">(</span>
<span class="w">      </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">TensorBase</span><span class="o">&amp;</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="k">override</span><span class="p">;</span>
<span class="w">  </span><span class="kt">unsigned</span><span class="w"> </span><span class="nf">_register_hook</span><span class="p">(</span>
<span class="w">      </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">TensorBase</span><span class="o">&amp;</span><span class="p">,</span>
<span class="w">      </span><span class="n">std</span><span class="o">::</span><span class="n">function</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">TensorBase</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">TensorBase</span><span class="o">&amp;</span><span class="p">)</span><span class="o">&gt;</span><span class="w"> </span><span class="n">hook</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="k">override</span><span class="p">;</span>
<span class="w">  </span><span class="kt">void</span><span class="w"> </span><span class="nf">remove_hook</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">TensorBase</span><span class="o">&amp;</span><span class="p">,</span><span class="w"> </span><span class="kt">unsigned</span><span class="w"> </span><span class="n">pos</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="k">override</span><span class="p">;</span>
<span class="w">  </span><span class="kt">bool</span><span class="w"> </span><span class="nf">is_view</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">TensorBase</span><span class="o">&amp;</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="k">override</span><span class="p">;</span>
<span class="w">  </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">TensorBase</span><span class="o">&amp;</span><span class="w"> </span><span class="nf">base</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">TensorBase</span><span class="o">&amp;</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="k">override</span><span class="p">;</span>
<span class="w">  </span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="o">&amp;</span><span class="w"> </span><span class="nf">name</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">TensorBase</span><span class="o">&amp;</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="k">override</span><span class="p">;</span>
<span class="w">  </span><span class="kt">bool</span><span class="w"> </span><span class="nf">is_leaf</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">TensorBase</span><span class="o">&amp;</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="k">override</span><span class="p">;</span>
<span class="w">  </span><span class="kt">int64_t</span><span class="w"> </span><span class="nf">output_nr</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">TensorBase</span><span class="o">&amp;</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="k">override</span><span class="p">;</span>
<span class="w">  </span><span class="kt">void</span><span class="w"> </span><span class="nf">set_data</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">TensorBase</span><span class="o">&amp;</span><span class="w"> </span><span class="n">self</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">TensorBase</span><span class="o">&amp;</span><span class="w"> </span><span class="n">new_data</span><span class="p">)</span>
<span class="w">      </span><span class="k">const</span><span class="w"> </span><span class="k">override</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">TensorBase</span><span class="w"> </span><span class="nf">data</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">TensorBase</span><span class="o">&amp;</span><span class="w"> </span><span class="n">self</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="k">override</span><span class="p">;</span>
<span class="w">  </span><span class="kt">int64_t</span><span class="w"> </span><span class="nf">_version</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">TensorBase</span><span class="o">&amp;</span><span class="w"> </span><span class="n">self</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="k">override</span><span class="p">;</span>
<span class="w">  </span><span class="kt">void</span><span class="w"> </span><span class="nf">retain_grad</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">TensorBase</span><span class="o">&amp;</span><span class="w"> </span><span class="n">self</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="k">override</span><span class="p">;</span>
<span class="w">  </span><span class="kt">bool</span><span class="w"> </span><span class="nf">retains_grad</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">TensorBase</span><span class="o">&amp;</span><span class="w"> </span><span class="n">self</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="k">override</span><span class="p">;</span>
<span class="w">  </span><span class="kt">void</span><span class="w"> </span><span class="nf">_backward</span><span class="p">(</span>
<span class="w">      </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&amp;</span><span class="w"> </span><span class="n">self</span><span class="p">,</span>
<span class="w">      </span><span class="n">at</span><span class="o">::</span><span class="n">TensorList</span><span class="w"> </span><span class="n">inputs</span><span class="p">,</span>
<span class="w">      </span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;&amp;</span><span class="w"> </span><span class="n">gradient</span><span class="p">,</span>
<span class="w">      </span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="kt">bool</span><span class="o">&gt;</span><span class="w"> </span><span class="n">keep_graph</span><span class="p">,</span>
<span class="w">      </span><span class="kt">bool</span><span class="w"> </span><span class="n">create_graph</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="k">override</span><span class="p">;</span>
<span class="w">  </span><span class="kt">void</span><span class="w"> </span><span class="nf">requires_grad_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">TensorBase</span><span class="o">&amp;</span><span class="w"> </span><span class="n">self</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">_requires_grad</span><span class="p">)</span>
<span class="w">      </span><span class="k">const</span><span class="w"> </span><span class="k">override</span><span class="p">;</span>
<span class="w">  </span><span class="kt">void</span><span class="w"> </span><span class="nf">basic_autograd_not_implemented_fallback</span><span class="p">(</span>
<span class="w">      </span><span class="k">const</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">OperatorHandle</span><span class="o">&amp;</span><span class="w"> </span><span class="n">op</span><span class="p">,</span>
<span class="w">      </span><span class="n">c10</span><span class="o">::</span><span class="n">DispatchKeySet</span><span class="w"> </span><span class="n">dispatch_keys</span><span class="p">,</span>
<span class="w">      </span><span class="n">torch</span><span class="o">::</span><span class="n">jit</span><span class="o">::</span><span class="n">Stack</span><span class="o">*</span><span class="w"> </span><span class="n">stack</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="k">override</span><span class="p">;</span>
<span class="p">};</span>

<span class="k">namespace</span><span class="w"> </span><span class="nn">utils</span><span class="w"> </span><span class="p">{</span>

<span class="n">TORCH_API</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">has_same_meta</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">Variable</span><span class="o">&amp;</span><span class="w"> </span><span class="n">base</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">Variable</span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>

<span class="p">}</span><span class="w"> </span><span class="c1">// namespace utils</span>
<span class="p">}</span><span class="w"> </span><span class="c1">// namespace torch::autograd</span>

<span class="cp">#endif </span><span class="cm">/* DOXYGEN_SHOULD_SKIP_THIS */</span>
</pre></div>
</div>
</div>


                </article>
              
  </article>
  
              
              
                <footer class="bd-footer-article">
                  <div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item">
<div class="feedback">
  
<div class="rating">
    Rate this Page
    <div class="stars">
        
        <span class="star" data-behavior="tutorial-rating" data-count="1" data-value="1">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="2" data-value="2">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="3" data-value="3">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="4" data-value="4">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="5" data-value="5">★</span>
        
    </div>
</div>

  <div class="feedback-send">
    <button class="feedback-btn"
            onclick="openGitHubIssue()"
            data-bs-title="Create a GitHub Issue"
            data-bs-placement="bottom"
            data-bs-toggle="tooltip"
            data-gtm="feedback-btn-click">Send Feedback
    </button>
  </div>
</div>

<div class="prev-next-area">
    <a class="left-prev"
       href="file_torch_csrc_autograd_variable.h.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">File variable.h</p>
      </div>
    </a>
    <a class="right-next"
       href="file_torch_csrc_autograd_generated_variable_factories.h.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">File variable_factories.h</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>

<div class="footer-info">
  <p class="copyright">
    
      
        © Copyright PyTorch Contributors.
      
      <br/>
    
  </p>

  <p class="theme-version">
    Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.15.4.
  </p>
</div>
</div>
  
</div>
                </footer>
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="file_torch_csrc_autograd_variable.h.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">File variable.h</p>
      </div>
    </a>
    <a class="right-next"
       href="file_torch_csrc_autograd_generated_variable_factories.h.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">File variable_factories.h</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc">
<div class="sidebar-secondary-items sidebar-secondary__inner">
    
       <div class="sidebar-secondary-item">
    <div class="tocsection sourcelink">
      <a href="../_sources/api/program_listing_file_torch_csrc_autograd_variable.h.rst.txt">
        <i class="fa-solid fa-file-lines"></i> Show Source
      </a>
    </div>
</div>
    




<div class="sidebar-secondary-item">
  <div class="sidebar-heading">PyTorch Libraries</div>
  <ul style="list-style-type: none; padding: 0; padding-bottom: 80px;">
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/ao" style="color: var(--pst-color-text-muted)">torchao</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/torchrec" style="color: var(--pst-color-text-muted)">torchrec</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/torchft" style="color: var(--pst-color-text-muted)">torchft</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/torchcodec" style="color: var(--pst-color-text-muted)">TorchCodec</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/vision" style="color: var(--pst-color-text-muted)">torchvision</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/executorch" style="color: var(--pst-color-text-muted)">ExecuTorch</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/xla" style="color: var(--pst-color-text-muted)">PyTorch on XLA Devices</a></li>
  
  </ul>
</div>

</div>
</div>
              
            
          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
  <div class="container">
    <div class="row">
      <div class="col-md-4">
        <h2>Docs</h2>
        <p>Access comprehensive developer documentation for PyTorch</p>
        <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
      </div>

      <div class="col-md-4">
        <h2>Tutorials</h2>
        <p>Get in-depth tutorials for beginners and advanced developers</p>
        <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
      </div>

      <div class="col-md-4">
        <h2>Resources</h2>
        <p>Find development resources and get your questions answered</p>
        <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
      </div>
    </div>
  </div>
</div>

<footer class="site-footer">
  <div class="container footer-container">

    
    <div class="newsletter" id="newsletter">

      <p class="newsletter__title is-style-max-width-800"><strong>Stay in touch</strong> for updates, event info, and
        the latest news</p>


      <script charset="utf-8" type="text/javascript" src="//js.hsforms.net/forms/embed/v2.js"></script>
      <script>
        hbspt.forms.create({
          region: "na1",
          portalId: "8112310",
          formId: "2fb2231c-000b-4ec5-88a0-1ab242549c9e"
        });
      </script>


      <p class="newsletter__privacy">By submitting this form, I consent to receive marketing emails from the LF and its
        projects regarding their events, training, research, developments, and related announcements. I understand that
        I can unsubscribe at any time using the links in the footers of the emails I receive. <a
          href="https://www.linuxfoundation.org/privacy/">Privacy Policy</a>.</p>

    </div>
    

    <div class="lf-grid">
      <ul class="social-links">
        <li><a href="https://www.facebook.com/pytorch" target="_blank" title="PyTorch on Facebook">
            <svg xmlns="http://www.w3.org/2000/svg" viewbox="-0.51 -0.26 26.45 26.45" aria-label="Facebook">
              <path fill="currentColor"
                d="M25.497 13.075c0-2.45-.698-4.848-2.011-6.911a12.765 12.765 0 0 0-5.398-4.73A12.671 12.671 0 0 0 11.008.38a12.705 12.705 0 0 0-6.529 2.95A12.827 12.827 0 0 0 .563 9.358a12.896 12.896 0 0 0-.07 7.201 12.831 12.831 0 0 0 3.801 6.103 12.709 12.709 0 0 0 6.471 3.078v-8.957H7.53v-3.708h3.235v-2.824c0-3.213 1.903-4.988 4.813-4.988.956.014 1.909.097 2.852.25V8.67h-1.607a1.83 1.83 0 0 0-1.518.497 1.854 1.854 0 0 0-.561 1.505v2.404h3.535l-.563 3.708h-2.97v8.957a12.725 12.725 0 0 0 7.697-4.337 12.87 12.87 0 0 0 3.054-8.328z" />
            </svg>
          </a></li>
        <li><a href="https://twitter.com/pytorch" target="_blank" title="PyTorch on X">
            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 300 300" aria-label="X">
              <path fill="currentColor"
                d="M178.57 127.15 290.27 0h-26.46l-97.03 110.38L89.34 0H0l117.13 166.93L0 300.25h26.46l102.4-116.59 81.8 116.59h89.34M36.01 19.54H76.66l187.13 262.13h-40.66" />
            </svg>
          </a></li>
        <li><a href="https://www.youtube.com/pytorch" target="_blank" title="PyTorch on YouTube">
            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0.21 0.27 34.45 25.07" aria-label="YouTube">
              <path fill="currentColor"
                d="M33.729 6.084s-.327-2.33-1.317-3.356a4.691 4.691 0 0 0-3.32-1.432c-4.634-.34-11.589-.34-11.589-.34h-.014s-6.954 0-11.59.342a4.692 4.692 0 0 0-3.32 1.432c-.993 1.025-1.315 3.354-1.315 3.354a52.189 52.189 0 0 0-.331 5.473v2.566c.014 1.829.125 3.656.331 5.472 0 0 .322 2.33 1.316 3.36 1.26 1.345 2.916 1.3 3.653 1.445 2.65.26 11.263.34 11.263.34s6.96-.01 11.597-.353a4.691 4.691 0 0 0 3.32-1.432c.993-1.026 1.316-3.356 1.316-3.356.206-1.817.316-3.644.33-5.473v-2.57a52.26 52.26 0 0 0-.33-5.472zM14.076 17.232V7.729l8.951 4.768-8.95 4.735z" />
            </svg>
          </a></li>
        <li><a href="https://www.linkedin.com/company/pytorch" target="_blank" title="PyTorch on LinkedIn">
            <svg xmlns="http://www.w3.org/2000/svg" viewbox="-10.23 -10.23 531.96 531.96" aria-label="LinkedIn">
              <rect width="512" height="512" rx="0" fill="currentColor" />
              <circle fill="#000" cx="142" cy="138" r="37" />
              <path stroke="#000" stroke-width="66" d="M244 194v198M142 194v198" />
              <path fill="#000"
                d="M276 282c0-20 13-40 36-40 24 0 33 18 33 45v105h66V279c0-61-32-89-76-89-34 0-51 19-59 32" />
            </svg>
          </a></li>
        <li><a href="https://pytorch.slack.com" target="_blank" title="PyTorch Slack">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0.16 -0.03 21.19 21.19" aria-label="Slack">
              <path fill="currentColor"
                d="M4.896 13.27a2.147 2.147 0 0 1-2.141 2.142A2.147 2.147 0 0 1 .613 13.27c0-1.178.963-2.141 2.142-2.141h2.141v2.141zm1.08 0c0-1.178.962-2.141 2.141-2.141s2.142.963 2.142 2.141v5.363a2.147 2.147 0 0 1-2.142 2.141 2.147 2.147 0 0 1-2.141-2.142V13.27zm2.141-8.6a2.147 2.147 0 0 1-2.141-2.14c0-1.18.962-2.142 2.141-2.142s2.142.963 2.142 2.141v2.142H8.117zm0 1.08c1.179 0 2.141.962 2.141 2.141a2.147 2.147 0 0 1-2.141 2.142H2.755A2.147 2.147 0 0 1 .613 7.89c0-1.179.963-2.141 2.142-2.141h5.362zm8.599 2.141c0-1.179.963-2.141 2.141-2.141 1.179 0 2.143.962 2.143 2.14a2.147 2.147 0 0 1-2.142 2.142h-2.141V7.89zm-1.08 0a2.147 2.147 0 0 1-2.141 2.142 2.147 2.147 0 0 1-2.141-2.142V2.53c0-1.178.962-2.141 2.141-2.141s2.142.963 2.142 2.141v5.362zm-2.141 8.6c1.179 0 2.142.962 2.142 2.14a2.147 2.147 0 0 1-2.142 2.142 2.147 2.147 0 0 1-2.141-2.141V16.49h2.141zm0-1.08a2.147 2.147 0 0 1-2.141-2.141c0-1.179.962-2.142 2.141-2.142h5.362c1.179 0 2.142.963 2.142 2.142a2.147 2.147 0 0 1-2.142 2.142h-5.362z">
              </path>
            </svg>
          </a></li>
        <li><a href="https://pytorch.org/wechat" title="PyTorch on WeChat">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0.14 -0.17 38.02 33.02" aria-label="WeChat">
              <path fill="currentColor"
                d="M26.289 10.976a12.972 12.972 0 0 0-8.742 3.53 10.386 10.386 0 0 0-3.224 8.795c-1.326-.164-2.535-.345-3.75-.448a2.332 2.332 0 0 0-1.273.216c-1.18.666-2.311 1.418-3.652 2.255.246-1.112.405-2.087.687-3.024a1.15 1.15 0 0 0-.523-1.52C1.737 17.902.02 13.601 1.307 9.165c1.189-4.1 4.11-6.587 8.077-7.884A13.54 13.54 0 0 1 24.18 5.617a10.135 10.135 0 0 1 2.109 5.359zM10.668 9.594a1.564 1.564 0 0 0-2.095-1.472 1.52 1.52 0 0 0-.895 1.964 1.502 1.502 0 0 0 1.391.966 1.545 1.545 0 0 0 1.598-1.46v.002zm8.15-1.566a1.567 1.567 0 0 0-1.528 1.543 1.528 1.528 0 0 0 1.571 1.492 1.52 1.52 0 0 0 1.375-2.117 1.518 1.518 0 0 0-1.415-.919l-.003.001z">
              </path>
              <path fill="currentColor"
                d="M33.914 32.137c-1.075-.478-2.062-1.196-3.11-1.306-1.049-.11-2.145.494-3.24.605a10.821 10.821 0 0 1-8.781-2.864c-4.682-4.33-4.013-10.97 1.403-14.518 4.811-3.154 11.874-2.102 15.268 2.273a8.671 8.671 0 0 1-1.002 12.095c-1.046.929-1.422 1.693-.751 2.917.102.257.174.525.213.798zM21.68 20.292a1.264 1.264 0 1 0 .01-2.528 1.264 1.264 0 0 0-.01 2.528zm7.887-2.526a1.266 1.266 0 0 0-1.256 1.21 1.247 1.247 0 1 0 1.256-1.21z">
              </path>
            </svg>
          </a></li>
      </ul>
    </div>

    <div class="privacy-policy">
      <div class="copyright">
        
        <p>
          &copy; PyTorch. Copyright © The Linux Foundation®. All rights reserved. The Linux Foundation has registered
          trademarks and uses trademarks. For more information, including terms of use, privacy policy, and trademark
          usage, please see our <a href="https://www.linuxfoundation.org/legal/policies">Policies</a> page. <a
            href="https://www.linuxfoundation.org/trademark-usage">Trademark Usage</a>. <a
            href="http://www.linuxfoundation.org/privacy">Privacy Policy</a>.
        </p>
        
      </div>
    </div>


  </div>
</footer>

<div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../_static/img/pytorch-x.svg">
  </div>
</div>
  
  <footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
  
    <div class="footer-items__start">
      
        <div class="footer-item">

  <p class="copyright">
    
      © Copyright PyTorch Contributors.
      <br/>
    
  </p>
</div>
      
        <div class="footer-item">

  <p class="sphinx-version">
    Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 5.3.0.
    <br/>
  </p>
</div>
      
    </div>
  
  
  
    <div class="footer-items__end">
      
        <div class="footer-item">
<p class="theme-version">
  Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.15.4.
</p></div>
      
    </div>
  
</div>

  </footer>
  <script type="application/ld+json">
    {
       "@context": "https://schema.org",
       "@type": "Article",
       "name": "Program Listing for File variable.h",
       "headline": "Program Listing for File variable.h",
       "description": "PyTorch Documentation. Explore PyTorch, an open-source machine learning library that accelerates the path from research prototyping to production deployment. Discover tutorials, API references, and guides to help you build and deploy deep learning models efficiently.",
       "url": "/api/program_listing_file_torch_csrc_autograd_variable.h.html",
       "articleBody": "Program Listing for File variable.h# \u21b0 Return to documentation for file (torch/csrc/autograd/variable.h) #pragma once #include \u003ctorch/csrc/utils/python_stub.h\u003e #include \u003ctorch/csrc/Export.h\u003e #include \u003ctorch/csrc/autograd/cpp_hook.h\u003e #include \u003ctorch/csrc/autograd/edge.h\u003e #include \u003ctorch/csrc/autograd/forward_grad.h\u003e #include \u003ctorch/csrc/autograd/function_hook.h\u003e #include \u003cATen/NamedTensorUtils.h\u003e #include \u003cATen/core/Tensor.h\u003e #include \u003cATen/core/VariableHooksInterface.h\u003e #include \u003cc10/util/Exception.h\u003e #include \u003ccstdint\u003e #include \u003cmemory\u003e #include \u003cmutex\u003e #include \u003cstring\u003e #include \u003cutility\u003e #include \u003cvector\u003e namespace torch::autograd { using Variable = at::Tensor; } // namespace torch::autograd // The following are all internal APIs and should not be shown in libtorch docs. // Therefore, we wrap the following code with `#ifndef DOXYGEN_SHOULD_SKIP_THIS // ... #endif` #ifndef DOXYGEN_SHOULD_SKIP_THIS namespace torch::autograd { inline bool isDifferentiableType(at::ScalarType t) { return isFloatingType(t) || isComplexType(t); } struct Node; struct AutogradMeta; struct DifferentiableViewMeta; // Private-ish functions for manipulating variables; we don\u0027t want to put them // on Tensor proper namespace impl { // WARNING: This may return a nullptr. If you require AutogradMeta to return // a materialized structure, use materialize_autograd_meta instead. TORCH_API AutogradMeta* get_autograd_meta(const at::TensorBase\u0026); // WARNING: This will return a nullptr if the Tensor is not a view. TORCH_API DifferentiableViewMeta* get_view_autograd_meta(const at::TensorBase\u0026); // Returns the current autograd meta, materializing it if it was previously // none. This counts as a *mutating* operation, so do not call it on // \"read-only\" operators; in particular, this is NOT thread safe TORCH_API AutogradMeta* materialize_autograd_meta(const at::TensorBase\u0026); TORCH_API void set_grad_accumulator( const Variable\u0026, std::weak_ptr\u003cNode\u003e grad_accumulator); TORCH_API std::shared_ptr\u003cNode\u003e try_get_grad_accumulator(const Variable\u0026); TORCH_API std::shared_ptr\u003cNode\u003e grad_accumulator(const Variable\u0026); TORCH_API Edge gradient_edge(const Variable\u0026); TORCH_API void set_gradient_edge(const Variable\u0026, Edge edge); // Autograd Graph Interaction //~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ TORCH_API void rebase_history(const Variable\u0026, Edge gradient_edge); TORCH_API Node* grad_fn_unsafe(const Variable\u0026); TORCH_API void bump_version(const Variable\u0026); TORCH_API void set_version_counter( const Variable\u0026, const c10::VariableVersion\u0026 version_counter); TORCH_API const c10::VariableVersion\u0026 version_counter(const Variable\u0026); TORCH_API void set_name(const Variable\u0026, const std::string\u0026 name); TORCH_API void add_hook( const at::TensorBase\u0026, std::unique_ptr\u003cFunctionPreHook\u003e hook); TORCH_API std::vector\u003cstd::unique_ptr\u003cFunctionPreHook\u003e\u003e\u0026 hooks(const Variable\u0026); TORCH_API void clear_hooks(const at::TensorBase\u0026); TORCH_API void set_post_acc_grad_hooks( const at::TensorBase\u0026, std::unique_ptr\u003cPostAccumulateGradHook\u003e dict); TORCH_API std::unique_ptr\u003cPostAccumulateGradHook\u003e\u0026 post_acc_grad_hooks( const Variable\u0026); TORCH_API void create_cpp_hook( const at::TensorBase\u0026, bool is_retains_grad_hooks = false); } // namespace impl //~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ // AutogradMeta //~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ struct TORCH_API AutogradMeta : public c10::AutogradMetaInterface { std::string name_; Variable grad_; std::shared_ptr\u003cNode\u003e grad_fn_; std::weak_ptr\u003cNode\u003e grad_accumulator_; // This field is used to store all the forward AD gradients // associated with this AutogradMeta (and the Tensor it corresponds to) // There is a semantic 1:1 correspondence between AutogradMeta and // ForwardGrad but: // - This field is lazily populated. // - This field is a shared_ptr but it must never be // shared by multiple Tensors. See Note [ Using ForwardGrad ] // Any transition from not_initialized to initialized // must be protected by mutex_ mutable std::shared_ptr\u003cForwardGrad\u003e fw_grad_; // The hooks_ field is actually reused by both python and cpp logic // For both cases, we have a data structure, cpp_hooks_list_ (cpp) // or dict (python) which is the canonical copy. // Then, for both cases, we always register a single hook to // hooks_ which wraps all the hooks in the list/dict. // And, again in both cases, if the grad_fn exists on that tensor // we will additionally register a single hook to the grad_fn. // // Note that the cpp and python use cases aren\u0027t actually aware of // each other, so using both is not defined behavior. std::vector\u003cstd::unique_ptr\u003cFunctionPreHook\u003e\u003e hooks_; std::shared_ptr\u003chooks_list\u003e cpp_hooks_list_; // The post_acc_grad_hooks_ field stores only Python hooks // (PyFunctionTensorPostAccGradHooks) that are called after the // .grad field has been accumulated into. This is less complicated // than the hooks_ field, which encapsulates a lot more. std::unique_ptr\u003cPostAccumulateGradHook\u003e post_acc_grad_hooks_ = nullptr; // Only meaningful on leaf variables (must be false otherwise) bool requires_grad_{false}; // Only meaningful on non-leaf variables (must be false otherwise) bool retains_grad_{false}; bool is_view_{false}; // The \"output number\" of this variable; e.g., if this variable // was the second output of a function, then output_nr == 1. // We use this to make sure we can setup the backwards trace // correctly when this variable is passed to another function. uint32_t output_nr_; // Mutex to ensure that concurrent read operations that modify internal // state are still thread-safe. Used by grad_fn(), grad_accumulator(), // fw_grad() and set_fw_grad() // This is mutable because we need to be able to acquire this from const // version of this class for the functions above mutable std::mutex mutex_; void set_requires_grad(bool requires_grad, at::TensorImpl* self_impl) final { TORCH_CHECK( !requires_grad || isDifferentiableType(at::typeMetaToScalarType(self_impl-\u003edtype())), \"Only Tensors of floating point and complex dtype can require gradients\"); requires_grad_ = requires_grad; } bool requires_grad() const override { return requires_grad_ || grad_fn_; } Variable\u0026 mutable_grad() override { return grad_; } const Variable\u0026 grad() const override { return grad_; } const Variable\u0026 fw_grad(uint64_t level, const at::TensorBase\u0026 self) const override; void set_fw_grad( const at::TensorBase\u0026 new_grad, const at::TensorBase\u0026 self, uint64_t level, bool is_inplace_op) override; AutogradMeta( at::TensorImpl* self_impl = nullptr, bool requires_grad = false, Edge gradient_edge = Edge()) : grad_fn_(std::move(gradient_edge.function)), output_nr_(gradient_edge.input_nr) { // set_requires_grad also checks error conditions. if (requires_grad) { TORCH_INTERNAL_ASSERT(self_impl); set_requires_grad(requires_grad, self_impl); } TORCH_CHECK( !grad_fn_ || !requires_grad_, \"requires_grad should be false if grad_fn is set\"); } ~AutogradMeta() override { // If AutogradMeta is being destroyed, it means that there is no other // reference to its corresponding Tensor. It implies that no other thread // can be using this object and so there is no need to lock mutex_ here to // guard the check if fw_grad_ is populated. if (fw_grad_) { // See note [ Using ForwardGrad ] fw_grad_-\u003eclear(); } } }; struct TORCH_API ViewFunc { virtual ~ViewFunc() = default; virtual std::vector\u003cc10::SymInt\u003e get_symints() const { return {}; } virtual size_t num_symints() const { return 0; } virtual std::vector\u003cat::Tensor\u003e get_tensors() const { return {}; } virtual size_t num_tensors() const { return 0; } virtual at::Tensor operator()(const at::Tensor\u0026) const = 0; virtual std::unique_ptr\u003cViewFunc\u003e clone_and_set( std::optional\u003cstd::vector\u003cc10::SymInt\u003e\u003e = std::nullopt, std::optional\u003cstd::vector\u003cat::Tensor\u003e\u003e = std::nullopt) const = 0; protected: virtual void set_symints(std::vector\u003cc10::SymInt\u003e) {} virtual void set_tensors(std::vector\u003cat::Tensor\u003e) {} }; struct ChainedViewFunc : public ViewFunc { ChainedViewFunc( std::unique_ptr\u003cViewFunc\u003e first, std::unique_ptr\u003cViewFunc\u003e second) : first(std::move(first)), second(std::move(second)) {} ~ChainedViewFunc() override = default; std::vector\u003cc10::SymInt\u003e get_symints() const override; size_t num_symints() const override { return first-\u003enum_symints() + second-\u003enum_symints(); } std::vector\u003cat::Tensor\u003e get_tensors() const override; size_t num_tensors() const override { return first-\u003enum_tensors() + second-\u003enum_tensors(); } at::Tensor operator()(const at::Tensor\u0026) const override; std::unique_ptr\u003cViewFunc\u003e clone_and_set( std::optional\u003cstd::vector\u003cc10::SymInt\u003e\u003e = std::nullopt, std::optional\u003cstd::vector\u003cat::Tensor\u003e\u003e = std::nullopt) const override; private: std::unique_ptr\u003cViewFunc\u003e first; std::unique_ptr\u003cViewFunc\u003e second; }; struct ErroringViewFunc : public ViewFunc { ErroringViewFunc(std::string error_msg) : error_msg(std::move(error_msg)) {} ~ErroringViewFunc() override = default; at::Tensor operator()(const at::Tensor\u0026) const override { TORCH_CHECK(false, error_msg); } std::unique_ptr\u003cViewFunc\u003e clone_and_set( std::optional\u003cstd::vector\u003cc10::SymInt\u003e\u003e = std::nullopt, std::optional\u003cstd::vector\u003cat::Tensor\u003e\u003e = std::nullopt) const override { return std::make_unique\u003cErroringViewFunc\u003e(error_msg); } private: std::string error_msg; }; struct TORCH_API ViewInfo { Variable base_; std::unique_ptr\u003cViewFunc\u003e view_fn_; std::function\u003cVariable(const Variable\u0026)\u003e rev_view_fn_; bool has_view_fn() const { // assume either BOTH or NEITHER of view_fn_ and rev_view_fn_ exist return view_fn_ != nullptr; } const ViewFunc\u0026 view_fn() const { TORCH_CHECK( has_view_fn(), \"Can only access the view function if it exists.\"); return *view_fn_; } std::function\u003cVariable(const Variable\u0026)\u003e rev_view_fn() const { TORCH_CHECK( has_view_fn(), \"Can only access the reverse view function if it exists.\"); return rev_view_fn_; } ViewInfo chain( const Variable\u0026 base, const Variable\u0026 tensor, std::unique_ptr\u003cViewFunc\u003e view_func = nullptr, std::function\u003cVariable(const Variable\u0026)\u003e rev_view_func = nullptr) const; ViewInfo( Variable base, std::unique_ptr\u003cViewFunc\u003e view_fn, std::function\u003cVariable(const Variable\u0026)\u003e rev_view_fn) : base_(std::move(base)), view_fn_(std::move(view_fn)), rev_view_fn_(std::move(rev_view_fn)) { TORCH_CHECK(base_.defined(), \"base is undefined\"); } }; //~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ // DifferentiableViewMeta //~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ enum class CreationMeta : uint8_t { DEFAULT, IN_CUSTOM_FUNCTION, MULTI_OUTPUT_NODE, NO_GRAD_MODE, INFERENCE_MODE }; inline CreationMeta propagate_creation_meta( CreationMeta prev_view_creation_meta, CreationMeta new_view_creation_meta) { return (new_view_creation_meta == CreationMeta::DEFAULT) ? prev_view_creation_meta : (prev_view_creation_meta == CreationMeta::INFERENCE_MODE ? prev_view_creation_meta : new_view_creation_meta); } TORCH_API void handle_view_on_rebase( DifferentiableViewMeta* diff_view_meta, bool indirect = false); struct TORCH_API DifferentiableViewMeta : public AutogradMeta { private: std::optional\u003cViewInfo\u003e backward_info_; std::optional\u003cViewInfo\u003e forward_info_; // Optimization to reduce the number of ViewInfo we create. // In the (very common) case where backward_info_ == forward_info_, we only // populate backward_info_ (that should be used as both the forward and // backward view information) and set shared_view_info_ = true. Invariants: // - If shared_view_info_ is false, there is no special constraints on // backward_info_ and forward_info_ // - If shared_view_info_ is true, we must have: // - backward_info_.has_value() == true // - forward_info_.has_value() == false bool shared_view_info_; uint32_t attr_version_; CreationMeta creation_meta_; public: bool requires_grad() const override { return requires_grad_ || grad_fn_ || (has_bw_view() \u0026\u0026 get_backward_view().base_.requires_grad()); } bool shared_view_info() const { return shared_view_info_; } bool has_bw_view() const { return backward_info_.has_value(); } const ViewInfo\u0026 get_backward_view() const { TORCH_CHECK( has_bw_view(), \"backward view info can only exist for backward views.\"); // NOLINTNEXTLINE(bugprone-unchecked-optional-access) return backward_info_.value(); } uint32_t get_attr_version() const { TORCH_CHECK( has_bw_view(), \"attr_version can only exist for backward views.\"); return attr_version_; } void set_attr_version(uint32_t new_attr_version) { TORCH_CHECK( has_bw_view(), \"attr_version can only exist for backward views.\"); attr_version_ = new_attr_version; } CreationMeta get_creation_meta() const { TORCH_CHECK( has_bw_view(), \"creation_meta can only exist for backward views.\"); return creation_meta_; } void set_creation_meta(CreationMeta new_creation_meta) { TORCH_CHECK( has_bw_view(), \"creation_meta can only exist for backward views.\"); creation_meta_ = new_creation_meta; } bool has_fw_view() const { return shared_view_info_ || forward_info_.has_value(); } const ViewInfo\u0026 get_forward_view() const { TORCH_CHECK( has_fw_view(), \"forward view info can only exist for forward views.\"); TORCH_CHECK( !shared_view_info_ || has_bw_view(), \"forward view info can only exist for forward views.\"); // NOLINTNEXTLINE(bugprone-unchecked-optional-access) return shared_view_info_ ? backward_info_.value() : forward_info_.value(); } DifferentiableViewMeta( at::TensorImpl* self_impl, std::optional\u003cViewInfo\u003e backward_info, std::optional\u003cViewInfo\u003e forward_info, bool shared_view_info, CreationMeta creation_meta = CreationMeta::DEFAULT); }; //~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ // Variable Implementation //~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ // Factory Functions //~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ // See NOTE [ Autograd View Variables ] for details. // Differentiable view. Track history with DifferentiableViewMeta. inline Variable make_variable_differentiable_view( const at::Tensor\u0026 data, std::optional\u003cViewInfo\u003e backward_info, std::optional\u003cViewInfo\u003e forward_info, bool shared_view_info, CreationMeta creation_meta, bool allow_tensor_metadata_change = true) { if (data.defined()) { TORCH_CHECK( data.getIntrusivePtr()-\u003eautograd_meta() == nullptr, \"Attempted to make a tensor into a differentiable view, but the \" \"tensor already had autograd metadata associated with it. If you are \" \"using a __torch_dispatch__ mode, the most common cause for this \" \"problem is that you used torch.overrides.enable_reentrant_dispatch() \" \"improperly; tensors created within the extent of reentrant dispatch \" \"MUST NOT be directly returned from __torch_dispatch__; instead, they \" \"must be wrapped into fresh tensors that serve as the output. If you \" \"are not using wrappers, you probably don\u0027t need reentrant dispatch. \" \"If this doesn\u0027t seem applicable, please file a bug to PyTorch.\"); at::TensorImpl* data_impl = data.unsafeGetTensorImpl(); data_impl-\u003eset_allow_tensor_metadata_change(allow_tensor_metadata_change); data_impl-\u003eset_autograd_meta(std::make_unique\u003cDifferentiableViewMeta\u003e( data_impl, std::move(backward_info), std::move(forward_info), shared_view_info, creation_meta)); return data; } return Variable(); } // See NOTE [ Autograd View Variables ] for details. // Non-differentiable view. Just share version counter. inline Variable make_variable_non_differentiable_view( const Variable\u0026 base, const at::Tensor\u0026 data, bool allow_tensor_metadata_change = true) { if (data.defined()) { // Currently all of non-differentiable view ops(detach/_indices/_values) // share the same TensorImpl as their base Tensor. Thus a new TensorImpl // allocation here is required. auto data_impl_copy = data.getIntrusivePtr()-\u003eshallow_copy_and_detach( /*version_counter=*/impl::version_counter(base), /*allow_tensor_metadata_change=*/allow_tensor_metadata_change); data_impl_copy-\u003eset_autograd_meta(nullptr); return Variable(data_impl_copy); } return Variable(); } inline Variable make_variable( at::Tensor data, bool requires_grad = false, bool allow_tensor_metadata_change = true) { if (data.defined()) { if (data.getIntrusivePtr().use_count() == 1 \u0026\u0026 data.getIntrusivePtr()-\u003eunique_version()) { auto data_impl = data.unsafeReleaseIntrusivePtr(); data_impl-\u003eset_allow_tensor_metadata_change(allow_tensor_metadata_change); if (requires_grad) { data_impl-\u003eset_autograd_meta( std::make_unique\u003cAutogradMeta\u003e(data_impl.get(), requires_grad)); } else { data_impl-\u003eset_autograd_meta(nullptr); } return Variable(std::move(data_impl)); } else { auto data_impl_copy = data.getIntrusivePtr()-\u003eshallow_copy_and_detach( /*version_counter=*/0, /*allow_tensor_metadata_change=*/allow_tensor_metadata_change); if (requires_grad) { data_impl_copy-\u003eset_autograd_meta(std::make_unique\u003cAutogradMeta\u003e( data_impl_copy.get(), requires_grad)); } else { data_impl_copy-\u003eset_autograd_meta(nullptr); } return Variable(std::move(data_impl_copy)); } } return Variable(); } inline Variable make_variable( const at::Tensor\u0026 data, Edge gradient_edge, bool allow_tensor_metadata_change = true) { if (data.defined()) { auto data_impl_copy = data.getIntrusivePtr()-\u003eshallow_copy_and_detach( /*version_counter=*/0, /*allow_tensor_metadata_change=*/allow_tensor_metadata_change); data_impl_copy-\u003eset_autograd_meta(std::make_unique\u003cAutogradMeta\u003e( data_impl_copy.get(), false, std::move(gradient_edge))); return Variable(data_impl_copy); } return Variable(); } struct VariableHooks final : at::impl::VariableHooksInterface { at::TensorBase tensor_data(const at::TensorBase\u0026) const override; at::TensorBase variable_data(const at::TensorBase\u0026) const override; const std::shared_ptr\u003ctorch::autograd::Node\u003e\u0026 grad_fn( const at::TensorBase\u0026) const override; unsigned _register_hook( const at::TensorBase\u0026, std::function\u003cat::TensorBase(const at::TensorBase\u0026)\u003e hook) const override; void remove_hook(const at::TensorBase\u0026, unsigned pos) const override; bool is_view(const at::TensorBase\u0026) const override; const at::TensorBase\u0026 base(const at::TensorBase\u0026) const override; const std::string\u0026 name(const at::TensorBase\u0026) const override; bool is_leaf(const at::TensorBase\u0026) const override; int64_t output_nr(const at::TensorBase\u0026) const override; void set_data(const at::TensorBase\u0026 self, const at::TensorBase\u0026 new_data) const override; at::TensorBase data(const at::TensorBase\u0026 self) const override; int64_t _version(const at::TensorBase\u0026 self) const override; void retain_grad(const at::TensorBase\u0026 self) const override; bool retains_grad(const at::TensorBase\u0026 self) const override; void _backward( const at::Tensor\u0026 self, at::TensorList inputs, const std::optional\u003cat::Tensor\u003e\u0026 gradient, std::optional\u003cbool\u003e keep_graph, bool create_graph) const override; void requires_grad_(const at::TensorBase\u0026 self, bool _requires_grad) const override; void basic_autograd_not_implemented_fallback( const c10::OperatorHandle\u0026 op, c10::DispatchKeySet dispatch_keys, torch::jit::Stack* stack) const override; }; namespace utils { TORCH_API bool has_same_meta(const Variable\u0026 base, const Variable\u0026 other); } // namespace utils } // namespace torch::autograd #endif /* DOXYGEN_SHOULD_SKIP_THIS */",
       "author": {
         "@type": "Organization",
         "name": "PyTorch Contributors",
         "url": "https://pytorch.org"
       },
       "image": "../_static/img/pytorch_seo.png",
       "mainEntityOfPage": {
         "@type": "WebPage",
         "@id": "/api/program_listing_file_torch_csrc_autograd_variable.h.html"
       },
       "datePublished": "2023-01-01T00:00:00Z",
       "dateModified": "2023-01-01T00:00:00Z"
     }
 </script>
  <script>
    // Tutorials Call to action event tracking
    $("[data-behavior='call-to-action-event']").on('click', function () {
      fbq('trackCustom', "Download", {
        tutorialTitle: $('h1:first').text(),
        downloadLink: this.href,
        tutorialLink: window.location.href,
        downloadTitle: $(this).attr("data-response")
      });
      if (typeof gtag === 'function') {
        gtag('event', 'click', {
          'event_category': $(this).attr("data-response"),
          'event_label': $("h1").first().text(),
          'tutorial_link': window.location.href
        });
      }
    });
  </script>
  
  </body>
</html>